[{"contents":"Opaque Secret 要求 value 必须是 base64 编码的\n测试：如果 value 不是 base64 呢？ 准备如下 yaml 用来创建 secret，其中 value 不是 base64 格式的：\nsecret_not_base64.yaml\napiVersion: v1 kind: Secret metadata: name: mysecret-not-base64 type: Opaque data: user: root pass: 123r 运行并查看：\n$ kubectl apply -f secret_not_base64.yaml $ kubectl get secret NAME TYPE DATA AGE mysecret-not-base64 Opaque 2 42m $ kubectl get secret mysecret-not-base64 -oyaml apiVersion: v1 data: pass: 123r user: root kind: Secret metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;data\u0026quot;:{\u0026quot;pass\u0026quot;:\u0026quot;123r\u0026quot;,\u0026quot;user\u0026quot;:\u0026quot;root\u0026quot;},\u0026quot;kind\u0026quot;:\u0026quot;Secret\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;mysecret-not-base64\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;type\u0026quot;:\u0026quot;Opaque\u0026quot;} creationTimestamp: \u0026quot;2022-10-10T14:25:30Z\u0026quot; name: mysecret-not-base64 namespace: default resourceVersion: \u0026quot;158457\u0026quot; uid: 44ae7fda-3f7f-4da3-95b1-c3068727d765 type: Opaque 貌似没什么影响啊？可以正常创建，也可以查看其中的值，继续测试看看\n该 yaml 从名为 mysecret-not-base64 的 secret 中引用值，并将其设置为环境变量，然后会执行 env 命令查看当前系统里的所有环境变量（busybox 是一个集成了一些 linux 同样命令的镜像）\nsecret_not_base64_pod.yaml\napiVersion: v1 kind: Pod metadata: name: secret-not-base64-pod spec: containers: - name: secret1 image: busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;env\u0026quot; ] env: - name: USERNAME # 设置名为 USERNAME 的环境变量，其值引用 mysecret-not-base64 中的 user 字段 valueFrom: secretKeyRef: name: mysecret-not-base64 key: user - name: PASSWORD valueFrom: secretKeyRef: name: mysecret-not-base64 key: pass 运行并查看：\n$ kubectl apply -f secret_not_base64_pod.yaml $ kubectl logs secret-not-base64-pod KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=secret-not-base64-pod SHLVL=1 HOME=/root USERNAME=��- KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ PASSWORD=�m� 发现我们设置的两个环境变量 USERNAME 和 PASSWORD 的值都显示为乱码，莫非如果引用了Opaque 类型的 secret 中的字段，会执行解码操作，将解码出来的值作为 value 进行设置吗？\n为了搞清楚这个问题，我们可以再尝试做一个 value 是 base64 编码过的 secret：\nsecret.yaml\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: user: YWRtaW4= pass: MWYyZDFlMmU2N2Rm tips：linux/mac 自带了 base64 命令，可以方便的获取值的 base64:\n$ echo -n \u0026quot;admin\u0026quot; | base64 YWRtaW4= $ echo -n \u0026quot;admin321\u0026quot; | base64 YWRtaW4zMjE= $ echo \u0026quot;YWRtaW4=\u0026quot; | base64 -d admin # 解密 $ echo \u0026quot;MWYyZDFlMmU2N2Rm\u0026quot; | base64 -d 1f2d1e2e67df secret_base64_pod.yaml\napiVersion: v1 kind: Pod metadata: name: secret-base64-pod spec: containers: - name: secret1 image: busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;env\u0026quot; ] env: - name: USERNAME valueFrom: secretKeyRef: name: mysecret key: user - name: PASSWORD valueFrom: secretKeyRef: name: mysecret key: pass 运行：\n$ kubectl apply -f secret_base64_pod.yaml pod/secret-base64-pod created $ kubectl logs secret-base64-pod KUBERNETES_SERVICE_PORT=443 KUBERNETES_PORT=tcp://10.96.0.1:443 HOSTNAME=secret-base64-pod SHLVL=1 HOME=/root USERNAME=admin KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443 KUBERNETES_SERVICE_HOST=10.96.0.1 PWD=/ PASSWORD=1f2d1e2e67df 发现环境变量 USERNAME 和 PASSWORD 的值都正常显示了，看来确实是这样，当引用Opaque 类型的 secret 中的值时，会自动做 base64 解码操作\n再试试用挂载的方式：\n这里将 mysecret 这个 secret 下的所有条目挂载到了容器内的 /projected-volume 目录下，mysecret 就是上面定义的 secret.yaml：\nsecret-test-pod.yaml\napiVersion: v1 kind: Pod metadata: name: test-projected-volume spec: containers: - name: test-secret-volume image: busybox args: - sleep - \u0026quot;86400\u0026quot; volumeMounts: - name: mysql-cred mountPath: \u0026quot;/projected-volume\u0026quot; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: mysecret 运行并查看效果：\n$ kubectl apply -f secret-test-pod.yaml pod/test-projected-volume created $ kubectl get po NAME READY STATUS RESTARTS AGE test-projected-volume 1/1 Running 0 7s $ kubectl exec -it test-projected-volume -- cat /projected-volume/pass 1f2d1e2e67df% $ kubectl exec -it test-projected-volume -- cat /projected-volume/user admin% 发现这里将 secret 中的每个条目都单独做成了一个文件，并且文件内容都是已经解码过的值\n再试试挂载引用之前那个 value 不是 base64 的 secret：\napiVersion: v1 kind: Pod metadata: name: test-projected-volume-not-base64 spec: containers: - name: test-secret-volume image: busybox args: - sleep - \u0026quot;86400\u0026quot; volumeMounts: - name: mysql-cred mountPath: \u0026quot;/projected-volume\u0026quot; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: mysecret-not-base64 # 修改这里 运行：\n$ kubectl exec -it test-projected-volume-not-base64 -- cat /projected-volume/user ??-% $ kubectl exec -it test-projected-volume-not-base64 -- cat /projected-volume/pass ?m?% 发现乱码了\n","date":"2022年10月10日","permalink":"/posts/k8s-secret/","summary":"Opaque Secret 要求 value 必须是 base64 编码的","title":"k8s Secret"},{"contents":"架构 Informers 由几个核心的组件构成：\nReflector：负责从 api-server list（全量拉取数据） and watch（监听数据变更） DeltaFIFO：一个存储事件的队列，里面记录了事件的类型 Indexer：存储数据，数据来源是从 DeltaFIFO 中 pop 出来的，然后会根据事件类型进行对于的操作 sharedProcessor：用于运行用户设置的事件回调函数，里面用 map 存储了所有的 listener，每次调用 AddEventHandler 都会创建一个 listener，同时这个函数可以调用多次，也就是创建多个 listener，当发送事件时，会调用所有的 listener 的对应回调函数 controller：上面提到的 Reflector、DeltaFIFO、Indexer 各自有各自的作用，但是它们彼此之间还没有关联起来，而 controller 就是负责这件事的，它是这 3 个组件的 master，让它们可以协同运作，大致流程是：当 Reflector watch 到事件时会将其保存到 DeltaFIFO 中，controller 这件会持续从 DeltaFIFO 中 pop 元素，然后根据事件类型对 indexer 进行相应操作（add、update、delete），使得 indexer 中的数据和 api-server 中的一致，同时还会调用 sharedProcessor 的对应回调，来完成用户设置的对应事件操作。 informers/factory.go types sharedInformerFactory type sharedInformerFactory struct { client kubernetes.Interface namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc lock sync.Mutex defaultResync time.Duration customResync map[reflect.Type]time.Duration informers map[reflect.Type]cache.SharedIndexInformer // startedInformers is used for tracking which informers have been started. // This allows Start() to be called multiple times safely. startedInformers map[reflect.Type]bool // wg tracks how many goroutines were started. wg sync.WaitGroup // shuttingDown is true when Shutdown has been called. It may still be running // because it needs to wait for goroutines. shuttingDown bool } Start() // Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh \u0026lt;-chan struct{}) { f.lock.Lock() defer f.lock.Unlock() // 遍历 factory 中已注册的 informer，如果该 informer 还未运行过，则运行它 for informerType, informer := range f.informers { if !f.startedInformers[informerType] {\t// 还未运行过 go informer.Run(stopCh)\t// 运行该 informer f.startedInformers[informerType] = true\t// 标记为已运行 } } } InformerFor() InformerFor 填充 sharedInformerFactory 的 informers 字段\n会检查该 type 是否已经注册，如果没注册则会调用传递进来的 newFunc 回调函数进行创建，并注册到 informers 中\n// InternalInformerFor returns the SharedIndexInformer for obj using an internal // client. func (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer { f.lock.Lock() defer f.lock.Unlock() // 使用反射获取 obj 的类型 informerType := reflect.TypeOf(obj) // 查看是否已注册过该 informerType，如果注册过则直接返回，符合了 shared 的定义 informer, exists := f.informers[informerType] if exists { return informer } resyncPeriod, exists := f.customResync[informerType] if !exists { resyncPeriod = f.defaultResync } // 走到这里说明 informerType 还未注册过，那么就创建一个 informer 并注册 informer = newFunc(f.client, resyncPeriod) f.informers[informerType] = informer return informer } 示例 比如有一个 podInformer，它有一个 Informer 方法，该方法会调用 InformerFor\nfunc (f *podInformer) Informer() cache.SharedIndexInformer { return f.factory.InformerFor(\u0026amp;corev1.Pod{}, f.defaultInformer) } 它传递的回调函数是 f.defaultInformer，该函数定义如下：\nfunc (f *podInformer) defaultInformer(client kubernetes.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer { return NewFilteredPodInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions) } 该函数又调用了 NewFilteredPodInformer：\n// NewFilteredPodInformer constructs a new informer for Pod type. // Always prefer using an informer factory to get a shared informer instead of getting an independent // one. This reduces memory footprint and number of connections to the server. func NewFilteredPodInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer { return cache.NewSharedIndexInformer( \u0026amp;cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { if tweakListOptions != nil { tweakListOptions(\u0026amp;options) } return client.CoreV1().Pods(namespace).List(context.TODO(), options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { if tweakListOptions != nil { tweakListOptions(\u0026amp;options) } return client.CoreV1().Pods(namespace).Watch(context.TODO(), options) }, }, \u0026amp;corev1.Pod{}, resyncPeriod, indexers, ) } 这个函数又调用了 NewSharedIndexInformer ，最终会创建出一个 SharedIndexInformer，该函数定义在 tools/cache/shared_informer.go\nfunctions NewSharedInformerFactory 实际调用的是 NewSharedInformerFactoryWithOptions\n// NewSharedInformerFactory constructs a new instance of sharedInformerFactory for all namespaces. func NewSharedInformerFactory(client kubernetes.Interface, defaultResync time.Duration) SharedInformerFactory { return NewSharedInformerFactoryWithOptions(client, defaultResync) } NewSharedInformerFactoryWithOptions // NewSharedInformerFactoryWithOptions constructs a new instance of a SharedInformerFactory with additional options. func NewSharedInformerFactoryWithOptions(client kubernetes.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory { factory := \u0026amp;sharedInformerFactory{ client: client, namespace: v1.NamespaceAll, defaultResync: defaultResync, informers: make(map[reflect.Type]cache.SharedIndexInformer), startedInformers: make(map[reflect.Type]bool), customResync: make(map[reflect.Type]time.Duration), } // Apply all options for _, opt := range options { factory = opt(factory) } return factory } tools/cache/shared_informer.go types sharedIndexInformer sharedIndexInformer 结构体，里面的几个核心属性是用来 list-watch 的 listerWatcher，负责存储的 indexer，负责执行整套流程的的 controller（从 reflector 中 list-watch，从 DefltaFIFO 中 pop 并更新 indexer），负责执行用户设置的 eventHandle 的 processor\n// `*sharedIndexInformer` implements SharedIndexInformer and has three // main components. One is an indexed local cache, `indexer Indexer`. // The second main component is a Controller that pulls // objects/notifications using the ListerWatcher and pushes them into // a DeltaFIFO --- whose knownObjects is the informer's local cache // --- while concurrently Popping Deltas values from that fifo and // processing them with `sharedIndexInformer::HandleDeltas`. Each // invocation of HandleDeltas, which is done with the fifo's lock // held, processes each Delta in turn. For each Delta this both // updates the local cache and stuffs the relevant notification into // the sharedProcessor. The third main component is that // sharedProcessor, which is responsible for relaying those // notifications to each of the informer's clients. type sharedIndexInformer struct { indexer Indexer controller Controller processor *sharedProcessor cacheMutationDetector MutationDetector listerWatcher ListerWatcher // objectType is an example object of the type this informer is // expected to handle. Only the type needs to be right, except // that when that is `unstructured.Unstructured` the object's // `\u0026quot;apiVersion\u0026quot;` and `\u0026quot;kind\u0026quot;` must also be right. objectType runtime.Object // resyncCheckPeriod is how often we want the reflector's resync timer to fire so it can call // shouldResync to check if any of our listeners need a resync. resyncCheckPeriod time.Duration // defaultEventHandlerResyncPeriod is the default resync period for any handlers added via // AddEventHandler (i.e. they don't specify one and just want to use the shared informer's default // value). defaultEventHandlerResyncPeriod time.Duration // clock allows for testability clock clock.Clock started, stopped bool startedLock sync.Mutex // blockDeltas gives a way to stop all event distribution so that a late event handler // can safely join the shared informer. blockDeltas sync.Mutex // Called whenever the ListAndWatch drops the connection with an error. watchErrorHandler WatchErrorHandler transform TransformFunc } Run() Run 会调用 s.controller.Run\nfunc (s *sharedIndexInformer) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() if s.HasStarted() { klog.Warningf(\u0026quot;The sharedIndexInformer has started, run more than once is not allowed\u0026quot;) return } // 创建一个 DeltaFIFO fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{ KnownObjects: s.indexer, EmitDeltaTypeReplaced: true, }) // 配置用来创建 controller 的 Config cfg := \u0026amp;Config{ Queue: fifo, // 设置为 DeltaFIFO ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, WatchErrorHandler: s.watchErrorHandler, } func() { s.startedLock.Lock() defer s.startedLock.Unlock() // 创建 controller s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() // 运行 controller s.controller.Run(stopCh) } HandleDeltas() 对 DeltaFIFO 中 pop 出的元素进行处理\nfunc (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() if deltas, ok := obj.(Deltas); ok { // 又调用了 processDeltas，其中第二个参数传递的是 indexer return processDeltas(s, s.indexer, s.transform, deltas) } return errors.New(\u0026quot;object given as Process argument is not Deltas\u0026quot;) } AddEventHandler() 添加事件处理函数，但发生事件时（add，update，delete）会调用对应的回调函数，实际调用的是AddEventHandlerWithResyncPeriod，该函数可以调用多次（也就是 add 多个 EventHandler），每次都会创建一个 listener，当发生事件时，会一并通知所有的 listener\nfunc (s *sharedIndexInformer) AddEventHandler(handler ResourceEventHandler) (ResourceEventHandlerRegistration, error) { return s.AddEventHandlerWithResyncPeriod(handler, s.defaultEventHandlerResyncPeriod) } AddEventHandlerWithResyncPeriod() 该函数会创建一个 listener\nfunc (s *sharedIndexInformer) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) (ResourceEventHandlerRegistration, error) { s.startedLock.Lock() defer s.startedLock.Unlock() if s.stopped { return nil, fmt.Errorf(\u0026quot;handler %v was not added to shared informer because it has stopped already\u0026quot;, handler) } if resyncPeriod \u0026gt; 0 { if resyncPeriod \u0026lt; minimumResyncPeriod { klog.Warningf(\u0026quot;resyncPeriod %v is too small. Changing it to the minimum allowed value of %v\u0026quot;, resyncPeriod, minimumResyncPeriod) resyncPeriod = minimumResyncPeriod } if resyncPeriod \u0026lt; s.resyncCheckPeriod { if s.started { klog.Warningf(\u0026quot;resyncPeriod %v is smaller than resyncCheckPeriod %v and the informer has already started. Changing it to %v\u0026quot;, resyncPeriod, s.resyncCheckPeriod, s.resyncCheckPeriod) resyncPeriod = s.resyncCheckPeriod } else { // if the event handler's resyncPeriod is smaller than the current resyncCheckPeriod, update // resyncCheckPeriod to match resyncPeriod and adjust the resync periods of all the listeners // accordingly s.resyncCheckPeriod = resyncPeriod s.processor.resyncCheckPeriodChanged(resyncPeriod) } } } // 创建一个 listener listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize) if !s.started { return s.processor.addListener(listener), nil } // in order to safely join, we have to // 1. stop sending add/update/delete notifications // 2. do a list against the store // 3. send synthetic \u0026quot;Add\u0026quot; events to the new handler // 4. unblock s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // 将 listener 添加到 s.processor handle := s.processor.addListener(listener) // 遍历当前 indexer 里的所有数据，对所有的 listener 进行通知 for _, item := range s.indexer.List() { listener.add(addNotification{newObj: item}) } return handle, nil } OnAdd() // Conforms to ResourceEventHandler func (s *sharedIndexInformer) OnAdd(obj interface{}) { // Invocation of this function is locked under s.blockDeltas, so it is // save to distribute the notification s.cacheMutationDetector.AddObject(obj) s.processor.distribute(addNotification{newObj: obj}, false) } OnUpdate() // Conforms to ResourceEventHandler func (s *sharedIndexInformer) OnUpdate(old, new interface{}) { isSync := false // If is a Sync event, isSync should be true // If is a Replaced event, isSync is true if resource version is unchanged. // If RV is unchanged: this is a Sync/Replaced event, so isSync is true if accessor, err := meta.Accessor(new); err == nil { if oldAccessor, err := meta.Accessor(old); err == nil { // Events that didn't change resourceVersion are treated as resync events // and only propagated to listeners that requested resync isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion() } } // Invocation of this function is locked under s.blockDeltas, so it is // save to distribute the notification s.cacheMutationDetector.AddObject(new) s.processor.distribute(updateNotification{oldObj: old, newObj: new}, isSync) } OnDelete() // Conforms to ResourceEventHandler func (s *sharedIndexInformer) OnDelete(old interface{}) { // Invocation of this function is locked under s.blockDeltas, so it is // save to distribute the notification s.processor.distribute(deleteNotification{oldObj: old}, false) } sharedProcessor // sharedProcessor has a collection of processorListener and can // distribute a notification object to its listeners. There are two // kinds of distribute operations. The sync distributions go to a // subset of the listeners that (a) is recomputed in the occasional // calls to shouldResync and (b) every listener is initially put in. // The non-sync distributions go to every listener. type sharedProcessor struct { listenersStarted bool listenersLock sync.RWMutex // Map from listeners to whether or not they are currently syncing listeners map[*processorListener]bool clock clock.Clock wg wait.Group } addListener() func (p *sharedProcessor) addListener(listener *processorListener) { p.listenersLock.Lock() defer p.listenersLock.Unlock() p.addListenerLocked(listener) if p.listenersStarted { p.wg.Start(listener.run) p.wg.Start(listener.pop) } } distribute() func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for listener, isSyncing := range p.listeners { switch { case !sync: // non-sync messages are delivered to every listener listener.add(obj) case isSyncing: // sync messages are delivered to every syncing listener listener.add(obj) default: // skipping a sync obj for a non-syncing listener } } } processorListener // processorListener relays notifications from a sharedProcessor to // one ResourceEventHandler --- using two goroutines, two unbuffered // channels, and an unbounded ring buffer. The `add(notification)` // function sends the given notification to `addCh`. One goroutine // runs `pop()`, which pumps notifications from `addCh` to `nextCh` // using storage in the ring buffer while `nextCh` is not keeping up. // Another goroutine runs `run()`, which receives notifications from // `nextCh` and synchronously invokes the appropriate handler method. // // processorListener also keeps track of the adjusted requested resync // period of the listener. type processorListener struct { nextCh chan interface{} addCh chan interface{} handler ResourceEventHandler // pendingNotifications is an unbounded ring buffer that holds all notifications not yet distributed. // There is one per listener, but a failing/stalled listener will have infinite pendingNotifications // added until we OOM. // TODO: This is no worse than before, since reflectors were backed by unbounded DeltaFIFOs, but // we should try to do something better. pendingNotifications buffer.RingGrowing // requestedResyncPeriod is how frequently the listener wants a // full resync from the shared informer, but modified by two // adjustments. One is imposing a lower bound, // `minimumResyncPeriod`. The other is another lower bound, the // sharedIndexInformer's `resyncCheckPeriod`, that is imposed (a) only // in AddEventHandlerWithResyncPeriod invocations made after the // sharedIndexInformer starts and (b) only if the informer does // resyncs at all. requestedResyncPeriod time.Duration // resyncPeriod is the threshold that will be used in the logic // for this listener. This value differs from // requestedResyncPeriod only when the sharedIndexInformer does // not do resyncs, in which case the value here is zero. The // actual time between resyncs depends on when the // sharedProcessor's `shouldResync` function is invoked and when // the sharedIndexInformer processes `Sync` type Delta objects. resyncPeriod time.Duration // nextResync is the earliest time the listener should get a full resync nextResync time.Time // resyncLock guards access to resyncPeriod and nextResync resyncLock sync.Mutex } newProcessListener() 该函数在 sharedIndexInformer.AddEventHandlerWithResyncPeriod 中调用，创建一个新的 listener\nfunc newProcessListener(handler ResourceEventHandler, requestedResyncPeriod, resyncPeriod time.Duration, now time.Time, bufferSize int) *processorListener { ret := \u0026amp;processorListener{ nextCh: make(chan interface{}), addCh: make(chan interface{}), handler: handler, pendingNotifications: *buffer.NewRingGrowing(bufferSize), requestedResyncPeriod: requestedResyncPeriod, resyncPeriod: resyncPeriod, } ret.determineNextResync(now) return ret } functions NewSharedIndexInformer // NewSharedIndexInformer creates a new instance for the listwatcher. // The created informer will not do resyncs if the given // defaultEventHandlerResyncPeriod is zero. Otherwise: for each // handler that with a non-zero requested resync period, whether added // before or after the informer starts, the nominal resync period is // the requested resync period rounded up to a multiple of the // informer's resync checking period. Such an informer's resync // checking period is established when the informer starts running, // and is the maximum of (a) the minimum of the resync periods // requested before the informer starts and the // defaultEventHandlerResyncPeriod given here and (b) the constant // `minimumResyncPeriod` defined in this file. func NewSharedIndexInformer(lw ListerWatcher, exampleObject runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer { realClock := \u0026amp;clock.RealClock{} sharedIndexInformer := \u0026amp;sharedIndexInformer{ processor: \u0026amp;sharedProcessor{clock: realClock}, indexer: NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers), listerWatcher: lw, objectType: exampleObject, resyncCheckPeriod: defaultEventHandlerResyncPeriod, defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod, cacheMutationDetector: NewCacheMutationDetector(fmt.Sprintf(\u0026quot;%T\u0026quot;, exampleObject)), clock: realClock, } return sharedIndexInformer } tools/cache/controller.go types controller // `*controller` implements Controller type controller struct { config Config reflector *Reflector reflectorMutex sync.RWMutex clock clock.Clock } processLoop() processLoop 会不断从 DeltaFIFO 中 pop 出元素，并调用 c.config.Process 对 pop 出的元素进行处理，这个 Process 实际是 HandleDeltas\n// processLoop drains the work queue. // TODO: Consider doing the processing in parallel. This will require a little thought // to make sure that we don't end up processing the same object multiple times // concurrently. // // TODO: Plumb through the stopCh here (and down to the queue) so that this can // actually exit when the controller is stopped. Or just give up on this stuff // ever being stoppable. Converting this whole package to use Context would // also be helpful. func (c *controller) processLoop() { for { obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == ErrFIFOClosed { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } Run() sharedIndexInformer.Run 里会创建一个 DeltaFIFO，并将 config.Queue 设置为这个 DeltaFIFO\n// Run begins processing items, and will continue until a value is sent down stopCh or it is closed. // It's an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() go func() { \u0026lt;-stopCh c.config.Queue.Close() }() // 创建一个 reflector 用来 list-watch // 这里的 c.config.Queue 实际是一个 DeltaFIFO r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.WatchListPageSize = c.config.WatchListPageSize r.clock = c.clock if c.config.WatchErrorHandler != nil { r.watchErrorHandler = c.config.WatchErrorHandler } c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group wg.StartWithChannel(stopCh, r.Run) wait.Until(c.processLoop, time.Second, stopCh) wg.Wait() } Config type Config struct { // The queue for your objects - has to be a DeltaFIFO due to // assumptions in the implementation. Your Process() function // should accept the output of this Queue's Pop() method. // 应该传递一个 DeltaFIFO 类型的 queue，Process() 中会从该 queue // 中 pop 元素进行处理 Queue // Something that can list and watch your objects. ListerWatcher // Something that can process a popped Deltas. // Pop 出来的 obj 处理函数 Process ProcessFunc // ObjectType is an example object of the type this controller is // expected to handle. Only the type needs to be right, except // that when that is `unstructured.Unstructured` the object's // `\u0026quot;apiVersion\u0026quot;` and `\u0026quot;kind\u0026quot;` must also be right. ObjectType runtime.Object // FullResyncPeriod is the period at which ShouldResync is considered. FullResyncPeriod time.Duration // ShouldResync is periodically used by the reflector to determine // whether to Resync the Queue. If ShouldResync is `nil` or // returns true, it means the reflector should proceed with the // resync. ShouldResync ShouldResyncFunc // If true, when Process() returns an error, re-enqueue the object. // TODO: add interface to let you inject a delay/backoff or drop // the object completely if desired. Pass the object in // question to this interface as a parameter. This is probably moot // now that this functionality appears at a higher level. RetryOnError bool // Called whenever the ListAndWatch drops the connection with an error. WatchErrorHandler WatchErrorHandler // WatchListPageSize is the requested chunk size of initial and relist watch lists. WatchListPageSize int64 } ProcessFunc // ProcessFunc processes a single object. type ProcessFunc func(obj interface{}) error functions New // New makes a new Controller from the given Config. func New(c *Config) Controller { ctlr := \u0026amp;controller{ config: *c, clock: \u0026amp;clock.RealClock{}, } return ctlr } processDeltas 如果是 HandleDeltas 调用该函数，那么第二个参数传递的是 indexer，那么这里就会根据 pop 的事件类型对 indexer 进行相应操作，同时还传递了一个 handler 参数，这是一个接口类型，需要实现 OnAdd、OnUpdate、OnDelete 三个方法，sharedIndexInformer 实现了这个接口，实际会调用 sharedProcessor 的 OnAdd 等方法，这些方法是用户自己设置的回调函数\n// Multiplexes updates in the form of a list of Deltas into a Store, and informs // a given handler of events OnUpdate, OnAdd, OnDelete func processDeltas( // Object which receives event notifications from the given deltas handler ResourceEventHandler, clientState Store, transformer TransformFunc, deltas Deltas, ) error { // from oldest to newest for _, d := range deltas { obj := d.Object if transformer != nil { var err error obj, err = transformer(obj) if err != nil { return err } } switch d.Type { case Sync, Replaced, Added, Updated: if old, exists, err := clientState.Get(obj); err == nil \u0026amp;\u0026amp; exists { // 更新 indexer 里的数据 if err := clientState.Update(obj); err != nil { return err } // 同时执行用户设置的回调函数 handler.OnUpdate(old, obj) } else { if err := clientState.Add(obj); err != nil { return err } handler.OnAdd(obj) } case Deleted: if err := clientState.Delete(obj); err != nil { return err } handler.OnDelete(obj) } } return nil } tools/cache/reflector.go types functions watchHandler 当 watch 到有事件产生时，会执行该函数对事件进行相应的处理\n// watchHandler watches w and sets setLastSyncResourceVersion func watchHandler(start time.Time, w watch.Interface, store Store, // 省略其他参数... ) error { eventCount := 0 // Stopping the watcher should be idempotent and if we return from this function there's no way // we're coming back in with the same watch interface. defer w.Stop() loop: for { select { // 省略... // 有事件产生了 case event, ok := \u0026lt;-w.ResultChan(): if !ok { break loop } if event.Type == watch.Error { return apierrors.FromObject(event.Object) } // 省略... // 判断事件类型，进行相应的处理 switch event.Type { case watch.Added: // 这里的 store 实际是 DeltaFIFO，调用的是 DeltaFIFO.Add() err := store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026quot;%s: unable to add watch event object (%#v) to store: %v\u0026quot;, name, event.Object, err)) } case watch.Modified: err := store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026quot;%s: unable to update watch event object (%#v) to store: %v\u0026quot;, name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \u0026quot;last known // state\u0026quot;, which is passed in event.Object? If so, may need // to change this. err := store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026quot;%s: unable to delete watch event object (%#v) from store: %v\u0026quot;, name, event.Object, err)) } case watch.Bookmark: // A `Bookmark` means watch has synced here, just update the resourceVersion default: utilruntime.HandleError(fmt.Errorf(\u0026quot;%s: unable to understand watch event %#v\u0026quot;, name, event)) } setLastSyncResourceVersion(resourceVersion) if rvu, ok := store.(ResourceVersionUpdater); ok { rvu.UpdateResourceVersion(resourceVersion) } eventCount++ } } // 省略... } tools/cache/delta_fifo.go types DeltaFIFO // DeltaFIFO is like FIFO, but differs in two ways. One is that the // accumulator associated with a given object's key is not that object // but rather a Deltas, which is a slice of Delta values for that // object. Applying an object to a Deltas means to append a Delta // except when the potentially appended Delta is a Deleted and the // Deltas already ends with a Deleted. In that case the Deltas does // not grow, although the terminal Deleted will be replaced by the new // Deleted if the older Deleted's object is a // DeletedFinalStateUnknown. // // The other difference is that DeltaFIFO has two additional ways that // an object can be applied to an accumulator: Replaced and Sync. // If EmitDeltaTypeReplaced is not set to true, Sync will be used in // replace events for backwards compatibility. Sync is used for periodic // resync events. // // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // // DeltaFIFO solves this use case: // - You want to process every object change (delta) at most once. // - When you process an object, you want to see everything // that's happened to it since you last processed it. // - You want to process the deletion of some of the objects. // - You might want to periodically reprocess objects. // // DeltaFIFO's Pop(), Get(), and GetByKey() methods return // interface{} to satisfy the Store/Queue interfaces, but they // will always return an object of type Deltas. List() returns // the newest object from each accumulator in the FIFO. // // A DeltaFIFO's knownObjects KeyListerGetter provides the abilities // to list Store keys and to get objects by Store key. The objects in // question are called \u0026quot;known objects\u0026quot; and this set of objects // modifies the behavior of the Delete, Replace, and Resync methods // (each in a different way). // // A note on threading: If you call Pop() in parallel from multiple // threads, you could end up with multiple threads processing slightly // different versions of the same object. type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. lock sync.RWMutex cond sync.Cond // `items` maps a key to a Deltas. // Each such Deltas has at least one Delta. items map[string]Deltas // `queue` maintains FIFO order of keys for consumption in Pop(). // There are no duplicates in `queue`. // A key is in `queue` if and only if it is in `items`. queue []string // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update/AddIfNotPresent was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // knownObjects list keys that are \u0026quot;known\u0026quot; --- affecting Delete(), // Replace(), and Resync() knownObjects KeyListerGetter // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRUD operations. closed bool // emitDeltaTypeReplaced is whether to emit the Replaced or Sync // DeltaType when Replace() is called (to preserve backwards compat). emitDeltaTypeReplaced bool } Add() 实际调用的是 queueActionLocked 方法\n// Add inserts an item, and puts it in the queue. The item is only enqueued // if it doesn't already exist in the set. func (f *DeltaFIFO) Add(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true return f.queueActionLocked(Added, obj) } queueActionLocked() queueActionLocked 会将 actionType 和 obj 封装为一个 Delta 对象，并 push 到 DeltaFIFO\n// queueActionLocked appends to the delta list for the object. // Caller must lock first. func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error { id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } oldDeltas := f.items[id] newDeltas := append(oldDeltas, Delta{actionType, obj}) newDeltas = dedupDeltas(newDeltas) if len(newDeltas) \u0026gt; 0 { if _, exists := f.items[id]; !exists { f.queue = append(f.queue, id) } f.items[id] = newDeltas f.cond.Broadcast() } else { // This never happens, because dedupDeltas never returns an empty list // when given a non-empty list (as it is here). // If somehow it happens anyway, deal with it but complain. if oldDeltas == nil { klog.Errorf(\u0026quot;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; ignoring\u0026quot;, id, oldDeltas, obj) return nil } klog.Errorf(\u0026quot;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; breaking invariant by storing empty Deltas\u0026quot;, id, oldDeltas, obj) f.items[id] = newDeltas return fmt.Errorf(\u0026quot;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; broke DeltaFIFO invariant by storing empty Deltas\u0026quot;, id, oldDeltas, obj) } return nil } functions 参考资料 K8s 系列(四) - 浅谈 Informer.md\n深入了解 Kubernetes Informer\n","date":"2022年10月07日","permalink":"/posts/client-go-yuan-ma/","summary":"架构 Informers 由几个核心的组件构成：\nReflector：负责从 api-server list（全量拉取数据） and watch（监听数据变更） DeltaFIFO：一个存储事件的队列，里面记录了事件的类型 Indexer：存储数据，数据来源是从 DeltaFIFO 中 pop 出来的，然后会根据事件类型进行对于的操作 sharedProcessor：用于运行用户设置的事件回调函数，里面用 map 存储了所有的 listener，每次调用 AddEventHandler 都会创建一个 listener，同时这个函数可以调用多次，也就是创建多个 listener，当发送事件时，会调用所有的 listener 的对应回调函数 controller：上面提到的 Reflector、DeltaFIFO、Indexer 各自有各自的作用，但是它们彼此之间还没有关联起来，而 controller 就是负责这件事的，它是这 3 个组件的 master，让它们可以协同运作，大致流程是：当 Reflector watch 到事件时会将其保存到 DeltaFIFO 中，controller 这件会持续从 DeltaFIFO 中 pop 元素，然后根据事件类型对 indexer 进行相应操作（add、update、delete），使得 indexer 中的数据和 api-server 中的一致，同时还会调用 sharedProcessor 的对应回调，来完成用户设置的对应事件操作。 informers/factory.","title":"k8s client-go 源码阅读"},{"contents":"快速入门 下载 kubebuiler $ os=$(go env GOOS) $ arch=$(go env GOARCH) $ curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/${os}/${arch} $ chmod +x kubebuilder $ sudo mv ./kubebuilder /usr/local/bin 创建一个项目 $ mkdir operator $ cd operator $ go mod init operator.example.com # 我们将使用 my.domain 域， # 所以所有的 API 组将是\u0026lt;group\u0026gt;.my.domain. $ kubebuilder init --domain my.domain 创建一个 API 运行下面的命令，创建一个新的 API（组/版本）为 “webapp/v1”，并在上面创建新的 Kind(CRD) “Guestbook”。\n$ kubebuilder create api --group webapp --version v1 --kind Guestbook 安装 kustomize # cd 到当前项目目录的 bin 目录下 $ cd bin # go version≥go1.17 $ GOBIN=$(pwd)/ GO111MODULE=on go install sigs.k8s.io/kustomize/kustomize/v4@latest # 查看当前目录下是否下载好 kustomize $ ls controller-gen kustomize 测试 将 CRD 安装到集群中\n这步操作需要你已经安装好 kustomize\n$ make install 运行控制器：\n$ make run go fmt ./... go vet ./... go run ./main.go 1.664805588618038e+09 INFO controller-runtime.metrics Metrics server is starting to listen {\u0026quot;addr\u0026quot;: \u0026quot;:8080\u0026quot;} 1.664805588619189e+09 INFO setup starting manager 1.664805588620343e+09 INFO Starting server {\u0026quot;path\u0026quot;: \u0026quot;/metrics\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;metrics\u0026quot;, \u0026quot;addr\u0026quot;: \u0026quot;[::]:8080\u0026quot;} 1.664805588620474e+09 INFO Starting server {\u0026quot;kind\u0026quot;: \u0026quot;health probe\u0026quot;, \u0026quot;addr\u0026quot;: \u0026quot;[::]:8081\u0026quot;} 1.664805588620887e+09 INFO controller.guestbook Starting EventSource {\u0026quot;reconciler group\u0026quot;: \u0026quot;webapp.my.domain\u0026quot;, \u0026quot;reconciler kind\u0026quot;: \u0026quot;Guestbook\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;kind source: *v1.Guestbook\u0026quot;} 1.664805588620948e+09 INFO controller.guestbook Starting Controller {\u0026quot;reconciler group\u0026quot;: \u0026quot;webapp.my.domain\u0026quot;, \u0026quot;reconciler kind\u0026quot;: \u0026quot;Guestbook\u0026quot;} 1.6648055887228339e+09 INFO controller.guestbook Starting workers {\u0026quot;reconciler group\u0026quot;: \u0026quot;webapp.my.domain\u0026quot;, \u0026quot;reconciler kind\u0026quot;: \u0026quot;Guestbook\u0026quot;, \u0026quot;worker count\u0026quot;: 1} 运行这个 CRD：\n$ kubectl apply -f config/samples/ guestbook.webapp.my.domain/guestbook-sample created $ kubectl get Guestbook NAME AGE guestbook-sample 33s 实践 继续沿用 快速入门 里已经创建好的项目\n注意这里的 group 必须要指定为和 快速入门 中相同的 webapp，如果指定为别的，会报错：Error: failed to create API: unable to inject the resource to \u0026quot;base.go.kubebuilder.io/v3\u0026quot;: multiple groups are not allowed by default, to enable multi-group visit https://kubebuilder.io/migration/multi-group.html\n$ kubebuilder create api --group webapp --version v1 --kind CronJob ","date":"2022年10月03日","permalink":"/posts/kubebuilder/","summary":"快速入门 下载 kubebuiler $ os=$(go env GOOS) $ arch=$(go env GOARCH) $ curl -L -o kubebuilder https://go.","title":"kubebuilder 实践"},{"contents":"docker tag 相当于对镜像做一次备份，将当前的镜像（比如 tag:latest）备份出去一份（比如 tag: v1），然后如果开发了新的镜像，可以直接覆盖掉当前的（tag:latest），然后再给 latest 打个标签：tag:v2，相当于又备份了一份，现在就有了 v1，v2，latest 三个镜像，使用 tag 不仅可以备份镜像，还可以对镜像做标注区分。\n详细可以看下面的例子：\n$ mkdir hello-world-go $ vim Dockerfile # 写入以下内容 FROM golang:alpine AS builder # 为我们的镜像设置必要的环境变量 ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=arm64 # 移动到工作目录：/build WORKDIR /build # 将代码复制到容器中 COPY . . # 将我们的代码编译成二进制可执行文件 app RUN go build -o app . ################### # 接下来创建一个小镜像 ################### FROM scratch # 从builder镜像中把/dist/app 拷贝到当前目录 COPY --from=builder /build/app / # 需要运行的命令 ENTRYPOINT [\u0026quot;/app\u0026quot;] $ vim hello.go # 写入以下内容 package main import ( \u0026quot;fmt\u0026quot; ) func main() { fmt.Println(\u0026quot;[v1] hello, world!\u0026quot;) } # 创建 go.mod $ go mod init helloworld go: creating new go.mod: module helloworld # 构建镜像 $ docker build -t hello-world . Sending build context to Docker daemon 4.608kB Step 1/8 : FROM golang:alpine AS builder ---\u0026gt; 97a79d0cc013 Step 2/8 : ENV GO111MODULE=on CGO_ENABLED=0 GOOS=linux GOARCH=arm64 ---\u0026gt; Using cache ---\u0026gt; accc05f16e78 Step 3/8 : WORKDIR /build ---\u0026gt; Using cache ---\u0026gt; 3cbd504d7581 Step 4/8 : COPY . . ---\u0026gt; 2bb03068056c Step 5/8 : RUN go build -o app . ---\u0026gt; Running in 6077f4d304a4 Removing intermediate container 6077f4d304a4 ---\u0026gt; c9509017f175 Step 6/8 : FROM scratch ---\u0026gt; Step 7/8 : COPY --from=builder /build/app / ---\u0026gt; d97340567693 Step 8/8 : ENTRYPOINT [\u0026quot;/app\u0026quot;] ---\u0026gt; Running in 7f5508753fa9 Removing intermediate container 7f5508753fa9 ---\u0026gt; 1b57ca8ce7d2 Successfully built 1b57ca8ce7d2 Successfully tagged hello-world:latest # 查看镜像 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest 1b57ca8ce7d2 4 seconds ago 1.82MB # 给镜像打个 tag $ docker tag 1b57 hello-world:v1 # 再次查看镜像，发现已经有了一个新的 tag 为 v1 的镜像，并且 ID 和 latest 相同，相当于有了一份备份 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest 1b57ca8ce7d2 2 minutes ago 1.82MB hello-world v1 1b57ca8ce7d2 2 minutes ago 1.82MB # 运行镜像查看效果 $ docker run hello-world:v1 hello, world! # 现在可以再次开发镜像了，修改 hello.go，将输出内容修改为 [v2] $ vim hello.go package main import ( \u0026quot;fmt\u0026quot; ) func main() { fmt.Println(\u0026quot;[v2] hello, world!\u0026quot;) } # 再次构建新版本的镜像 $ docker build -t hello-world . Sending build context to Docker daemon 4.608kB Step 1/8 : FROM golang:alpine AS builder ---\u0026gt; 97a79d0cc013 Step 2/8 : ENV GO111MODULE=on CGO_ENABLED=0 GOOS=linux GOARCH=arm64 ---\u0026gt; Using cache ---\u0026gt; accc05f16e78 Step 3/8 : WORKDIR /build ---\u0026gt; Using cache ---\u0026gt; 3cbd504d7581 Step 4/8 : COPY . . ---\u0026gt; Using cache ---\u0026gt; 6ee29a59885a Step 5/8 : RUN go build -o app . ---\u0026gt; Running in 9e6388ccd40c Removing intermediate container 9e6388ccd40c ---\u0026gt; 78cb76e165de Step 6/8 : FROM scratch ---\u0026gt; Step 7/8 : COPY --from=builder /build/app / ---\u0026gt; 9fbd2d306353 Step 8/8 : ENTRYPOINT [\u0026quot;/app\u0026quot;] ---\u0026gt; Running in 49e410971798 Removing intermediate container 49e410971798 ---\u0026gt; 245cbeaf290f Successfully built 245cbeaf290f Successfully tagged hello-world:latest # 再次查看镜像，新的镜像覆盖掉了之前的 hello-world:latest $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest 245cbeaf290f 11 seconds ago 1.82MB hello-world v1 1b57ca8ce7d2 37 minutes ago 1.82MB # 查看新镜像的运行效果 $ docker run hello-world [v2] hello, world! # 再次给新镜像打 tag，相当于做一次备份 $ docker tag 245cbeaf290f hello-world:v2 # 查看现在的 iamge $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest 245cbeaf290f 49 seconds ago 1.82MB hello-world v2 245cbeaf290f 49 seconds ago 1.82MB hello-world v1 1b57ca8ce7d2 38 minutes ago 1.82MB # 查看新镜像的运行效果 $ docker run hello-world:v2 [v2] hello, world! $ docker run hello-world:v1 [v1] hello, world! # 现在就已经有了两个版本的镜像了 ","date":"2022年09月29日","permalink":"/posts/docker-tag/","summary":"docker tag 相当于对镜像做一次备份，将当前的镜像（比如 tag:latest）备份出去一份（比如 tag: v1），然后如果开发了新的镜像，可以直接覆盖掉当前的（tag:latest），然后再给 latest 打个标签：tag:v2，相当于又备份了一份，现在就有了 v1，v2，latest 三个镜像，使用 tag 不仅可以备份镜像，还可以对镜像做标注区分。","title":"docker tag 的用处"},{"contents":"创建一个 handless service：\nkubia-service-headless.yaml\napiVersion: v1 kind: Service metadata: name: kubia spec: clusterIP: None selector: app: kubia ports: - name: http port: 80 运行：\n$ kubectl apply -f kubia-service-headless.yaml service/kubia created $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 32h kubia ClusterIP None \u0026lt;none\u0026gt; 80/TCP 9s 准备一个程序用来测试，这是一个 HTTP 程序，如果发送 POST 请求，那么会在指定目录下创建/打开一个文件，并将请求体内容写入该文件；如果发送 GET 请求，会在指定目录下读取文件内容，并作为 response 返回：\nconst http = require('http'); const os = require('os'); const fs = require('fs'); const dataFile = \u0026quot;/var/data/kubia.txt\u0026quot;; function fileExists(file) { try { fs.statSync(file); return true; } catch (e) { return false; } } var handler = function(request, response) { if (request.method == 'POST') { var file = fs.createWriteStream(dataFile); file.on('open', function (fd) { request.pipe(file); console.log(\u0026quot;New data has been received and stored.\u0026quot;); response.writeHead(200); response.end(\u0026quot;Data stored on pod \u0026quot; + os.hostname() + \u0026quot;\\n\u0026quot;); }); } else { var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : \u0026quot;No data posted yet\u0026quot;; response.writeHead(200); response.write(\u0026quot;You've hit \u0026quot; + os.hostname() + \u0026quot;\\n\u0026quot;); response.end(\u0026quot;Data stored on this pod: \u0026quot; + data + \u0026quot;\\n\u0026quot;); } }; var www = http.createServer(handler); www.listen(8080); 对应的 Dockerfile，注意这里使用的是 arm64 类型的镜像：\nFROM arm64v8/node:7 ADD app.js /app.js ENTRYPOINT [\u0026quot;node\u0026quot;, \u0026quot;app.js\u0026quot;] kubia-statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: kubia spec: serviceName: kubia replicas: 2 selector: matchLabels: app: kubia # has to match .spec.template.metadata.labels template: metadata: labels: app: kubia spec: containers: - name: kubia image: stdoutt/kubia-pet-arm64 # 注意这里更换成了 arm64 版本的 ports: - name: http containerPort: 8080 volumeMounts: - name: data mountPath: /var/data volumeClaimTemplates: - metadata: name: data spec: resources: requests: storage: 1Mi accessModes: - ReadWriteOnce 运行：\n$ kubectl apply -f kubia-statefulset.yaml statefulset.apps/kubia created $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-0 1/1 Running 0 61s kubia-1 1/1 Running 0 26s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-path-pvc Bound pvc-0e7db0b6-fa32-4c2e-ac0c-53a354ead8a4 50Mi RWO local-path 21h data-kubia-0 Bound pvc-b2742307-c5d5-4402-a9a6-54730d60c4a3 1Mi RWO local-path 62m data-kubia-1 Bound pvc-28570e23-c08a-4610-8b6e-cc35434ded21 1Mi RWO local-path 61m 使用 API server 访问 pod：\n$ kubectl proxy Starting to serve on 127.0.0.1:8001 $ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/ You've hit kubia-0 Data stored on this pod: No data posted yet 本次发送的是 GET 请求，因为还没有发送过 POST 请求，所以文件还不存在，所以返回 No data posted yet\n发送一个 POST 请求：\n$ curl -X POST -d \u0026quot;Hey there! This greeting was submitted to kubia-0.\u0026quot; localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/ Data stored on pod kubia-0 此时已经将请求体里的内容保存在了容器内\n再次发送 GET 请求：\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/ You've hit kubia-0 Data stored on this pod: Hey there! This greeting was submitted to kubia-0. 再次发送请求，发现可以输出文件内容了\n现在删除一个 statefulset，看看其保存的数据会不会丢失：\n$ kubectl delete po kubia-0 pod \u0026quot;kubia-0\u0026quot; deleted $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-1 1/1 Running 0 12h kubia-0 1/1 Running 0 27s $ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/ You've hit kubia-0 Data stored on this pod: Hey there! This greeting was submitted to kubia-0. 发现该 po 被删除后立马就重新创建出了一个新的，并且名字与与之前被删除的 pod 完全相同，并且其存储的数据也没有丢失\n在 statefulset 中发现伙伴节点 之前以已经创建了一个 handless service，通过解析（lookup）这个 service 的 dns 便可获取其管辖的所有 pod 的 ip，那么 pod 就可以通过这种方式来获取同属于一个 service 下的其他 pod 的地址，进而完成通信。下面就来实践一下：\n首先来尝试一下获取 handless service 下所有 pod 的 SRV 记录：\n什么是 SRV 记录？\n查看 handless service 的 SRV 记录：\nPS: 书里写的是 \u0026ndash;image=tutum/dnstuils，但是这个镜像是 amd64 的，所以我重新做了一个镜像：stdoutt/dnstuils-arm64\n附：该镜像的 Dockerfile\nFROM ubuntu:trusty RUN apt-get update \u0026amp;\u0026amp; apt-get install -yq dnsutils \u0026amp;\u0026amp; apt-get clean \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists 具体操作：\n# 先看一下 handless service 的 name $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 2d3h kubia ClusterIP None \u0026lt;none\u0026gt; 80/TCP 12s # 查看 kubia 这个 service 的 SRV 记录（该 service 的完整域名是 kubia.default.svc.cluster.local ），这个 pod 在 # 运行完毕后会立马删除，这里使用的是 stdoutt/dnsutils-arm64 这个适用于 arm64 的镜像 $ kubectl run -it srvlookup --image=stdoutt/dnsutils-arm64 --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.9.5-3ubuntu0.19-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; SRV kubia.default.svc.cluster.local ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 24375 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 3 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;kubia.default.svc.cluster.local. IN\tSRV ;; ANSWER SECTION: kubia.default.svc.cluster.local. 5 IN\tSRV\t0 50 80 kubia-0.kubia.default.svc.cluster.local. kubia.default.svc.cluster.local. 5 IN\tSRV\t0 50 80 kubia-1.kubia.default.svc.cluster.local. # 这里便获取到了该 service 下的所有 pod 的 ip 地址 ;; ADDITIONAL SECTION: kubia-1.kubia.default.svc.cluster.local. 5 IN A\t10.42.0.22 kubia-0.kubia.default.svc.cluster.local. 5 IN A\t10.42.0.23 ;; Query time: 12 msec ;; SERVER: 10.43.0.10#53(10.43.0.10) ;; WHEN: Mon Sep 26 16:37:29 UTC 2022 ;; MSG SIZE rcvd: 350 pod \u0026quot;srvlookup\u0026quot; deleted ","date":"2022年09月11日","permalink":"/posts/k8s-statefulset/","summary":"创建一个 handless service：\nkubia-service-headless.yaml\napiVersion: v1 kind: Service metadata: name: kubia spec: clusterIP: None selector: app: kubia ports: - name: http port: 80 运行：","title":"k8s StatefulSet"},{"contents":"使用 snap 可以比较方便的安装 docker，但是有一次出现了 k8s pod 无法拉取镜像的问题，看了下 pod describe，发现是因为走代理的问题（我使用的是 ubuntu 虚拟机，连接宿主机的 clashx 进行代理），连接的代理是宿主机的老地址 192.168.2.5，但是因为重启过路由器，所以宿主机的地址变更为了 192.168.2.3，代理不通自然也拉取不了镜像了，报错：\nFailed to pull image \u0026quot;redis\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: Get \u0026quot;https://registry-1.docker.io/v2/\u0026quot;: proxyconnect tcp: dial tcp 192.168.2.5:7890: connect: connection refused 虽然我重新设置了 https_proxy 等相关环境变量，并且 curl google 可以正常返回，但是 pod 这边依旧无法拉取，走的还是 192.168.2.5 这个地址。\n于是我就尝试重启一下 docker 看能不能解决，按照网上提供的几个方案，systemctl restart docker，报错：Failed to restart docker.service: Unit docker.service not found.，于是我又尝试使用 snap restart docker，发现可以重启成功，输出信息：\nRun service command \u0026quot;restart\u0026quot; for running services of snap \u0026quot;docker\u0026quot; Restarted. 看起来已经重启成功了，但是等我执行 docker ps 时，直接报错：docker ps Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?，根本就没重启成功！于是我又尝试执行 snap start docker 命令，返回 Started.，但是然并卵，依然报之前的错误，实属是个坑爹玩意。\n没得办法，只能从 snap 中删掉 docker，再重新使用 apt-get 安装了，但是 apt-get 安装就略有繁琐了，参照 这篇文章 ：\n$ sudo apt update $ sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - # 注意这一步，arch 这里，要根据你的机器架构来指定，比如 arm64 或者是 amd64，我一开始就是直接复制文章中的命令，导致加入的是 # amd64 的软件源，而我的机器是 arm 的，导致后面安装 docker 失败 # 可以使用 uname -c 查看机器的架构 $ sudo add-apt-repository \u0026quot;deb [arch=arm64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; $ sudo apt update $ sudo apt install docker-ce docker-ce-cli containerd.io 使用 apt-get 安装的 docker 就可以使用 systemctl 操作了。\n$ systemctl restart docker ","date":"2022年09月10日","permalink":"/posts/ubuntu-arm64-an-zhuang-docker/","summary":"使用 snap 可以比较方便的安装 docker，但是有一次出现了 k8s pod 无法拉取镜像的问题，看了下 pod describe，发现是因为走代理的问题（我使用的是 ubuntu 虚拟机，连接宿主机的 clashx 进行代理），连接的代理是宿主机的老地址 192.","title":"ubuntu arm64 使用 apt-get 安装 docker，以及 snap 的一些问题记录"},{"contents":"环境：\n机器：MacBook Air (M1, 2020) 系统：MacOS 12.5.1 Goland 版本：2022.2.2 Idea 版本：2022.1.1 今天启动 GoLand 发现会立马闪退，提示程序意外退出，后来又试了下 Idea 和 Clion，发现全部都是如此，尝试在终端启动查看错误信息：\n$ cd ~/Library/Application\\ Support/JetBrains/Toolbox/apps/Goland/ch-0/222.3739.57/GoLand.app/Contents/MacOS $ ./goland 2022-09-07 22:53:36.422 goland[12904:58283] allVms required 1.8*,1.8+ 2022-09-07 22:53:36.425 goland[12904:58286] Current Directory: /Users/zenghao/Library/Application Support/JetBrains/Toolbox/apps/Goland/ch-0/222.3739.57/GoLand.app/Contents/MacOS 2022-09-07 22:53:36.425 goland[12904:58286] parseVMOptions: GOLAND_VM_OPTIONS = /Users/zenghao/Downloads/jetbra/vmoptions/goland.vmoptions 2022-09-07 22:53:36.428 goland[12904:58286] parseVMOptions: platform=-1 user=-1 file=(null) 2022-09-07 22:53:36.531 goland[12904:58286] *** Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'NSWindow drag regions should only be invalidated on the Main Thread!' *** First throw call stack: ( 0 CoreFoundation 0x00000001c26951a8 __exceptionPreprocess + 240 1 libobjc.A.dylib 0x00000001c23dfe04 objc_exception_throw + 60 2 CoreFoundation 0x00000001c26c0128 _CFBundleGetValueForInfoKey + 0 3 AppKit 0x00000001c51a3930 -[NSWindow(NSWindow_Theme) _postWindowNeedsToResetDragMarginsUnlessPostingDisabled] + 372 4 AppKit 0x00000001c518e92c -[NSWindow _initContent:styleMask:backing:defer:contentView:] + 948 5 AppKit 0x00000001c533607c -[NSPanel _initContent:styleMask:backing:defer:contentView:] + 48 6 AppKit 0x00000001c518e56c -[NSWindow initWithContentRect:styleMask:backing:defer:] + 56 7 AppKit 0x00000001c5336030 -[NSPanel initWithContentRect:styleMask:backing:defer:] + 48 8 AppKit 0x00000001c518cd94 -[NSWindowTemplate nibInstantiate] + 292 9 AppKit 0x00000001c51579e8 -[NSIBObjectData instantiateObject:] + 236 10 AppKit 0x00000001c515726c -[NSIBObjectData nibInstantiateWithOwner:options:topLevelObjects:] + 392 11 AppKit 0x00000001c514b800 loadNib + 416 12 AppKit 0x00000001c514ad30 +[NSBundle(NSNibLoading) _loadNibFile:nameTable:options:withZone:ownerBundle:] + 800 13 AppKit 0x00000001c514a934 -[NSBundle(NSNibLoading) loadNibNamed:owner:topLevelObjects:] + 220 14 AppKit 0x00000001c54b4674 -[NSAlert init] + 148 15 goland 0x0000000100415270 -[Launcher buildArgsFor:] + 1144 16 goland 0x0000000100415878 -[Launcher launch] + 312 17 Foundation 0x00000001c34f060c __NSThread__start__ + 808 18 libsystem_pthread.dylib 0x00000001c254826c _pthread_start + 148 19 libsystem_pthread.dylib 0x00000001c254308c thread_start + 8 ) libc++abi: terminating with uncaught exception of type NSException [1] 12904 abort ./goland 发现报错信息是 NSInternalInconsistencyException，最终找到了 解决方法\n这个 issue 下面有一条回复：\nhey I could fix this issue with following instructions:\nopen this file /Users/{USER_NAME}/Library/LaunchAgents/jetbrains.vmoptions.plist\nand then remove all launchctl setenv \u0026quot;*_OPTIONS\u0026quot;.\nsave and close it.\nreboot your mac.\nnow you can use jetbrain :)\n流程：\n# 1. $ cd ~/Library/LaunchAgents # 2. $ open jetbrains.vmoptions.plist # 3. # 删除所有以 launchctl setenv 开头的行，也就是除第一行和最后一行外的所有内容 # 4. # 重启 mac # 5. # 大功告成 不知道是什么原因导致的，个人感觉大概率是因为用了破解的缘故，按照上面的流程重启以后会发现认证信息已经没有了，需要重新执行破解脚本并输入激活码。\n","date":"2022年09月08日","permalink":"/posts/jetbrain_crash/","summary":"环境：\n机器：MacBook Air (M1, 2020) 系统：MacOS 12.","title":"GoLand/Idea 启动闪退，报错 NSInternalInconsistencyException 的解决方法"},{"contents":"介绍 veth-pair 是什么 两个 namespace 间通信 直接连接 # 创建 2 个 namespace $ ip netns add ns1 $ ip netns add ns2 # 查看创建的 namespace $ ip netns ns2 ns1 # 创建一对 veth-pair veth0 veth1 $ ip l a veth0 type veth peer name veth1 # 也可以写成： # ip link add veth0 type veth peer name veth1 # 将 veth0 veth1 分别加入两个 ns $ ip l s veth0 netns ns1 $ ip l s veth1 netns ns2 # 给两个 veth0 veth1 配上 IP 并启用 $ ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0 $ ip netns exec ns1 ip l s veth0 up $ ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1 $ ip netns exec ns2 ip l s veth1 up # 从 veth0 ping veth1 $ ip netns exec ns1 ping 10.1.1.3 PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data. 64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.997 ms 64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.102 ms 64 bytes from 10.1.1.3: icmp_seq=3 ttl=64 time=0.063 ms ^C --- 10.1.1.3 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2026ms rtt min/avg/max/mdev = 0.063/0.387/0.997/0.431 ms 参考 https://www.cnblogs.com/bakari/p/10613710.html\nhttps://segmentfault.com/a/1190000009251098\nhttps://mp.weixin.qq.com/s/dP12ptPlsxS35qPr0mDjCw\n","date":"2022年09月06日","permalink":"/posts/linux_veth_pair/","summary":"介绍 veth-pair 是什么 两个 namespace 间通信 直接连接 # 创建 2 个 namespace $ ip netns add ns1 $ ip netns add ns2 # 查看创建的 namespace $ ip netns ns2 ns1 # 创建一对 veth-pair veth0 veth1 $ ip l a veth0 type veth peer name veth1 # 也可以写成： # ip link add veth0 type veth peer name veth1 # 将 veth0 veth1 分别加入两个 ns $ ip l s veth0 netns ns1 $ ip l s veth1 netns ns2 # 给两个 veth0 veth1 配上 IP 并启用 $ ip netns exec ns1 ip a a 10.","title":"Linux 虚拟网络设备 veth-pair [未完]"},{"contents":"今天更新了一下之前写的 tcp server，结果测试的时候发现了一个十分诡异的 BUG：客户端这边发送完数据，进程都已经退出了，但是服务端却依然源源不断的接收到客户端发来的数据，而且数据包的大小都为 0，这个 BUG 花费了我大半天的时间，终于找到了问题所在，特此记录。\n问题关键就是对 io.EOF 的判断逻辑出了问题，以下是部分代码：\n错误：\nfunc (t *TCPConn) UnpackHeader(d *DataPack) (*Message, error) { head := make([]byte, d.HeadSize()) _, err := io.ReadFull(t.socketConn, head) if err != nil \u0026amp;\u0026amp; err != io.EOF { log.Println(\u0026quot;read head error: \u0026quot;, err) return nil, err } body, err := d.UnPack(head) if err != nil { return nil, err } return body, nil } func (t *TCPConn) Receive() (*Message, error) { pack := NewDataPack() header, err := t.UnpackHeader(pack) if err != nil { return nil, err } dataLen := header.dataLen msgType := header.typ } UnpackHeader 这里的判断逻辑是 if err != nil \u0026amp;\u0026amp; err != io.EOF，问题就出在这里，如果 err 是 EOF，那么就不满足条件，也就不会 return err，而是继续走下面的代码，最终的 error return 为 nil。EOF 代表已经无数据可读了，所以返回的 Message 为空，没有读到任何东西。\n然后 Receive 又调用了 UnpackHeader，因为 err == nil，所以继续执行到 dataLen := header.dataLen 这里，又因为 EOF 导致 dataLen == 0，这便是为何调试时发现数据包大小为 0 的原因。\n而因为没有对 EOF 错误进行 return error，导致 Receive 这边一直返回空包，而我又在另一个函数 Handle 中通过一个死循环来持续调用 Receive，这就导致服务端可以源源不断的接收到客户端发来的数据，实际上这是因为没有对 EOF 进行合理的处理。\n正确：\nfunc (t *TCPConn) UnpackHeader(d *DataPack) (*Message, error) { head := make([]byte, d.HeadSize()) _, err := io.ReadFull(t.socketConn, head) if err != nil { if err != io.EOF { log.Println(\u0026quot;read head error: \u0026quot;, err) } return nil, err } body, err := d.UnPack(head) if err != nil { return nil, err } return body, nil } 只需要将判断逻辑修改一下即可：\nif err != nil { if err != io.EOF { log.Println(\u0026quot;read head error: \u0026quot;, err) } return nil, err } 只要产生了错误就必须 return err，如果不是 EOF 的话要额外输出错误信息。\n","date":"2022年09月06日","permalink":"/posts/eof_cuo_wu/","summary":"今天更新了一下之前写的 tcp server，结果测试的时候发现了一个十分诡异的 BUG：客户端这边发送完数据，进程都已经退出了，但是服务端却依然源源不断的接收到客户端发来的数据，而且数据包的大小都为 0，这个 BUG 花费了我大半天的时间，终于找到了问题所在，特此记录。","title":"一个 EOF 引发的低级错误"},{"contents":"简介 tcp 的粘包拆包也属于老生常谈的问题了，虽然不少人都鄙夷的认为粘包是一个错误的说法，因为 tcp 是面向流传输的，但无论如何，粘包这个词都已经算得上是深入人心了。\ntcp 的流式传输，简单的说就是没有消息边界记录，比如发送端为了将多个发往接收端的包，更有效的发到对方，使用了优化方法（Nagle算法），将多次间隔较小、数据量小的数据包，合并成一个大的数据包发送(把发送端的缓冲区填满一次性发送。这样接收端收到的整个数据中其实是包含了多个小包，需要将数据拆分，还原成小包，从而才能进行处理，但因为没有消息边界记录，所以如何还原就成了一个问题。\n在上面的例子中，tcp 将多次写入缓冲区的数据包合并为一次发送，就是 粘包 问题。\n此外 tcp 还存在 拆包 问题，就是发送包的大小超过了缓冲区大小或者 MSS，比如发送端使用 protobuf 编码数据，然后使用 tcp 进行传输，这个编码的数据大小是 10000 byte，超过了 tcp 的发送缓存区大小 5000 byte（随便写的数字），一次性无法全部发送，所以便会发生拆包，分两次发送，接收端这边第一次只收到了 5000 byte 数据，因为数据使用 protobuf 编码的，不同于文本消息，必须要拿到不多不少刚刚好的 10000 byte 大小的数据，才能反序列化出原始的数据，但因为 tcp 不记录消息长度，所以接收端这边无法知道到底要拿多少数据，这也是一个问题。\n可以看到发生粘包和拆包的原因都是因为 tcp 不记录消息边界（也就是消息长度），既然 tcp 自身不记录，那就只能用户自己解决了，比如写一个协议，为消息添加上消息头，在消息头中记录消息的长度，然后将消息头 + 消息本体作为一个整体发送，（这里的消息头是需要定长的，比如固定为 10 byte），接收端这边根据协议定义，去读取这个定长的消息头，从里面获取到消息本体的长度，现在知道了消息的长度就好解决上面的问题了。\n拆包演示程序 下面先使用一个错误的 protobuf 的示例程序来演示一下拆包问题：\nserver 端 server 负责读取报文，然后通过 proto.Unmarshal 还原出原始的数据。其中将缓冲区大小设置为了 client 要发送的大小 279896。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net\u0026quot; \u0026quot;protobuftest/pb/pbfile\u0026quot; \u0026quot;google.golang.org/protobuf/proto\u0026quot; ) func main() { lis, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;:7788\u0026quot;) assert(err, \u0026quot;listen error: %v\u0026quot;) for { conn, err := lis.Accept() if err != nil { fmt.Println(err) continue } for { // b := make([]byte, 1024)\t// proto: cannot parse invalid wire-format data b := make([]byte, 279896) n, err := conn.Read(b) if err != nil { fmt.Println(err) break } fmt.Printf(\u0026quot;read %v bytes\\n\u0026quot;, n) var m pbfile.Message if err := proto.Unmarshal(b, \u0026amp;m); err != nil { fmt.Println(err) break } fmt.Println(m.Uid) } } } func assert(err error, format string) { if err != nil { panic(fmt.Sprintf(format, err.Error())) } } client 端 client 使用 protobuf 序列化 struct pbfile.Message，然后使用 tcp 发送，这个 struct 中有一个 Data 字段，这里我将一张 140k 的图片读取出来，然后将 Data 字段设置为读取出的 []byte 。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;google.golang.org/protobuf/proto\u0026quot; \u0026quot;google.golang.org/protobuf/types/known/wrapperspb\u0026quot; \u0026quot;net\u0026quot; \u0026quot;os\u0026quot; \u0026quot;path/filepath\u0026quot; \u0026quot;protobuftest/pb/pbfile\u0026quot; ) func main() { home := os.Getenv(\u0026quot;HOME\u0026quot;) fp := filepath.Join(home, \u0026quot;Downloads/jDKUARa.jpg\u0026quot;) f, err := os.Open(fp) assert(err, \u0026quot;open file error: %v\u0026quot;) defer f.Close() file, err := os.ReadFile(fp) assert(err, \u0026quot;read file error: %v\u0026quot;) msg := pbfile.Message{ Uid: wrapperspb.UInt64(10086), Data: file, MessageType: wrapperspb.String(\u0026quot;image\u0026quot;), } conn, err := net.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;:7788\u0026quot;) assert(err, \u0026quot;dial error: %v\u0026quot;) defer conn.Close() b, err := proto.Marshal(\u0026amp;msg) assert(err, \u0026quot;proto marshal error: %v\u0026quot;) fmt.Printf(\u0026quot;proto marshal size: %v\\n\u0026quot;, len(b)) n, err := conn.Write(b) assert(err, \u0026quot;write to conn error: %v\u0026quot;) fmt.Printf(\u0026quot;write %v bytes\\n\u0026quot;, n) } func assert(err error, format string) { if err != nil { panic(fmt.Sprintf(format, err.Error())) } } proto syntax=\u0026quot;proto3\u0026quot;; option go_package = \u0026quot;/pbfile\u0026quot;; import \u0026quot;google/protobuf/wrappers.proto\u0026quot;; message Message { google.protobuf.UInt64Value uid = 1; bytes data = 2; google.protobuf.StringValue messageType = 3; } 运行结果 分别运行 server 和 client，输出：\n# client $ ./client proto marshal size: 279896 write 279896 bytes # server $ ./server read 81660 bytes proto: cannot parse invalid wire-format data 可以看到 client 这边发送了 279896 字节的数据，但是 server 这边只读到了 81660 字节，因为 protobuf 使用二进制编码，所以如果数据不完整会无法解析，从而报错：proto: cannot parse invalid wire-format data\n部分测试结果可以印证这一点，只要读取的长度不等于 279896 就会解析失败：\nread 97992 bytes proto: cannot parse invalid wire-format data read 81660 bytes proto: cannot parse invalid wire-format data read 130656 bytes proto: cannot parse invalid wire-format data 但是有时候又是正常的：\n# client $ ./client proto marshal size: 279896 write 279896 bytes # server $ read 279896 bytes value:10086 EOF 疑问 这里有个疑问，既然 server 的缓冲区已经将长度设置为了 279896，刚好等于 client 要发送的包的大小，但是为什么会出现没有读取满的情况？\n可能是因为：发送方虽然发送了 279896 长度的数据，但是这些数据不会直接通过网络传输，而是先添加到 socket 发送缓冲区，至于什么时候发送，发送多少，都是由系统内核来决定的，所以会出现诸如上面这种没有完全发完的情况，偶尔又会出现全部发送的情况。\n粘包演示程序 server 服务端试图从收到的包中反序列化出原始数据。\nfunc main() { lis, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;:7788\u0026quot;) assert(err, \u0026quot;listen error: %v\u0026quot;) for { conn, err := lis.Accept() if err != nil { fmt.Println(err) continue } for { b := make([]byte, 1024) n, err := conn.Read(b) if err != nil { fmt.Println(err) break } fmt.Printf(\u0026quot;read %v bytes\\n\u0026quot;, n) var m pbfile.Message if err := proto.Unmarshal(b, \u0026amp;m); err != nil { fmt.Println(err) break } fmt.Println(m.Uid) } } } client 客户端发送 10 条消息，每条消息设置不同的 Uid，且序列化的对象都比较小。\nfunc main() { conn, err := net.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;:7788\u0026quot;) assert(err, \u0026quot;dial error: %v\u0026quot;) defer conn.Close() var totalN int for i := 0; i \u0026lt; 10; i++ { msg := pbfile.Message{ Uid: wrapperspb.UInt64(uint64(i)), Data: []byte(\u0026quot;hello\u0026quot;), MessageType: wrapperspb.String(\u0026quot;text\u0026quot;), } b, err := proto.Marshal(\u0026amp;msg) assert(err, \u0026quot;proto marshal error: %v\u0026quot;) fmt.Printf(\u0026quot;proto marshal size: %v\\n\u0026quot;, len(b)) n, err := conn.Write(b) totalN += n assert(err, \u0026quot;write to conn error: %v\u0026quot;) fmt.Printf(\u0026quot;write %v bytes\\n\u0026quot;, n) } fmt.Printf(\u0026quot;total write %v bytes\\n\u0026quot;, totalN) } 运行结果 $ go run server.go read 74 bytes proto: cannot parse invalid wire-format data $ go run client.go proto marshal size: 17 write 17 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes proto marshal size: 19 write 19 bytes total write 188 bytes 可以看到客户端这边每次发送的报文长度基本都是 19 ，只有第一次是 17，一共发送了 188 bytes 的数据，而服务端这边，第一次从连接中接收到的数据大小是 74 bytes，这显然是有几个包 “粘” 在了一起，最终显然也是无法反序列化成功的。\n解决方法 我写了一个简单的 tcp server 库，里面解决了上面的粘包拆包问题，思路是自定义一个协议，协议的头部记录了数据的长度，然后接收端在收到报文后，先解析出头部，获取到数据的长度，在进行下一步读取。为了保证读满，可以使用 io.ReadFull 这个函数，大致的读取逻辑如下：\n首先是读取协议头部的函数：\nfunc (t *TCPConn) UnpackHeader(d *DataPack) (*Message, error) { head := make([]byte, d.HeadSize()) _, err := io.ReadFull(t.socketConn, head) if err != nil { if err != io.EOF { log.Println(\u0026quot;read head error: \u0026quot;, err) } return nil, err } body, err := d.UnPack(head) if err != nil { return nil, err } return body, nil } 然后是读取数据：\nfunc (t *TCPConn) Receive() (*Message, error) { pack := NewDataPack() header, err := t.UnpackHeader(pack) if err != nil { return nil, err } dataLen := header.dataLen var ( tmpBuf = make([]byte, 0, 4096) needN = dataLen // 还剩多少需要读取 readN int64 // 总共需要读取多少 ) for { buf := make([]byte, needN) // 必须读满 len(buf)，否则返回一个 err n, err := io.ReadFull(t.socketConn, buf) // 没有读满 buf if err != nil { // 虽然没读满，但是返回了 EOF 错误，说明没有数据可读了，可能是对方已经断开了连接， // 此时就不需要再尝试读取了 if err == io.EOF { return nil, err } // 将当前读的这部分添加到 tmp 中，暂时保存 tmpBuf = append(tmpBuf, buf[:n]...) needN -= uint64(n) // 更新 needN 的值 readN += int64(n) continue } // 读满了 tmpBuf = append(tmpBuf, buf...) readN += int64(n) if readN == int64(dataLen) { break } } m := NewMessage(tmpBuf, msgType) return m, nil } 测试了前面的粘包拆包代码，发现结果都正确，说明可以解决这类问题，但是目前测试的依据较少，也可能不一定准确。\n附：\n项目地址\n用 protobuf 来进行粘包拆包的测试代码\n","date":"2022年09月05日","permalink":"/posts/tcp_zhan_bao/","summary":"简介 tcp 的粘包拆包也属于老生常谈的问题了，虽然不少人都鄙夷的认为粘包是一个错误的说法，因为 tcp 是面向流传输的，但无论如何，粘包这个词都已经算得上是深入人心了。","title":"tcp 粘包/拆包问题"},{"contents":"看看下面这段代码的运行结果是什么？\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net\u0026quot; ) func main() { lis, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;:7788\u0026quot;) assert(err, \u0026quot;listen error: %v\u0026quot;, err.Error()) _ = lis } func assert(err error, format, msg string) { if err != nil { panic(fmt.Sprintf(format, msg)) } } 细心眼尖的你一定立马想到了：\n$ go run nil_pointer.go panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x2 addr=0x18 pc=0x100412238] goroutine 1 [running]: main.main() /Users/zenghao/pj/justtest/nil_pointer.go:10 +0x38 exit status 2 没错，就是一个普通的空指针错误，发生在 err.Error() 这里，如果 net.Listen 没有产生错误，那么 err == nil，err.Error() 自然就会产生空指针异常了。但这个错误居然困扰了许久，虽然 panic 提示了代码行数，但是我一开始硬是没注意到 err.Error() 这里，加上 [signal SIGSEGV: segmentation violation code=0x2 addr=0x18 pc=0x100412238] 这里混淆了我（因为之前跑过一个项目，也是运行就会 panic，提示的也是 SIG 之类的，但是这个项目 panic 的原因是有些包没有适配当时最新的 go1.17 版本，需要使用 go get -u 来更新部分包，即可解决问题），导致我朝着一个错误的方向解决问题。\n问题到这里就算结束了，但是这种低级错误耽误了许多时间，我在想有没有什么工具可以对这种空指针错误进行检测呢？\n使用 go vet：\n$ go vet nil_pointer.go 没有输出任何信息\n此外，GoLand 和 Vscode 也没有任何 warning 警告\n使用 golangci-lint：\n$ golangci-lint run nil_pointer.go 也没有输出任何信息\n暂时没有找到可以检测这类问题的工具，虽然 Goland 可以对部分潜在的 nil pointer 问题进行警告，但是在这篇文章的代码中无效。\n","date":"2022年09月05日","permalink":"/posts/go_nil_pointer/","summary":"看看下面这段代码的运行结果是什么？\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net\u0026quot; ) func main() { lis, err := net.","title":"一个 go nil pointer 错误记录"},{"contents":"写这篇文章是因为最近在看 go netpoll 的源码，里面有一部分代码是创建一个管道，一开始没有搞懂用意何在，后来查阅了网上的文章，有说法是这个管道是为了打断 epoll_wait 的等待，将这个管道的 fd 添加到 epoll 中，如果想主动打断 epoll_wait，那么就往管道中写入数据，然后在 epoll_wait 处就会产生事件，终止阻塞。\n搜索 epoll 打断，发现资料非常少，go netpoll 这部分的源码讲解也比较少（主要是 netpollBreak 这个函数），这导致我不明白为什么需要打断 epoll，我的猜测是：可能是为了避免让某条线程持续阻塞在 epoll_wait，这样会导致这条线程比较闲，如果当前系统的任务比较繁忙，为了保证性能就需要让每条线程都 “忙” 起来，就需要主动打断这些阻塞在 epoll_wait 的线程，先给它们分配一些任务，让它们忙起来。\n在一篇博客上还看到了一种说法：\n和poll/epoll搭配使用，一个消费者多生产者场景 实际这种使用场景会多一点，生产者是多个线程，可以通过 eventfd_write 唤醒消费者。消费者是单个线程，后台 loop 处理。使用 epoll 监听 eventfd 的可读事件，这样能做到一旦有请求入队，消费者就立马唤醒处理。所以这里可以总结出 eventfd 在实际场景中可以结合业务，做一个事件通知的通信机制，非常巧妙，而不用轮询这种耗时耗 cpu 的机制。这块就不详细写示例了。\n感觉说的挺有道理的，这种用 epoll 唤醒来描述更为合适，而不是 epoll 打断，个人的见解：eventfd 其实和 socket 类似，只不过是用于同一台机器上的一种进程间通信机制，而 socket 因为是网络传输所以可以跨机器。\n使用管道 创建一个匿名管道，用 epoll 监听管道的读端，然后 fork 一个子进程，子进程在 sleep 几秒后会向管道写入数据，然后父进程的 epoll_wait 就会被触发事件，进而跳出死循环，执行其他任务（这里逻辑可能不太合理）。\n#include \u0026lt;sys/epoll.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #define EVENTS_SIZE 128 #define BUF_SIZE 1024 int main() { int pipefd[2]; int epfd = epoll_create(1024); struct epoll_event event1, event2, events[EVENTS_SIZE]; char buf[BUF_SIZE]; event1.events = EPOLLIN; event1.data.fd = STDIN_FILENO; int ret = epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, \u0026amp;event1); if (ret != 0 ) { printf(\u0026quot;epoll_ctl error: %s\\n\u0026quot;, strerror(errno)); return 0; } if (pipe(pipefd) == -1) { perror(\u0026quot;pipe\u0026quot;); exit(EXIT_FAILURE); } // 注意要先调用 pipe() 创建管道，再添加到 epoll，否则报错 Bad file descriptor event2.events = EPOLLIN; event2.data.fd = pipefd[0]; ret = epoll_ctl(epfd, EPOLL_CTL_ADD, pipefd[0], \u0026amp;event2); // 将管道的读端添加到 epoll 监听队列 if (ret != 0) { printf(\u0026quot;epoll_ctl error: %s\\n\u0026quot;, strerror(errno)); return 0; } pid_t cpid = fork(); switch (cpid) { case -1: perror(\u0026quot;fork\u0026quot;); exit(EXIT_FAILURE); case 0:\t// child // 发现一个新坑：如果使用 printf 进行调试，必须添加 \\n 换行符，否则终端不会输出 // 搞得我还以为子进程没有执行 // printf(\u0026quot;child in\\n\u0026quot;); close(pipefd[0]);\t// 关闭管道的读端，因为子进程只负责向管道写入数据 sleep(3); char *s = \u0026quot;break\u0026quot;; write(pipefd[1], s, strlen(s)); printf(\u0026quot;child send sign to pipe\\n\u0026quot;); close(pipefd[1]); exit(0);\t// 必须要写这句，不然会出现程序已经退出（终端输出完毕，已经显示新行）， // 但是进程依然在执行的迷惑情况（终端输入全部会被当成 epoll 事件） default: // parent close(pipefd[1]);\t// 关闭管道的写端，因为父进程只负责从管道中读取 while (1) { printf(\u0026quot;epoll is wait\\n\u0026quot;); int n = epoll_wait(epfd, events, EVENTS_SIZE, -1); // 无限阻塞 for (int i = 0; i \u0026lt; n; i++) { //printf(\u0026quot;event fd: %d\\n\u0026quot;, events[i].data.fd); if (events[i].data.fd == STDIN_FILENO) { memset(buf, '\\0', BUF_SIZE); read(events[i].data.fd, buf, BUF_SIZE); printf(\u0026quot;read from stdin: %s\\n\u0026quot;, buf); } else if (events[i].data.fd == pipefd[0]) { printf(\u0026quot;pipe break epoll_wait!\\n\u0026quot;); memset(buf, '\\0', BUF_SIZE); read(events[i].data.fd, buf, BUF_SIZE);\t// 消费掉事件 close(pipefd[0]); goto STOP_WAIT; } } } STOP_WAIT: printf(\u0026quot;do something\\n\u0026quot;); exit(0); } } 结果：\n$ clang pipe_break_epoll.c $ ./a.out epoll is wait pipe break epoll_wait! do something child send sign to pipe 输出顺序有点问题，但是无伤大雅\n使用 eventfd ","date":"2022年09月02日","permalink":"/posts/epoll_zhong_duan/","summary":"写这篇文章是因为最近在看 go netpoll 的源码，里面有一部分代码是创建一个管道，一开始没有搞懂用意何在，后来查阅了网上的文章，有说法是这个管道是为了打断 epoll_wait 的等待，将这个管道的 fd 添加到 epoll 中，如果想主动打断 epoll_wait，那么就往管道中写入数据，然后在 epoll_wait 处就会产生事件，终止阻塞。","title":"epoll 打断/唤醒"},{"contents":" 来源：https://www.cnblogs.com/Anker/p/3271773.html\n基本概念 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被 init 进程(进程号为 1 )所收养，并由 init 进程对它们完成状态收集工作。\n僵尸进程：一个进程使用 fork 创建子进程，如果子进程退出，而父进程并没有调用 wait 或 waitpid 获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。\n危害 unix 提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号 the process ID,退出状态 the termination status of the process,运行时间 the amount of CPU time taken by the process等)。直到父进程通过 wait / waitpid 来取时才释放。 但这样就导致了问题，如果进程不调用 wait / waitpid 的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。\n孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。\n**任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。**这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。\n僵尸进程危害场景：\n例如有个进程，它定期的产生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用 ps 命令查看的话，就会看到很多状态为 Z 的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是 把产生大量僵死进程的那个元凶枪毙掉（也就是通过 kill 发送 SIGTERM 或者 SIGKILL 信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进程，这些孤儿进程会被 init 进程接管，init 进程会 wait() 这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程就能瞑目而去了。\n测试 孤儿进程测试程序 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { pid_t pid; //创建一个进程 pid = fork(); //创建失败 if (pid \u0026lt; 0) { perror(\u0026quot;fork error:\u0026quot;); exit(1); } //子进程 if (pid == 0) { printf(\u0026quot;I am the child process.\\n\u0026quot;); //输出进程ID和父进程ID printf(\u0026quot;pid: %d\\tppid:%d\\n\u0026quot;,getpid(),getppid()); printf(\u0026quot;I will sleep five seconds.\\n\u0026quot;); //睡眠5s，保证父进程先退出 sleep(5); printf(\u0026quot;pid: %d\\tppid:%d\\n\u0026quot;,getpid(),getppid()); printf(\u0026quot;child process is exited.\\n\u0026quot;); } //父进程 else { printf(\u0026quot;I am father process.\\n\u0026quot;); //父进程睡眠1s，保证子进程输出进程id sleep(1); printf(\u0026quot;father process is exited.\\n\u0026quot;); } return 0; } 大致逻辑就是让子进程睡眠更长的时间，以达到比父进程后退出的效果，从而变成孤儿进程。\n运行结果：\n$ ./a.out I am father process. I am the child process. pid: 15625\tppid:15618 I will sleep five seconds. father process is exited. ➜ justtest git:(master) ✗ pid: 15625\tppid:1 child process is exited. 从 pid: 15625 ppid:1 这里就可以看出子进程被 init 进程接管了（init 进程的 pid 为 1，pid 代表当前进程的 id，ppid 代表当前进程的父进程的 id）。\n（➜ justtest git:(master) ✗ pid: 15625\tppid:1 这一行是因为父进程退出后，终端会结束输出，变成待输入状态，所以会出现 justtest git:(master) ✗ 这部分，但是等几秒后子进程又会输出 pid: 15625\tppid:1，所以就变成了这样的显示效果。而且在输出了 child process is exited. 这句话之后终端会卡住，此时只需要按一下回车即可。）\n僵尸进程测试程序 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main() { pid_t pid; pid = fork(); if (pid \u0026lt; 0) { perror(\u0026quot;fork error:\u0026quot;); exit(1); } else if (pid == 0) { printf(\u0026quot;I am child process.I am exiting.\\n\u0026quot;); exit(0); } printf(\u0026quot;I am father process.I will sleep two seconds\\n\u0026quot;); //等待子进程先退出 sleep(2); //输出进程信息 system(\u0026quot;ps -o pid,ppid,state,tty,command\u0026quot;); printf(\u0026quot;father process is exiting.\\n\u0026quot;); return 0; } 这段程序让子进程先退出，同时不适用 wait 或者 waitpid 来回收。\n输出：\n./a.out I am father process.I will sleep two seconds I am child process.I am exiting. PID PPID STAT TTY COMMAND 7766 7750 S+ ttys000 -zsh 7771 7751 S ttys001 -zsh 91947 7771 S+ ttys001 ./a.out 91956 91947 Z+ ttys001 (a.out) # 僵尸进程 其中 STAT 为 Z+ 的就是僵尸进程\n僵尸进程测试2 父进程循环创建子进程，子进程退出，造成多个僵尸进程，程序如下所示：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; int main() { pid_t pid; //循环创建子进程 while(1) { pid = fork(); if (pid \u0026lt; 0) { perror(\u0026quot;fork error:\u0026quot;); exit(1); } else if (pid == 0) { printf(\u0026quot;I am a child process.\\nI am exiting.\\n\u0026quot;); //子进程退出，成为僵尸进程 exit(0); } else { //父进程休眠20s继续创建子进程 sleep(20); continue; } } return 0; } 运行结果：\n僵尸进程解决方法 fork 两次 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; int main() { pid_t pid; //创建第一个子进程 pid = fork(); if (pid \u0026lt; 0) { perror(\u0026quot;fork error:\u0026quot;); exit(1); } //第一个子进程 else if (pid == 0) { //子进程再创建子进程（第二个子进程） printf(\u0026quot;I am the first child process.pid:%d\\tppid:%d\\n\u0026quot;,getpid(),getppid()); pid = fork(); if (pid \u0026lt; 0) { perror(\u0026quot;fork error:\u0026quot;); exit(1); } // 让第一个子进程退出 else if (pid \u0026gt; 0) { printf(\u0026quot;first procee is exited.\\n\u0026quot;); exit(0); } //第二个子进程 //睡眠3s保证第一个子进程退出，这样第二个子进程的父亲就是init进程里 sleep(3); printf(\u0026quot;I am the second child process.pid: %d\\tppid:%d\\n\u0026quot;,getpid(),getppid()); exit(0); } //父进程处理第一个子进程退出 if (waitpid(pid, NULL, 0) != pid) { perror(\u0026quot;waitepid error:\u0026quot;); exit(1); } exit(0); return 0; } 大致思路就是先创建一个子进程，再让这个子进程再创建一个子进程（代称为孙子进程），然后让子进程立马退出，这样孙子进程就会被 init 接管。（使用孙子进程来执行任务）\n信号 子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程。测试程序如下所示：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;signal.h\u0026gt; static void sig_child(int signo); int main() { pid_t pid; //创建捕捉子进程退出信号 signal(SIGCHLD,sig_child); pid = fork(); if (pid \u0026lt; 0) { perror(\u0026quot;fork error:\u0026quot;); exit(1); } else if (pid == 0) { printf(\u0026quot;I am child process,pid id %d.I am exiting.\\n\u0026quot;,getpid()); exit(0); } printf(\u0026quot;I am father process.I will sleep two seconds\\n\u0026quot;); //等待子进程先退出 sleep(2); //输出进程信息 system(\u0026quot;ps -o pid,ppid,state,tty,command\u0026quot;); printf(\u0026quot;father process is exiting.\\n\u0026quot;); return 0; } static void sig_child(int signo) { pid_t pid; int stat; //处理僵尸进程 while ((pid = waitpid(-1, \u0026amp;stat, WNOHANG)) \u0026gt;0) printf(\u0026quot;child %d terminated.\\n\u0026quot;, pid); } ","date":"2022年08月31日","permalink":"/posts/gu_er_jin_cheng_he_jiang_shi_jin_cheng/","summary":"来源：https://www.cnblogs.com/Anker/p/3271773.html\n基本概念 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被 init 进程(进程号为 1 )所收养，并由 init 进程对它们完成状态收集工作。","title":"孤儿进程与僵尸进程"},{"contents":"先更新数据库，再更新缓存 存在的问题：\n更新数据库成功，更新缓存失败\n并发情况下可能会存在问题，如下（纵轴代表时间线）：\nA B 更新 DB x = 1 更新 DB，x = 2 更新缓存，x = 2 更新缓存，x = 1 可以看到因为并发操作导致 A 把 B 的更新缓存结果给覆盖了，最终使得数据库结果（x = 2）和缓存结果（x = 1）不一致\n先更新缓存，再更新数据库 存在的问题：\n和 先更新数据库，再更新 类似，这里就不再赘述了。\n先删缓存，再更新数据库 存在的问题：\n删除缓存成功，更新数据库失败\n并发情况下可能会存在问题，如下（纵轴代表时间线）：\nA B C 删缓存 读缓存，miss 读 DB，x = 1 写入缓存，x = 1 更新 db，x = 2 读缓存，x = 1，但此时 db 中 x = 2，不一致 先更新数据库，再删除缓存 存在的问题：\n更新数据库成功，删除缓存失败 并发情况下可能会存在问题，如下（纵轴代表时间线）： A B 读缓存，miss 读 DB，x = 1 更新 DB，x = 2 删除缓存 写入缓存，x = 1 此时数据库中的 x = 2，但缓存中 x = 1，存在不一致\n但是这种情况发生的概率非常低，因为缓存的写入速度大概率是要快于数据库的写入速度的（上表中就是因为更新数据库操作快于缓存写入操作，才导致最终不一致）\n延迟双删 基于 先删缓存，再更新数据库\nA B C 删缓存 读缓存，miss 读 DB，x = 1 写入缓存，x = 1 更新 db，x = 2（此时缓存和 db 不一致） 延迟一会，删除缓存 读缓存，miss，从 db 读取并写入缓存，此时一致 为什么要延迟一会？\n如果不延迟而是更新 db 后立马删除，可能存在下面的情况：\nA B 删缓存 读缓存，miss 读 DB，x = 1 更新 db，x = 2 立马删除缓存 写入缓存，x = 1 最终 db 中的 x = 2，而缓存中的 x = 1，二者又不一致了。如果让 A 延迟一会，等到 B 写完缓存后再删，就不会出现上面的问题了。\n但问题来了，需要延迟多长时间才能达到前面所说的效果呢？毕竟 B 什么时候能写完缓存可是不可预料的，而且如果延迟时间设置的过长，也会导致系统的延迟变高。\n","date":"2022年08月27日","permalink":"/posts/huan_cun_yi_zhi_xing/","summary":"先更新数据库，再更新缓存 存在的问题：\n更新数据库成功，更新缓存失败\n并发情况下可能会存在问题，如下（纵轴代表时间线）：\nA B 更新 DB x = 1 更新 DB，x = 2 更新缓存，x = 2 更新缓存，x = 1 可以看到因为并发操作导致 A 把 B 的更新缓存结果给覆盖了，最终使得数据库结果（x = 2）和缓存结果（x = 1）不一致","title":"如何保证缓存和数据库的一致性"},{"contents":"Go相关 map、channel、slice 的底层实现 map map 使用的是拉链法，用哈希桶来保存键值对，每个桶可以保存 8 个键值对，它会用哈希算法计算出 key 的 hash 值，然后用这个哈希值的高 8 位作为 tophash，用于在一个桶中区别出 key，然后桶的数量是 2^b ，用低 b 位来决定这个 key 放到哪个桶。每个桶还会连接一个溢出桶，如果一个桶已经满了，但是依然有 key 落入到这个桶，那么就会将其放到溢出桶中。\nmap 的默认负载因子是 6.5，负载因子表示的是当前键值对的数量 / 当前桶的数量，如果 map 的负载因子达到了 6.5，那么就说明此时可能所有桶都快装满了，如果再添加键值对，大概率会放到溢出桶中，所以此时就需要进行扩容了，对应的扩容策略是：创建数量是之前桶的 2 倍的新桶，然后将旧桶中的值迁移到新桶中（迁移的策略是怎样的？rehash），还有一种情况是溢出桶的数量过多，可能的原因是键值对数量少，但过于分散，导致桶的数量多，解决办法是创建同样数量的新桶，将旧桶中的数据迁移到这些新桶中，使整体键值对的排布更紧凑。\n此外 go map 扩容采用的渐进式扩容，当执行插入、更新、删除操作时进行小部分扩容，而不是一次性将所有桶进行迁移，这样可以减少搬迁操作对性能的影响。\nchannel channel 底层主要由一个发送阻塞队列和一个接收阻塞队列，以及一把互斥锁组成，发送阻塞队列存储的是因为发送操作而被阻塞的 goroutine，接收阻塞队列同理，这里面存储的都是被挂起的 goroutine，当执行发送操作时，会先检查接收阻塞队列里有没有值，有的话从队头取出，并将这个 goroutine 唤醒，并将数据拷贝给它，执行接收操作同理。互斥锁用来保证 chan 的线程安全。此外如果是有缓冲的 chan，还会存在一个环形数组，\nsync.map、sync.pool、sync.Once的原理 GC的过程、写屏障的含义及作用 GMP 模型，触发 Goroutine 切换的原因有哪些？for 死循环会怎么样？全局 goroutine 里面存储什么？ interface 的底层实现，怎么判空？ reflect 的使用 逃逸分析 context的使用 go 性能问题的定位过程（pprof的使用） 协程池的使用 Redis相关 redis的数据类型以及日常的应用 string 字符串类型 hashmap 哈希表 list 双向链表 set 去重集合 zset 排序 set redis的发布/订阅的原理 zset的底层实现 zset 底层使用跳表 + 哈希表实现，\n数据缓存过期策略 redis的部署模式 redis为什么速度比较快 reids的大key、热key的处理 如何实现分布式锁的 持久化策略及其对比 缓存雪崩、缓存击穿、缓存穿透 ","date":"2022年08月18日","permalink":"/posts/mian_shi_ti_shou_ji1/","summary":"Go相关 map、channel、slice 的底层实现 map map 使用的是拉链法，用哈希桶来保存键值对，每个桶可以保存 8 个键值对，它会用哈希算法计算出 key 的 hash 值，然后用这个哈希值的高 8 位作为 tophash，用于在一个桶中区别出 key，然后桶的数量是 2^b ，用低 b 位来决定这个 key 放到哪个桶。每个桶还会连接一个溢出桶，如果一个桶已经满了，但是依然有 key 落入到这个桶，那么就会将其放到溢出桶中。","title":"面试题收集 1"},{"contents":"首先先来看一下这两个问题：\n347. 前 K 个高频元素 给你一个整数数组 nums 和一个整数 k ，请你返回其中出现频率前 k 高的元素。你可以按 任意顺序 返回答案。\n示例 1:\n输入: nums = [1,1,1,2,2,3], k = 2 输出: [1,2]\n示例 2:\n输入: nums = [1], k = 1 输出: [1]\n提示：\n1 \u0026lt;= nums.length \u0026lt;= 105 k 的取值范围是 [1, 数组中不相同的元素的个数] 题目数据保证答案唯一，换句话说，数组中前 k 个高频元素的集合是唯一的\n进阶：你所设计算法的时间复杂度 必须 优于 O(n log n) ，其中 n 是数组大小。\n692. 前 K 个高频单词 给定一个单词列表 words 和一个整数 k ，返回前 k 个出现次数最多的单词。\n返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率， 按字典顺序 排序。\n示例 1：\n输入: words = [\u0026ldquo;i\u0026rdquo;, \u0026ldquo;love\u0026rdquo;, \u0026ldquo;leetcode\u0026rdquo;, \u0026ldquo;i\u0026rdquo;, \u0026ldquo;love\u0026rdquo;, \u0026ldquo;coding\u0026rdquo;], k = 2 输出: [\u0026ldquo;i\u0026rdquo;, \u0026ldquo;love\u0026rdquo;] 解析: \u0026ldquo;i\u0026rdquo; 和 \u0026ldquo;love\u0026rdquo; 为出现次数最多的两个单词，均为2次。 注意，按字母顺序 \u0026ldquo;i\u0026rdquo; 在 \u0026ldquo;love\u0026rdquo; 之前。\n示例 2：\n输入: [\u0026ldquo;the\u0026rdquo;, \u0026ldquo;day\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, \u0026ldquo;sunny\u0026rdquo;, \u0026ldquo;the\u0026rdquo;, \u0026ldquo;the\u0026rdquo;, \u0026ldquo;the\u0026rdquo;, \u0026ldquo;sunny\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, \u0026ldquo;is\u0026rdquo;], k = 4 输出: [\u0026ldquo;the\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, \u0026ldquo;sunny\u0026rdquo;, \u0026ldquo;day\u0026rdquo;] 解析: \u0026ldquo;the\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, \u0026ldquo;sunny\u0026rdquo; 和 \u0026ldquo;day\u0026rdquo; 是出现次数最多的四个单词， 出现次数依次为 4, 3, 2 和 1 次。\n注意：\n1 \u0026lt;= words.length \u0026lt;= 500 1 \u0026lt;= words[i] \u0026lt;= 10 words[i] 由小写英文字母组成。 k 的取值范围是 [1, 不同 words[i] 的数量]\n进阶：尝试以 O(n log k) 时间复杂度和 O(n) 空间复杂度解决。\n发现这两个问题有点类似（从名字都能看出来），只不过一个是从 int 数组里找，且结果对顺序没有要求（也就是说如果有两个元素出现的次数相同，那么不需要考虑它们的顺序问题，谁前谁后都可以）；另一个是从 string 数组里找，如果两个单词出现次数相同，那么需要按字典序进行排序，比如 a 和 b 都出现了 10 次，那么 a 要排在前面。\n像这种 topk 问题，貌似都可以使用 堆 来解决，大致逻辑是：用最小堆来解决，建一个大小为 K 的堆，然后遍历数组，如果当前堆的元素不到 K 个，则直接将元素添加到堆中，如果堆的元素 \u0026gt;= K，此时堆顶保存的是整个堆中最小的元素，那么就比较当前元素是否小于堆顶，如果比堆顶还小，那么直接跳过，如果大于堆顶，那么可以将堆顶这个最小的元素 pop 掉，然后把 push 当前元素，因为堆会自动调整结构，所以 push 后堆顶依然是整个堆中最小的元素。当数组遍历完后，整个堆中保存的就是最大的 K 个元素。\n以上只是大致的逻辑，但是相对来说还是比较通用的，只是比较的规则需要调整一下，按照上面的逻辑写一下这两道题。\n347 代码 type myheap struct { s []int mp map[int]int } func (m myheap) Swap(i, j int) { m.s[i], m.s[j] = m.s[j], m.s[i] } func (m myheap) Len() int { return len(m.s) } func (m myheap) Less(i, j int) bool { return m.mp[m.s[i]] \u0026lt; m.mp[m.s[j]] } func (m *myheap) Push(x interface{}) { m.s = append(m.s, x.(int)) } func (m *myheap) Pop() interface{} { p := m.s[len(m.s)-1] m.s = m.s[:len(m.s)-1] return p } func topKFrequent(nums []int, k int) (res []int) { h := \u0026amp;myheap{mp : make(map[int]int)} for _, v := range nums { h.mp[v]++ } for v := range h.mp { if h.Len() \u0026lt; k { heap.Push(h, v) } else { if h.mp[h.s[0]] \u0026lt; h.mp[v] { heap.Pop(h) heap.Push(h, v) } } } return h.s } （不得不吐槽一下 go 的容器库，实在太蛋疼了，没有泛型就算了，只提供一个最基本的 heap，需要自己实现 2 个接口 5 个方法，写起来实在繁琐，不像其他语言的优先队列用起来那么方便 ）\n因为题目要求的是出现次数，所以需要用一个 map 来保存所有元素的出现次数，然后基于这个 map 来进行比较，这里需要注意的是 Less() 这个方法，它的实现逻辑是 return m.mp[m.s[i]] \u0026lt; m.mp[m.s[j]]，也就是基于元素的出现次数来作为堆的比较依据；还有当前元素和堆顶的比较逻辑 h.mp[h.s[0]] \u0026lt; h.mp[v] 也是基于元素的出现次数进行比较。除了这两点以外，基本和之前的通用逻辑相似。\n692 代码 type myheap struct { s []string mp map[string]int } func (m myheap) Swap(i, j int) { m.s[i], m.s[j] = m.s[j], m.s[i] } func (m myheap) Len() int { return len(m.s) } func (m myheap) Less(i, j int) (res bool) { o1 := m.s[i] o2 := m.s[j] m1 := m.mp[o1] m2 := m.mp[o2] if m1 == m2 { return o1 \u0026gt; o2 // 如果出现频率相同，按字典序排列 } return m1 \u0026lt; m2 } func (m *myheap) Push(x interface{}) { m.s = append(m.s, x.(string)) } func (m *myheap) Pop() interface{} { p := m.s[len(m.s)-1] m.s = m.s[:len(m.s)-1] return p } func topKFrequent(words []string, k int) (res []string) { m := \u0026amp;myheap{ mp: make(map[string]int), } for _, v := range words { m.mp[v]++ } for v := range m.mp { if m.Len() \u0026lt; k { heap.Push(m, v) } else { if (m.mp[v] == m.mp[m.s[0]] \u0026amp;\u0026amp; v \u0026lt; m.s[0]) || m.mp[v] \u0026gt; m.mp[m.s[0]] { heap.Pop(m) heap.Push(m, v) } } } res = make([]string, m.Len()) for i := k; i \u0026gt; 0; i-- { res[i-1] = heap.Pop(m).(string) } //for i := 0; i \u0026lt; k; i++ { // res = append(res, heap.Pop(m).(string)) //} // 因为是小顶堆，堆顶为出现次数最小的，而题目要求次数多的排在前面，所以需要反转结果 //for i, j := 0, len(res)-1; i \u0026lt; j; i++ { // res[i], res[j] = res[j], res[i] // j-- //} return } 这道题相比 347 要麻烦一些，当时写的时候有一个比较迷的地方，就是 Less() 里的判断逻辑：\no1 := m.s[i] o2 := m.s[j] m1 := m.mp[o1] m2 := m.mp[o2] if m1 == m2 { return o1 \u0026gt; o2 // 如果出现频率相同，按字典序排列 } 当时没搞懂为什么是 o1 \u0026gt; o2，其实这里需要理解 go heap 的底层实现，Less() 是用来判断是否需要交换元素的，如果 Less 返回 true 才需要交换，也就是说如果 o1 \u0026gt; o2（这是两个 string，且 o1 对应下标 i，o2 对应下标 j，这里假设 i 在前，j 在后），那么按照字典序排列，应该是小在前大在后（也就是 i 对应的元素要小于 j 对应的元素），所以需要进行交换，把小的 o2 换到前面，大的 o1 换到后面。\n此外还有和堆顶比较的逻辑：\nif (m.mp[v] == m.mp[m.s[0]] \u0026amp;\u0026amp; v \u0026lt; m.s[0]) || m.mp[v] \u0026gt; m.mp[m.s[0]] 这里为什么又是 v \u0026lt; m.s[0]) 呢？还是因为字典序。比如 [\u0026ldquo;i\u0026rdquo;,\u0026ldquo;love\u0026rdquo;,\u0026ldquo;leetcode\u0026rdquo;,\u0026ldquo;i\u0026rdquo;,\u0026ldquo;love\u0026rdquo;,\u0026ldquo;coding\u0026rdquo;]，k=3 这个测试用例，最终的结果是[\u0026ldquo;i\u0026rdquo;,\u0026ldquo;love\u0026rdquo;,\u0026ldquo;coding\u0026rdquo;]，leetcode 和 coding 都出现了 1 次，所以要比较它们的字典序，而 coding \u0026lt; leetcode，所以选择 coding 这个较小的。对应到堆，如果此时堆顶为 leetcode，现在要添加 coding 到堆中，需要判断逻辑为：在出现次数相同的情况下，如果当前元素的字典序小于堆顶，则 pop 堆顶，并 push 当前元素，这样才能让这个较小的元素入堆。\n此外，因为题目要求返回的答案按单词出现频率由高到低排序，所以也不能像 347 那样，直接把堆中的元素返回，因为堆是不保证整体有序的，它只保证两个子节点都小于（大于）父节点，那如何让其有序呢？只需要不断 pop 即可，这里 go heap 的 pop 接口实现看起来有点诡异：\nfunc (m *myheap) Pop() interface{} { p := m.s[len(m.s)-1] m.s = m.s[:len(m.s)-1] return p } 看到这里难免会有点纳闷，我明明是要 pop 堆顶元素，不应该是移除 m.s[0] 才对吗？为什么这里要移除最后一个元素？这里又得看源码了：\nfunc Pop(h Interface) any { n := h.Len() - 1 h.Swap(0, n) down(h, 0, n) return h.Pop() } 这下明白了，原来底层实现会将堆顶换到堆尾，所以才需要移除最后一个元素。同时也知道了，在交换堆顶到堆尾后，会从堆顶开始，执行 down 操作，调整整体的堆，让堆顶继续是整个堆中最小的元素（会忽略末尾元素，也就是刚刚被换到末尾的那个最小的元素），所以每次 pop 都是当前堆中最小的那个元素，进而保证有序。\n代码里将 pop 的元素保存到一个 slice 里面，因为是小顶堆，所以 pop 的元素是由小到大的，而这不满足题目 “由大到小” 的要求，需要将 slice 翻转，变成降序排列。其实这里用一个链表保存更合理，每次 pop 往链表头部添加即可，如果是 slice 的话，每次都往头部添加会导致开销特别大（频繁的分配拷贝，效率还不如整体翻转），但是题目给的定义是返回一个 slice，也只能这么做了（java 版本的题目定义返回的就是一个 linkedlist）。 而题目要求的是由大到小，只需要先预分配 slice 的空间为堆的长度，然后从尾部向头部添加就可以了。\n另一种模板 看了一些题解，发现他们的写法略有不同，主要是在添加到堆的逻辑这里，不需要和堆顶元素进行比较，而是先 push 进去，如果发现长度大于 K 了，则执行 pop，感觉这种写法的代码会比较简洁明了容易理解，但是可能效率会低一些，像之前的逻辑会和堆顶进行比较，如果不满足某些条件会直接跳过，不执行任何操作，而这里的逻辑是遍历的每个元素都先无脑 push 进去，push 还需要执行堆的调整操作，所以相比叫而言会多做一些无用功。\n692 type myheap struct { s []string mp map[string]int } func (m myheap) Swap(i, j int) { m.s[i], m.s[j] = m.s[j], m.s[i] } func (m myheap) Len() int { return len(m.s) } func (m myheap) Less(i, j int) bool { o1 := m.s[i] o2 := m.s[j] m1 := m.mp[o1] m2 := m.mp[o2] if m1 == m2 { return o1 \u0026gt; o2 // 如果出现频率相同，按字典序排列 } return m1 \u0026lt; m2 } func (m *myheap) Push(x interface{}) { m.s = append(m.s, x.(string)) } func (m *myheap) Pop() interface{} { p := m.s[len(m.s)-1] m.s = m.s[:len(m.s)-1] return p } func topKFrequent(words []string, k int) (res []string) { m := \u0026amp;myheap{ mp: make(map[string]int), } for _, v := range words { m.mp[v]++ } for v := range m.mp { heap.Push(m, v) if m.Len() \u0026gt; k { heap.Pop(m) } } res = make([]string, m.Len()) for i := k; i \u0026gt; 0; i-- { res[i-1] = heap.Pop(m).(string) } return } 不得不说确实简洁很多，主要是 range m.mp 这里的逻辑，对比一下：\n// 第一种写法 for v := range m.mp { if m.Len() \u0026lt; k { heap.Push(m, v) } else { if (m.mp[v] == m.mp[m.s[0]] \u0026amp;\u0026amp; v \u0026lt; m.s[0]) || m.mp[v] \u0026gt; m.mp[m.s[0]] { heap.Pop(m) heap.Push(m, v) } } } // 第二种写法 for v := range m.mp { heap.Push(m, v) if m.Len() \u0026gt; k { heap.Pop(m) } } 347 type myheap struct { s []int mp map[int]int } func (m myheap) Swap(i, j int) { m.s[i], m.s[j] = m.s[j], m.s[i] } func (m myheap) Len() int { return len(m.s) } func (m myheap) Less(i, j int) bool { return m.mp[m.s[i]] \u0026lt; m.mp[m.s[j]] } func (m *myheap) Push(x interface{}) { m.s = append(m.s, x.(int)) } func (m *myheap) Pop() interface{} { p := m.s[len(m.s)-1] m.s = m.s[:len(m.s)-1] return p } func topKFrequent(nums []int, k int) (res []int) { h := \u0026amp;myheap{mp : make(map[int]int)} for _, v := range nums { h.mp[v]++ } for v := range h.mp { heap.Push(h, v) if h.Len() \u0026gt; k { heap.Pop(h) } } return h.s } ","date":"2022年08月17日","permalink":"/posts/leetcode_347_and_692/","summary":"首先先来看一下这两个问题：\n347. 前 K 个高频元素 给你一个整数数组 nums 和一个整数 k ，请你返回其中出现频率前 k 高的元素。你可以按 任意顺序 返回答案。","title":"两个类似的堆问题：leetcode 347 前 K 个高频元素 和 692. 前 K 个高频单词 "},{"contents":"一个示例 demo：\npackage main import ( \u0026quot;bytes\u0026quot; \u0026quot;encoding/binary\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io\u0026quot; ) type I interface { Do() } type S struct { Id int64 Name [10]byte } func (s *S) Do() { fmt.Println(\u0026quot;Do\u0026quot;) } var _ I = \u0026amp;S{} func write(b io.Writer, i I) { if err := binary.Write(b, binary.BigEndian, i); err != nil { panic(err) } } func read(b io.Reader, i I) { if err := binary.Read(b, binary.BigEndian, i); err != nil { panic(err) } } func main() { var ( buf = new(bytes.Buffer) //_buf = make([]byte, 4096) //buf = bytes.NewBuffer(_buf) 这样写会导致无法 read 出数据，原因未知 s = \u0026amp;S{ Id: 10001, Name: [10]byte{'a', 'b', 'c'}, } s_ = \u0026amp;S{} ) write(buf, s) read(buf, s_) fmt.Println(s_) s_.Do() } 运行结果：\n\u0026amp;{10001 [97 98 99 0 0 0 0 0 0 0]} Do 注意事项 1. 如果写入的是结构体，那么结构体中不能有非定长的数据类型 比如 int 和 string，这两个都是不定长的，int 会根据操作系统来决定长度，而 string 代表的是字符串，可以进行拼接/删除等操作，所以也不是定长的，这些类型都无法通过 binary.Write 进行写入。如果想写入 int 类型的变量，应该显示指定其长度，比如 int64。切片类型也是不定长的，所以只能传入需要指定长度的数组。\n如果将 demo 中的结构体 S 修改为：\ntype S struct { pri int64 Id int64 Name []byte } 运行结果：\npanic: binary.Write: invalid type *main.S\n2. 但是可以单独写入 []byte 比如下面的程序：\npackage main import ( \u0026quot;encoding/binary\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;bytes\u0026quot; ) func main() { b := []byte(\u0026quot;123\u0026quot;) //b := \u0026quot;123456\u0026quot; buf := new(bytes.Buffer) bb := make([]byte, len(b), len(b)) if err := binary.Write(buf, binary.BigEndian, b); err != nil { panic(err) } if err := binary.Read(buf, binary.BigEndian, bb); err != nil { panic(err) } fmt.Println(b, bb) } // Output: // [49 50 51] [49 50 51] 这里单独将 b 写入，发现是可以正常运行的，但是 string 类型依然不能写入，会报错：panic: binary.Write: invalid type string\n3. 如果写入的是 struct，需要保证所有字段为 public，但是结构体本身可以为 private 注意是 所有字段，如果有非导出（private）字段，会报错：reflect: reflect.Value.SetInt using value obtained using unexported field\n4. 可以写入接口类型 通过开头的示例程序，说明是可以写入接口类型的（也就是说，binary.Read 和 binary.Write 的第三个参数可以传入接口类型）。\n","date":"2022年08月08日","permalink":"/posts/go_binary/","summary":"一个示例 demo：\npackage main import ( \u0026quot;bytes\u0026quot; \u0026quot;encoding/binary\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io\u0026quot; ) type I interface { Do() } type S struct { Id int64 Name [10]byte } func (s *S) Do() { fmt.","title":"go binary 使用指南 "},{"contents":"发送方维护一个叫做 拥塞窗口 cwnd 的状态变量，其值 取决于网络的拥塞程度，并且 动态变化。\n拥塞窗口 cwnd 的维护原则：只要网络 没有出现拥塞，拥塞窗口 就再 增大 一些；但只要网络 出现拥塞，拥塞窗口就减少一些。 判断出现 网络拥塞的依据：没有按时收到应当到达的确认报文（即发生超时重传）。 发送方将拥塞窗口作为 发送窗口 swnd，即 swnd = cwnd。拥塞窗口的大小也就是发送方能发送的报文的数量，即：cwnd增加1也就是相当于字节数增加1个MSS大小 （这里存疑，是数量还是总大小？）。\n维护一个 慢开始门限 ssthresh 状态变量：\n当 cwnd \u0026lt; ssthresh 时，使用慢开始算法； 当 cwnd \u0026gt; ssthresh 时，停止使用慢开始算法，该用拥塞避免算法； 当 cwnd = ssthresh 时，即可以使用慢开始算法，也可以使用拥塞避免算法； 1. 慢开始 发送方发送报文后，如果收到了对方的确认报文（可以简称为一个 传输轮次），那么就将自己的 cwnd 在 原基础上乘以 2，比如从 1 开始，之后的窗口大小依次是：2，4，8，16，32，64\u0026hellip;（按指数规律增长）\n慢开始可以理解为 “在死亡边缘不断试探”，一开始试探性的向网络中发送少量报文，如果没有出现拥塞，就 “得寸进尺” 的多发送一些，以此类推。所以不要被慢开始的“慢”字给欺骗了，它可一点都不慢，慢开始并不是指 cwnd 的增长速度慢，而是指一开始向网络注入的报文段少。\n慢开始何时结束？ 既然慢开始的增长速度如此之快，那么肯定不能让其无限制增长，当出现以下情况时，会停止慢开始阶段，进入拥塞避免阶段：\n当 cwnd 达到 ssthresh 时，会进入拥塞避免阶段。\n发送超时重传时，此时网络可能出现了阻塞（也就是说，TCP 判断网络是否阻塞的依据是是否发生了超时重传）\n疑问：sshthresh 的值默认是多少？\n在一片博客上看的答案是：对于大多数 TCP 实现来说，ssthresh 的值是 65536(同样以字节计算)，这里的字节计算是什么意思，是 cwnd 的门限值 = 65535/mss 的意思吗？\n2. 拥塞避免 到了拥塞避免阶段，就说明发送报文的数量可能快引起拥塞了（cwnd = ssthresh），或者可能已经出现了拥塞（发生了超时重传），此时的拥塞窗口值就不能再像慢开始那样快速增长了，对于这两种情况，有不同的处理方法。\n当 cwnd 达到 ssthresh 时 此时说明发送报文的数量可能快引起拥塞了，不能再指数级增长了，变为线性增长，也就是每个传输轮次只将 cwnd + 1。\n发送超时重传 如果 发送方发生了超时重传，那么说明此时网络可能已经出现拥塞，此时需要做以下工作：\n将慢开始门限值 ssthresh 更新为发生拥塞时拥塞窗口值的一半。比如在 cwnd=24 时发生了超时重传，那么就会将 ssthresh 更新为 12. 将 cwnd 更新为 1，并重新开始慢开始算法。 拥塞避免 并非指完全能够避免拥塞，而是指在拥塞避免阶段将 拥塞窗口控制为按线性规律增长，使网络不容易出现拥塞。\n拥塞控制的整个流程如下图所示：\n（图片出自：计算机网络微课堂（有字幕无背景音乐版））\n3. 快重传 报文发生了超时重传，并不一定就代表网络发生了拥塞，也有可能是别的原因，比如因为误码，被路由器给扔了，或者网络出现了延迟，如果直接简单粗暴的进入到拥塞避免阶段，让 cwnd 回归到 1，可能会导致网络的传输效率降低。\n快重传算法可以让发送方尽早知道发生了个别报文段的丢失。\n所谓快重传，就是使发送方 尽快进行重传，而 不是等待超时重传计时器超时 再重传。\n要求接收方不要等待自己发送数据时才进行捎带确认，而是要立即发送确认； 即使收到了失序的报文段也要立即发出对已收到的报文段的重复确认。 发送方一旦收到 3 个连续的重复确认，就将相应的报文段立即重传，而不是等该报文段的超时重传计时器超时再重传。 比如下图的情况，M3 在网络传输过程中丢失了，接收方在收到 M4 后，发现 M3 还没收到，于是回复 ACK=2，表示已经收到 M2 包了，希望接下来收到 M3 包，之后发送的 M4，M5 也都是如此。此时发送方发现收到了 3 个 ACK=2，知道了 M2 这个包对方没有收到，于是就可以立马重传了，而不是等到 M3 的超时重传到期后再重传。\n这样对于个别丢失的报文段，发送方就不会出现超时重传，也就不会误认为出现了拥塞，避免了将拥塞窗口 cwnd 降低为 1。使用快重传可以使整个网络的吞吐量提高约 20 %。\n4. 快恢复 发送方一旦 收到 3 个重复确认，就知道现在只是丢失了个别的报文段。于是不启动慢开始算法，而 执行快恢复算法；\n发送方将慢开始门限 ssthresh 值和拥塞窗口 cwnd 值调整为当前窗口的一半；开始执行拥塞避免算法。\n也有的快恢复实现是把快恢复开始时的拥塞窗口 cwnd 值再增大一些，即等于新的 ssthresh +3。\n既然发送方收到3个重复的确认，就表明有3个数据报文段己经离开了网络：\n这3个报文段不再消耗网络资源而是停留在接收方的接收缓存中；\n可见现在网络中不是堆积了报文段而是减少了3个报文段。因此可以适当把拥塞窗口扩大些。\n下图描述了整个流程，可以看到快恢复是将 cwnd 调整为发生重复确认时，窗口的一半，而不是像拥塞避免直接将 cwnd 调整为 1，一定程度上提高了吞吐率。\n参考/引用 计算机网络微课堂（有字幕无背景音乐版）\nTCP慢启动、拥塞避免、快速重传、快速恢复\n","date":"2022年08月02日","permalink":"/posts/tcp_yong_se_kong_zhi/","summary":"发送方维护一个叫做 拥塞窗口 cwnd 的状态变量，其值 取决于网络的拥塞程度，并且 动态变化。","title":"TCP 拥塞控制"},{"contents":"如果没有调用 make 来对一个 chan 进行初始化，那么这个 chan 就是 nil chan。\n对于 nil chan：\n从 nil chan 接收将永远阻塞 发送值到 nil chan 会永远阻塞 close 一个 nil chan 会引发 panic 看到这里我不禁有点疑问，这样设计的意义在哪里？为什么写入和读取要阻塞，而不是像 nil map 一样直接 panic？\n带着这个问题找到了一篇文章：Why are there nil channels in Go?\n中译版：为什么 Go 会有 nil channels\n里面提到了 nil chan 的意义，这里简单概括一下：\nnil chan 主要是为了防止 select 的空转。\n比如以下代码：\nfunc fn(c1 chan int, c2 chan int) { for { select { case v := \u0026lt;- c1: case v := \u0026lt;- c2: } } } 如果某个时间 c1，c2 都被关闭了，按照 chan 的规则，如果尝试读取一个已关闭的 chan，将会读取出类型的空值，所以此时的 case v := \u0026lt;- c1: 和 case v := \u0026lt;- c2: 分支依然是会被选择的，而一个 chan 被关闭则说明我们已经不需要它了，这显然是一个多余的操作。\n而 select 的规则是，如果所有 case 都满足，则随机选择一个，此时的 c1，c2 依然满足条件，所以 select 会随机选择一个，又因为 select 定义在一个死循环里面，所以会导致 cpu 空转。\n现在终于知道 nil chan 阻塞的意义何在了，将上面的代码修改如下：\nfunc fn(c1 chan int, c2 chan int) { for { select { case v, ok := \u0026lt;- c1: if !ok {c1 = nil} case v, ok := \u0026lt;- c2: if !ok {c2 = nil} default: return } } } 如果某个 chan 被 close 了，那么将其设置为 nil，而 nil chan 的读取会阻塞，所以 select 就不会选择这个分支了，这样就避免了上述情况的空准，但是不要忘记加一个 default 分支，不然这个 select 就被永久阻塞了。\n","date":"2022年07月18日","permalink":"/posts/wei-shen-me-go-you-nil-chan/","summary":"如果没有调用 make 来对一个 chan 进行初始化，那么这个 chan 就是 nil chan。","title":"为什么 go 需要 nil chan"},{"contents":"设置代理后，minikube kubectl 无响应 在给虚拟机设置了代理后，整个网络体验爽快多了，再也不用担心拉不下来包了。。。\n只需要将虚拟机的网络设置为桥接模式，然后开启 clashx 的 “允许局域网连接”（非常重要的一步，之前就是因为忽略了这一步，导致虚拟机无法使用宿主机代理），然后再点击 clashx 的 “复制终端代理命令”，将里面的 127.0.0.1 替换为宿主机的 ip，然后粘贴到虚拟机的终端上允许，即可成功开启代理。\n为了方便起见，可以封装成两个函数，放到 .zshrc（或者 .bashrc）里面：\nhost_ip=\u0026quot;192.168.2.5\u0026quot; # 宿主机 IP function proxy_on() { export https_proxy=http://$host_ip:7890 http_proxy=http://$host_ip:7890 all_proxy=socks5://$host_ip:7890 # curl https://www.google.com/ # echo -e \u0026quot;\\n\u0026quot; echo -e \u0026quot;\\033[32m已开启代理\\033[0m\u0026quot; } function proxy_off() { unset http_proxy unset https_proxy unset all_proxy echo -e \u0026quot;已关闭代理\u0026quot; } 然后重新载入：\nsource ~/.zshrc 现在在终端可以输入 proxy_on 来开启代理了，同样可以输入 proxy_off 来关闭。\n但是之后发现了一个问题，kubectl 命令无响应，不管是使用 snap 下载的 kubectl，还是使用 minikube kubectl 下载的 kubectl 都是如此。\n如果执行 kubectl get po 会一直阻塞，但是 kubectl version 在等待一会后会返回错误信息：\n$ minikube kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;24\u0026quot;, GitVersion:\u0026quot;v1.24.3\u0026quot;, GitCommit:\u0026quot;aef86a93758dc3cb2c658dd9657ab4ad4afc21cb\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2022-07-13T14:30:46Z\u0026quot;, GoVersion:\u0026quot;go1.18.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm64\u0026quot;} Kustomize Version: v4.5.4 error: Get \u0026quot;https://192.168.49.2:8443/version?timeout=32s\u0026quot;: context deadline exceeded - error from a previous attempt: read tcp 192.168.2.2:35044-\u0026gt;192.168.2.5:7890: read: connection reset by peer 看到这里的 192.168.2.2:35044-\u0026gt;192.168.2.5:7890，我才发现有点不对劲，这不是我设置的代理地址吗？难道问题出在这里？\n然后查阅了一下 minikube 的官方文档，发现里面专门 有一节 专门提到了代理的问题：\n里面特别提到了：\n如果需要 HTTP 代理来访问互联网，您可能需要使用环境变量将代理连接信息传递给 minikube 和 Docker：\nHTTP_PROXY- 您的 HTTP 代理的 URL HTTPS_PROXY- HTTPS 代理的 URL NO_PROXY- 不应通过代理的主机的逗号分隔列表。 这里的 NO_PROXY 变量很重要：如果不设置它，minikube 可能无法访问 VM 内的资源。minikube 使用四个默认 IP 范围，它们不应通过代理：\n192.168.59.0/24：由 minikube VM 使用。可通过以下方式为某些管理程序配置--host-only-cidr 192.168.39.0/24：由 minikube kvm2 驱动程序使用。 192.168.49.0/24：由 minikube docker 驱动程序的第一个集群使用。 10.96.0.0/12：由服务集群 IP 使用。可通过以下方式配置 --service-cluster-ip-range 并且给出了示例：\nexport HTTP_PROXY=http://\u0026lt;proxy hostname:port\u0026gt; export HTTPS_PROXY=https://\u0026lt;proxy hostname:port\u0026gt; export NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24 现在照猫画虎，修改之前的 proxy_on 函数（这里我同样删除了 all_proxy 配置，不知道这个配置是否有影响）：\nhost_ip=\u0026quot;192.168.2.5\u0026quot; function proxy_on() { export no_proxy=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24 export https_proxy=http://$host_ip:7890 export http_proxy=http://$host_ip:7890 # curl https://www.google.com/ # echo -e \u0026quot;\\n\u0026quot; echo -e \u0026quot;\\033[32m已开启代理\\033[0m\u0026quot; } 现在 kubectl 就可以正常使用了。\n更新：\n最近发现又出现这个问题了，我的 proxy_on 函数也没有发生任何修改，报错：\n$ kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;24\u0026quot;, GitVersion:\u0026quot;v1.24.4\u0026quot;, GitCommit:\u0026quot;95ee5ab382d64cfe6c28967f36b53970b8374491\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2022-08-18T02:40:28Z\u0026quot;, GoVersion:\u0026quot;go1.18.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm64\u0026quot;} Kustomize Version: v4.5.4 error: Get \u0026quot;https://192.168.58.2:8443/version?timeout=32s\u0026quot;: context deadline exceeded - error from a previous attempt: read tcp 192.168.2.4:45562-\u0026gt;192.168.2.3:7890: read: connection reset by peer 这里我注意到了 \u0026ldquo;https://192.168.58.2:8443/version?timeout=32s\u0026rdquo; 这部分，发现请求的是 192.168.58.2 这个地址，我尝试将这个地址段（也就是 192.168.58.0/24）也添加到 no_proxy 列表中：\nfunction proxy_on() { export no_proxy=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24,192.168.58.0/24 export https_proxy=http://$host_ip:7890 export http_proxy=http://$host_ip:7890 # curl https://www.google.com/ # echo -e \u0026quot;\\n\u0026quot; echo \u0026quot;host_ip: $host_ip\u0026quot; echo -e \u0026quot;\\033[32m已开启代理\\033[0m\u0026quot; } 添加后 source .zshrc，proxy_off 一下，再重新 proxy_on，发现 kubectl 可以正常执行了。\n不过具体原因还不清楚，我的 minikube version 是 v1.26.1，是官方文档与这个版本不匹配吗？或者可能是因为 192.168.59.0 这个网段被占用了，导致 minikube 重新选择了 192.168.58.0 这个网段？暂时不知道原因，不过只要按照 kubectl version 里的报错信息，将请求的网段添加到 no_proxy 环境变量里就可以解决问题了。\nminikube 无法启动 重装了一下 docker，发现 minikube 无法启动了，部分报错信息如下：\n[kubelet-check] Initial timeout of 40s passed. Unfortunately, an error has occurred: timed out waiting for the condition This error is likely caused by: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands: - 'systemctl status kubelet' - 'journalctl -xeu kubelet' 解决方法，参考这里：\n$ minikube delete $ minikube start --kubernetes-version=v1.23.8 可能是 kubernetes 1.24 的问题导致的\n创建 pod 错误，Error: ImagePullBackOff 一个 redis yaml 如下：\napiVersion: v1 kind: Pod metadata: name: redis labels: app: redis spec: containers: - name: redis image: redis ports: - containerPort: 6379 hostPort: 6379 这是我之前使用过的 redis yaml，yaml 本身应该没问题，但是 kubectl apply -f redis.yaml 后，pod 无法创建成功，处于 ImagePullBackOff 状态，查看 describe，报错信息：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m47s default-scheduler Successfully assigned default/redis to minikube Warning Failed 9m31s kubelet Failed to pull image \u0026quot;redis\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: Get \u0026quot;https://registry-1.docker.io/v2/\u0026quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Failed 8m21s (x2 over 9m2s) kubelet Failed to pull image \u0026quot;redis\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: Get \u0026quot;https://registry-1.docker.io/v2/\u0026quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) Normal Pulling 7m26s (x4 over 9m47s) kubelet Pulling image \u0026quot;redis\u0026quot; Warning Failed 7m11s (x4 over 9m31s) kubelet Error: ErrImagePull Warning Failed 7m11s kubelet Failed to pull image \u0026quot;redis\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: Get \u0026quot;https://registry-1.docker.io/v2/\u0026quot;: dial tcp 54.242.59.189:443: i/o timeout Warning Failed 6m56s (x6 over 9m31s) kubelet Error: ImagePullBackOff Normal BackOff 4m37s (x14 over 9m31s) kubelet Back-off pulling image \u0026quot;redis\u0026quot; 但是我使用 docker pull redis 是成功的：\n$ docker pull redis Using default tag: latest latest: Pulling from library/redis 5b1423465504: Pull complete 4216a986e3df: Pull complete f74254280149: Pull complete 64dfe9963acc: Pull complete 097894d6d055: Pull complete b9381c45e088: Pull complete Digest: sha256:495732ba570db6a3626370a1fb949e98273a13d41eb3e26f7ecb1f6e31ad4041 Status: Downloaded newer image for redis:latest docker.io/library/redis:latest 而且我注意到 docker pull 的地址是 library/redis，而 k8s pod get 的地址是 registry-1.docker.io/v2/，因为虚拟机我挂了代理，应该也不是网络的问题。我又将 redis.yaml 里面的 spec.containers[0].image 改为了 library/redis，依旧创建失败，还是会 Get https://registry-1.docker.io/v2/ 这个地址。\n不知道问题出在哪里，试了下用 k3s 执行同样的操作，可以创建成功\n","date":"2022年07月14日","permalink":"/posts/minikube_wen_ti_hui_zong/","summary":"设置代理后，minikube kubectl 无响应 在给虚拟机设置了代理后，整个网络体验爽快多了，再也不用担心拉不下来包了。。。\n只需要将虚拟机的网络设置为桥接模式，然后开启 clashx 的 “允许局域网连接”（非常重要的一步，之前就是因为忽略了这一步，导致虚拟机无法使用宿主机代理），然后再点击 clashx 的 “复制终端代理命令”，将里面的 127.","title":"minikube 问题汇总"},{"contents":" 本笔记主要记录 go rabbitmq 库的使用示例，同时结合学习 rabbitmq\n一个基本的发布/订阅程序示例 首先可以通过一个最基础的程序，来一览 rabbitmq 的大致面貌：\n生产者 代码如下：\npackage main import ( \u0026quot;bufio\u0026quot; amqp \u0026quot;github.com/rabbitmq/amqp091-go\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; ) func Assert(err error, msg string) { if err != nil { log.Panicf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { // 1. 尝试连接 RabbitMQ，建立连接 // 该连接抽象了套接字连接，并为我们处理协议版本协商和认证等。 conn, err := amqp.Dial(\u0026quot;amqp://guest:guest@localhost:5672/\u0026quot;) Assert(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 2. 接下来，我们创建一个通道，大多数 API 都是用过该通道操作的。 ch, err := conn.Channel() Assert(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 3. 声明消息要发送到的队列 q, err := ch.QueueDeclare( \u0026quot;hello\u0026quot;, // name false, // durable 是否为持久化队列 false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) Assert(err, \u0026quot;Failed to declare a queue\u0026quot;) log.Printf(\u0026quot;Enter q or Q to exit.\u0026quot;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { if scanner.Err() != nil { Assert(scanner.Err(), \u0026quot;read stdin error\u0026quot;) } body := scanner.Bytes() if string(body) == \u0026quot;q\u0026quot; || string(body) == \u0026quot;Q\u0026quot; { log.Printf(\u0026quot;bye^\u0026quot;) break } // 4.将消息发布到声明的队列 err = ch.Publish( \u0026quot;\u0026quot;, // exchange q.Name, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: body, // 是否持久化消息，瞬态（0 或 1）或持久（2） DeliveryMode: amqp.Transient, }) Assert(err, \u0026quot;Failed to publish a message\u0026quot;) } } 消费者 package main import ( amqp \u0026quot;github.com/rabbitmq/amqp091-go\u0026quot; \u0026quot;log\u0026quot; ) func Assert(err error, msg string) { if err != nil { log.Panicf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { conn, err := amqp.Dial(\u0026quot;amqp://guest:guest@localhost:5672/\u0026quot;) Assert(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() Assert(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() q, err := ch.QueueDeclare( \u0026quot;hello\u0026quot;, // name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) Assert(err, \u0026quot;Failed to declare a queue\u0026quot;) msgs, err := ch.Consume( q.Name, // queue \u0026quot;\u0026quot;, // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) Assert(err, \u0026quot;Failed to register a consumer\u0026quot;) var forever chan struct{} go func() { for d := range msgs { log.Printf(\u0026quot;Received a message: %s\u0026quot;, d.Body) } }() log.Printf(\u0026quot; [*] Waiting for messages. To exit press CTRL+C\u0026quot;) \u0026lt;-forever } 核心方法 通过上面的程序，可以看到上面代码中有几个核心方法：\nQueueDeclare 用于创建一个 队列，或者连接到一个已存在的队列，需要提供以下参数：\nname：指定队列的名称，如果留空且执行的是创建操作，则会随机分配一个名称 durable：是否持久化 autoDelete: 是否自动删除队列，如果为 true 表示没有消息也没有消费者连接自动删除队列 exclusive: 是否独占，即当前声明队列的连接关闭后即被删除 noWait: 是否等待服务器返回 ok args: Consume queue：要消费的队列的名字 consumer： autoAck：如果为 true，代表 mq 会自动把发送出去的消息置为确认，不会考虑消费者是否真正的消费了这条消息 exclusive noLocal noWait args Publish exchange：交换机的名称，用于确定消息要投放到哪个交换机 key：路由键，类似路由的功能，可以为一个交换机定义多个路由键，通过不同路由键连接到不同的队列 mandatory ： immediate ： msg：要推送的消息，字节类型 临时队列 exchange 交换器 exchange 是消息交换机，它指定消息按什么规则,路由到哪个队列。\n比如下图，x 就是一个交换机，它定义了 3 个 routing_key，routing_key 就是上面说的规则，不同的 routing_key 关联到不同的队列，在下图中，orange 这个 key 关联到了队列 Q1，black 和 green 这两个 key 关联到了队列 Q2，消费者可以根据 routing_key 来消费不同的队列。\nexchange 的程序大致流程是：\n生产者：\n定义或者连接一个 exchange 推送消息到这个 exchange 消费者：\n定义一个队列 将队列和上面的 exchange 绑定 然后就可以从队列中消费了 其实上面的流程不是固定的，只需要记住一个固定的规则即可：消费者只能从队列消费，队列必须绑定一个 exchange，所以队列的定义和绑定（也就是消费者流程的 1，2 步）完全也可以让生产者去做，消费者直接进行消费就可以了，只要保证 rabbitmq 中存在这个已经绑定到 exchange 的队列即可。\n这里可能会有一个疑问：在文章最开始的示例中，并没有创建一个 exchange，也没有进行绑定操作，生产者和消费者都是直接操作队列，这是如何做到的呢？其实在 rabbitmq 中存在一个 默认的 exchange，如果没有指定 exchange，那么就会使用这个默认交换器，对于 routing key 也是一样的，如果没有指定，则使用默认的。\n绑定调用的函数是 QueueBind，其签名如下：\nQueueBind name key exchange noWait args exchange 有多种类型，下面会详细介绍，可以抽象出一个通用的程序，通过命令行参数来定义不同的 exchange\n通用代码 生产者 package main import ( \u0026quot;bufio\u0026quot; \u0026quot;bytes\u0026quot; \u0026quot;errors\u0026quot; \u0026quot;flag\u0026quot; amqp \u0026quot;github.com/rabbitmq/amqp091-go\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; ) type ExchangeKind = string const ( ExchangeKind_Direct ExchangeKind = \u0026quot;direct\u0026quot; ExchangeKind_Topic ExchangeKind = \u0026quot;topic\u0026quot; ExchangeKind_Headers ExchangeKind = \u0026quot;headers\u0026quot; ExchangeKind_Fanout ExchangeKind = \u0026quot;fanout\u0026quot; ) func Assert(err error, msg string) { if err != nil { log.Panicf(\u0026quot;%s: %s\u0026quot;, msg, err) } } // 检查输入的 exchange kind 是否合法 func checkExchangeKind(kind string) error { m := map[string]bool{ ExchangeKind_Direct: true, ExchangeKind_Topic: true, ExchangeKind_Headers: true, ExchangeKind_Fanout: true, } if exist := m[kind]; exist { return nil } return errors.New(\u0026quot;not found this kind\u0026quot;) } func main() { log.Printf(\u0026quot;usage: ./main -e [exchange name] -k [exchange kind]\u0026quot;) var ( exchangeName = flag.String(\u0026quot;e\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;exchange name\u0026quot;) exchangeKind = flag.String(\u0026quot;k\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;exchange kind\u0026quot;) ) flag.Parse() if err := checkExchangeKind(*exchangeKind); err != nil { Assert(err, \u0026quot;check exchange kind error: \u0026quot;) } log.Printf(\u0026quot;exchangeName: %v, exchangeKind: %v\\n\u0026quot;, *exchangeName, *exchangeKind) // 1. 尝试连接 RabbitMQ，建立连接 // 该连接抽象了套接字连接，并为我们处理协议版本协商和认证等。 conn, err := amqp.Dial(\u0026quot;amqp://guest:guest@localhost:5672/\u0026quot;) Assert(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() // 2. 接下来，我们创建一个通道，大多数 API 都是用过该通道操作的。 ch, err := conn.Channel() Assert(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 声明一个交换器 exchange err = ch.ExchangeDeclare( *exchangeName, // name 交换器名 *exchangeKind, // type 交换器类型 true, // durable false, // auto-deleted false, // internal false, // no-wait nil, // arguments ) Assert(err, \u0026quot;Failed to declare a exchange\u0026quot;) log.Printf(\u0026quot;Enter q or Q to exit.\u0026quot;) log.Printf(\u0026quot;usage: \u0026lt;routing_key\u0026gt;:\u0026lt;message\u0026gt;\u0026quot;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { if scanner.Err() != nil { Assert(scanner.Err(), \u0026quot;read stdin error\u0026quot;) } body := scanner.Bytes() if string(body) == \u0026quot;q\u0026quot; || string(body) == \u0026quot;Q\u0026quot; { log.Printf(\u0026quot;bye^\u0026quot;) break } // 输入的格式：\u0026lt;路由键\u0026gt;:\u0026lt;消息实体\u0026gt; before, after, found := bytes.Cut(body, []byte(\u0026quot;:\u0026quot;)) if !found { log.Printf(\u0026quot;usage: \u0026lt;routing_key\u0026gt;:\u0026lt;message\u0026gt;\u0026quot;) continue } log.Printf(\u0026quot;routingKey: %s, message: %s\\n\u0026quot;, before, after) // 4.将消息发布到 exchange 而不是某个单独的队列 err = ch.Publish( *exchangeName, // exchange string(before), // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType: \u0026quot;text/plain\u0026quot;, Body: after, // 是否持久化消息，瞬态（0 或 1）或持久（2） DeliveryMode: amqp.Transient, }) Assert(err, \u0026quot;Failed to publish a message\u0026quot;) } } 使用方法：\n./producer_exchange -e [exchange name] -k [exchange kind] 使用示例：\n# 推送消息到名为 logs_direct ，类型为 direct 的 exchange ./producer_exchange -e logs_direct -k direct # 推送消息到名为 logs_topic ，类型为 topic 的 exchange ./producer_exchange -e logs_topic -k topic 消费者 package main import ( \u0026quot;flag\u0026quot; \u0026quot;fmt\u0026quot; amqp \u0026quot;github.com/rabbitmq/amqp091-go\u0026quot; \u0026quot;log\u0026quot; . \u0026quot;rabbitmq\u0026quot; \u0026quot;strings\u0026quot; ) type routingKeys []string func (s *routingKeys) String() string { return fmt.Sprintf(\u0026quot;%v\u0026quot;, *s) } func (s *routingKeys) Set(value string) error { ss := strings.Split(value, \u0026quot;,\u0026quot;) *s = append(*s, ss...) return nil } func Assert(err error, msg string) { if err != nil { log.Panicf(\u0026quot;%s: %s\u0026quot;, msg, err) } } func main() { log.Printf(` usage: ./main -q [queue name] -e [exchange name] -r [routing key, can provide more, use ',' to sep, example: info,error,warning (don't have space) ] `) var ( exchangeName = flag.String(\u0026quot;e\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;exchange name\u0026quot;) queueName = flag.String(\u0026quot;q\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;queue name\u0026quot;) rks routingKeys ) flag.Var(\u0026amp;rks, \u0026quot;r\u0026quot;, \u0026quot;routing key\u0026quot;) flag.Parse() log.Printf(\u0026quot;queueName: %v, exchangeName: %v, routingKey: %v\\n\u0026quot;, *queueName, *exchangeName, \u0026amp;rks) conn, err := amqp.Dial(\u0026quot;amqp://guest:guest@localhost:5672/\u0026quot;) Assert(err, \u0026quot;Failed to connect to RabbitMQ\u0026quot;) defer conn.Close() ch, err := conn.Channel() Assert(err, \u0026quot;Failed to open a channel\u0026quot;) defer ch.Close() // 声明一个临时队列，一旦消费者断开连接，该队列就会被删除 q, err := ch.QueueDeclare( *queueName, // name 空字符串作为队列名称，表示使用随机名称 false, // durable false, // delete when unused true, // exclusive 独占队列（当前声明队列的连接关闭后即被删除） false, // no-wait nil, // arguments ) Assert(err, \u0026quot;Failed to declare a queue\u0026quot;) if len(rks) == 0 { err = ch.QueueBind( q.Name, \u0026quot;\u0026quot;, *exchangeName, false, nil, ) Assert(err, \u0026quot;Failed to bind a queue\u0026quot;) } else { for _, key := range rks { // 将 queue 绑定到对应的 exchange，使用 exchangeName + routingKeys 进行匹配 err = ch.QueueBind( q.Name, key, *exchangeName, false, nil, ) Assert(err, \u0026quot;Failed to bind a queue\u0026quot;) } } msgs, err := ch.Consume( q.Name, // queue \u0026quot;\u0026quot;, // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) Assert(err, \u0026quot;Failed to register a consumer\u0026quot;) var forever chan struct{} go func() { for d := range msgs { // 如果您调用 Channel.Consume 时将 autoAck 设置为 true，那么服务器将自动确认 // 每条消息，并且不应调用此方法。否则，您必须在成功处理此交付后调用 Delivery.Ack。 // 参数是什么意思？ // d.Ack(false) log.Printf(\u0026quot;Received a message: %s\u0026quot;, d.Body) } }() log.Printf(\u0026quot; [*] Waiting for messages. To exit press CTRL+C\u0026quot;) \u0026lt;-forever } 使用方法：\nusage: ./main -q [queue name] -e [exchange name] -r [routing key, 可以提供多个, 使用 , 来分隔, 比如: info,error,warning (注意 , 前后不要有空格) ] 使用示例：\n// ./consumer_exchange -e logs_direct -r error // ./consumer_exchange -e logs_direct -r error,info,warning // // ./consumer_exchange -e logs_topic -r \u0026quot;*.orange.*\u0026quot; // ./consumer_exchange -e logs_topic -r \u0026quot;*.*.rabbit\u0026quot;,\u0026quot;lazy.#\u0026quot; fanout fanout 类型的 exchange 提供广播功能，即将消息投递到所有与该 exchange 所绑定的 queue 上，此时指定的 routing key 会被忽略\n示例 这里直接使用上面的通用代码，只要在运行时指定不同的 flag 即可\n生产者 ./producer_exchange -e logs -k fanout 消费者 1 因为 fanout 会忽略 routing key，所以 -r 也不需要指定了\n./consumer_exchange -e logs 消费者 2 ./consumer_exchange -e logs 先执行生产者，再执行消费者1，2\n生产者执行后在终端输入消息（消息格式是 \u0026lt;路由键\u0026gt;:\u0026lt;消息\u0026gt;，因为是 fanout 类型，所以无需指定路由键）：\n$ ./producer_exchange -e logs -k fanout 2022/07/24 12:16:22 usage: ./main -e [exchange name] -k [exchange kind] 2022/07/24 12:16:22 exchangeName: logs, exchangeKind: fanout 2022/07/24 12:16:22 Enter q or Q to exit. 2022/07/24 12:16:22 usage: \u0026lt;routing_key\u0026gt;:\u0026lt;message\u0026gt; :123 2022/07/24 12:20:27 routingKey: , message: 123 之后观察两个消费者的 terminal，发现都收到了消息：\n$ ./consumer_exchange -e logs 2022/07/24 12:20:23 usage: ./main -q [queue name] -e [exchange name] -r [routing key, can provide more, use ',' to sep, example: info,error,warning (don't have space) ] 2022/07/24 12:20:23 queueName: , exchangeName: logs, routingKey: [] 2022/07/24 12:20:23 [*] Waiting for messages. To exit press CTRL+C 2022/07/24 12:20:27 Received a message: 123 direct direct 类型的 exchange 会严格按照 routing key 进行精准匹配\ntopic topic 类型的 exchange 提供了 routing key 的通配符匹配功能，支持 * 和 # 语法，\u0026quot;#\u0026quot; 代表匹配一个或多个单词，\u0026quot;*\u0026quot; 则匹配不多不少刚好一个单词\n示例 这里直接使用上面的通用代码，只要在运行时指定不同的 flag 即可\n生产者 $ ./producer_exchange -e logs_topic -k topic # 开始输入 quick.orange.rabbit:666 lazy.orange.elephant:aaa quick.orange.fox:888 消费者 1 这个消费者消费 routing key 匹配 *.orange.* 的消息，表示单词个数必须是 3 个，且中间的必须是 orange\n$ ./consumer_exchange -e logs_topic -r \u0026quot;*.orange.*\u0026quot; 消费者 2 注意这里要用 \u0026quot;\u0026quot; 将 routing key 包裹起来，否则 * 会被 shell 错误解析\n*.*.rabbit\t表示单词个数必须是 3 个，且最后一个单词必须是 rabbit\nlazy.# 表示的是只要第一个单词为 lazy 即可，之后的内容忽略\n$ ./consumer_exchange -e logs_topic -r \u0026quot;*.*.rabbit\u0026quot;,\u0026quot;lazy.#\u0026quot; 匹配情况：\n// \u0026gt; quick.orange.rabbit:666\t=\u0026gt; *.orange.* | *.*.rabbit //\t\u0026gt; lazy.orange.elephant:aaa\t=\u0026gt;\t*.orange.* //\t\u0026gt; quick.orange.fox:888\t=\u0026gt;\t*.orange.* //\t\u0026gt; lazy.brown.fox:666\t=\u0026gt;\tlazy.# //\t\u0026gt; lazy.pink.rabbit:xxx\t=\u0026gt;\tlazy.# | *.*.rabbit //\t\u0026gt; quick.brown.fox:yyy\t=\u0026gt; //\t\u0026gt; orange:ccc\t=\u0026gt; // \u0026gt; quick.orange.male.rabbit:bbb\t=\u0026gt; //\t\u0026gt; lazy.orange.male.rabbit:ooo\t=\u0026gt;\tlazy.# //\t\u0026gt; lazy.orange.male.rabbit.io:zzz\t=\u0026gt;\tlazy.# 可以按照上表的内容自己实践一下，看看不同的 routing_key 会传递给哪个消费者\n延迟队列 延迟队列可以用于订单超时取消的场景，可以用 rabbitmq 的死信队列来实现一个延迟队列。\n死信队列，顾名思义，就是用来存储那些无法被消费的，已经“死掉“的消息，和普通队列一样，也是由 exchange、routing_key 和 queue 三大组件构成的。\n什么条件下会将消息放到死信队列？\n消息被拒绝，并且 requeue 参数为 false，这个参数代表是否将被拒绝的消息重新放回队列，让其他消费者进行消费 消息的 TTL 过期 队列达到最大长度或队列空间已满，此时需要 queue 的拒绝策略设置为 reject-public-dlx，后续投递至该队列的消息会重新投递到死信队列中 这里需要注意的一点是：消息的 TTL 和消息的 ACK 是不相干的，比如设置了消息的 TTL 为 5s，但是消息超过 5s 没有回复 ACK，此时是不会判定这条消息过期的，引用书上的一段话：RabbitMQ 不会为未确认的消息设置过期时间，它判断此消息是否需要重新投递给消费者的唯一依据是消费该消息的消费者连接是否己经断开，这么设计的原因是 RabbitMQ 允许消费者费一条消息的时间可以很久很久。\n消息过期的依据是：如果一条消息超过 TTL 还没有被消费，则判断为过期。而 ACK 是在已经消费的情况下才能回复的东西，所以这两个东西当然不相干。\n之所以会记录上面的问题，是因为我在写代码时犯了这个错误，我在消费者开了两个线程，分别消费普通队列和死信队列，生产者这边会先将消息投放到普通队列，然后我的逻辑是消息普通队列的时候加一个随机的 sleep，让部分消息的 ACK 超过 TTL 的时间，从而让消息超时进入死信队列，但是运行时我发现消息始终不会进入死信队列，除非是直接结束掉消费者进程，然后生产者这边投放几条消息，等过了 TTL 时间后再运行消费者进程，此时就会从死信队列里消费到过期消息。\n实现延迟队列的基本逻辑\n生产者：\n创建一个正常交换机 创建一个死信交换机 创建一个正常队列，并绑定正常交换机和 routing_key，同时设置以下几个参数： \u0026quot;x-message-ttl\u0026quot;: 5000, // 消息过期时间,毫秒 \u0026quot;x-dead-letter-exchange\u0026quot;: dlxExchangeName, // 指定死信交换机 \u0026quot;x-dead-letter-routing-key\u0026quot;: dlxRoutingKey, // 指定死信 routing-key 创建一个死信队列，并绑定死信交换机和 routing_key 推送消息到正常交换机 消费者：\n因为队列和交换机都已经由生产者创建好了，所以消费者这边直接消费即可，并且只需要从死信交换机进行消费即可，这样一来，生产者放到正常队列里的消息在 TTL 之后过期，会放到死信队列，这样消费者就可以直接进行消费了。\n比如订单超时未支付自动取消的场景，当用户提交订单时，生产者将订单信息放到正常队列，同时设置过期时间，当到达时间后进入死信队列，消费者这边就可以开始进行消费了，具体的业务逻辑这里就不展开了，比如可以查询一下订单 ID，看支付状态是否为未支付，如果未支付就说明可以取消这个订单了。\n架构及组件 ","date":"2022年07月14日","permalink":"/posts/go-rabbitmq/","summary":"本笔记主要记录 go rabbitmq 库的使用示例，同时结合学习 rabbitmq","title":"rabbitmq 学习笔记"},{"contents":"引用自 cloudflare 学习中心\nTLS 握手有哪些步骤？ TLS 握手是由客户端和服务器交换的一系列数据报或消息。TLS 握手涉及多个步骤，因为客户端和服务器要交换完成握手和进行进一步对话所需的信息。\nTLS 握手的确切步骤将根据所使用的密钥交换算法的类型以及双方支持的密码套件而有所不同。RSA 密钥交换算法最为常用。具体如下：\n“客户端问候（client hello）” 消息： 客户端通过向服务器发送“问候”消息来开始握手。该消息将包含客户端支持的 TLS 版本，支持的密码套件，以及称为一串称为“客户端随机数（client random）”的随机字节。 “服务器问候（server hello）”消息： 作为对 client hello 消息的回复，服务器发送一条消息，内含服务器的 SSL 证书、服务器选择的密码套件，以及“服务器随机数（server random）”，即由服务器生成的另一串随机字节。 身份验证： 客户端使用颁发该证书的证书颁发机构验证服务器的 SSL 证书。此举确认服务器是其声称的身份，且客户端正在与该域的实际所有者进行交互。 预主密钥： 客户端再发送一串随机字节，即“预主密钥（premaster secret）”。预主密钥是使用公钥加密的，只能使用服务器的私钥解密。（客户端从服务器的 SSL 证书中获得公钥。） **私钥被使用：**服务器对预主密钥进行解密。 **生成会话密钥：**客户端和服务器均使用客户端随机数、服务器随机数和预主密钥生成会话密钥。双方应得到相同的结果。 **客户端就绪：**客户端发送一条“已完成”消息，该消息用会话密钥加密。 **服务器就绪：**服务器发送一条“已完成”消息，该消息用会话密钥加密。 **实现安全对称加密：**已完成握手，并使用会话密钥继续进行通信。 图片来源：https://segmentfault.com/a/1190000021559557\n","date":"2022年07月14日","permalink":"/posts/tls_wo_shou/","summary":"引用自 cloudflare 学习中心\nTLS 握手有哪些步骤？ TLS 握手是由客户端和服务器交换的一系列数据报或消息。TLS 握手涉及多个步骤，因为客户端和服务器要交换完成握手和进行进一步对话所需的信息。","title":"TLS 握手流程"},{"contents":" 从请求 URL 中获取请求域名，通过 DNS 获取域名对应的 IP 地址\n首先需要知道 DNS 域名服务器 的 IP 地址，这样才能给它发送 DNS 请求报文，而根据网络模型，光有 IP 地址是不够的，还需要知道 MAC 地址才能构建数据链路层的报文，所以需要通过 ARP 协议来根据 IP 获取对应的 MAC 地址。域名服务器的 IP 地址一般是默认配置好的，所以这里假定已经知道了 IP 地址，只需要获取 MAC 地址。\n首先，主机会先检查一下自己的 ARP 表，看看有没有该 IP 缓存记录，有的话可以直接获取到对应的 MAC 地址，如果没有，就需要构建一个 ARP 请求报文，源 IP 和源 MAC 填写自己的，目的 IP 填写域名服务器的，目的 MAC 填写广播地址（FF-FF-FF-FF-FF-FF-FF），然后这个报文会发送给当前子网的所有主机，每个主机会查看报文里的目的 IP 地址是否与自己的 IP 地址相同，如果相同的话，就会回复 ARP 响应报文，里面有自己的 MAC 地址；如果不相同则会直接丢弃报文。主机获得域名服务器的 MAC 地址后，记录到自己的 ARP 表中，至此，主机已经获取到域名服务器的 MAC 地址了。\n知道了域名服务器的 IP 地址和 MAC 地址后，就可以向其发送 DNS 请求报文了，DNS 请求报文在运输层被封装为 UDP 用户数据报，目的端口号为 53；UDP 用户数据报在网络层被封装为 IP 数据包，目的 IP 地址为域名服务器的 IP 地址；IP 数据报被封装在以太单播帧中发送，目的 MAC 地址为之前 ARP 请求获得的 MAC 地址，该单播帧通过交换机转发给本地域名服务器，本地域名服务器在收到报文后逐层解封，知道这是一个 DNS 请求，于是进入域名解析阶段。给主机发送 DNS 响应报文，里面包含了域名对应的 IP 地址。\n域名解析\n根据域名的层级关系，由大到小为：根域、顶级域、权威域，层级代表的意思是：高层级里面保存了下一层级的域名到 IP 的映射记录，对应到域名是从右往左，也就是右边最大，左边最小，比如 www.baidu.com 这个域名，（最高层的根域在这个域名里没有体现出来，看了下别的博客，说的是实际是最后还有一个隐藏的 . 代表的就是根域名，这里暂时先不研究了），根域里保存的下一级顶级域的记录，也就是最右边的 .com 的记录，而 .com 又保存了 baidu.com 这个权威域的记录，最终再从 baidu.com 这个权威域里面找到最终的域名 www.baidu.com 的记录 构建 HTTP 请求报文\n运输层封装应用层的 HTTP 报文\n因为 HTTP 底层使用的运输层协议是 TCP，所以会将应用层的报文封装到一个 TCP 报文中，然后添加上 TCP 的报文头，目的端口通过请求的 URL 获取，如果没有指定，则使用 HTTP 默认的 80 端口，或者 HTTPS 的默认端口 443，源端口取决于是否显式指定，如果没有则会由系统随机分配一个。 然后就可以发起三次握手来建立连接了 网络层封装传输层的 TCP 报文\n和运输层类似，将 TCP 报文封装到一个 IP 数据报中，添加上 IP 的报文头，源地址填写自己的 IP，目的地址填写之前从 DNS 获取到的 IP MAC\n可以通过 ARP 获取请求 IP 对应的 MAC 地址，这里有个问题：ARP 只能在一个子网内工作，如果请求的 IP 不是一个子网内的，此时该如何获取对应的 MAC 地址呢？原来在每个子网中，都配置了一台网关路由器，它连接了其他几个子网的交换机，当请求的 IP 不属于同一个子网时，就需要这台网关路由器来做转发（也就是将 MAC 地址填写为这个网关路由器的 MAC 地址），传递到另一个子网，具体如何转发取决于路由表，整个流程就像坐公交使用地图导航一样，选择一条路径，在不同的公交站间穿梭前进，网络也是这样的，多个子网相互连接组成一个大网络，数据报在这些互联的子网中选择一条道路前进，经过一个个子网，最终到达目的地。这里也发现了一个问题：**在网络包传输的过程中，源 IP 和目标 IP 始终是不会变的，一直变化的是 MAC 地址，因为需要 MAC 地址在以太网内进行两个设备之间的包传输。**可能这也是为什么需要 IP 和 MAC 两个地址的一点原因。 ","date":"2022年07月14日","permalink":"/posts/url_dao_xian_shi_wang_ye_fa_shen_le_shen_me/","summary":"从请求 URL 中获取请求域名，通过 DNS 获取域名对应的 IP 地址","title":"输入 url 到浏览器显示网页，期间发生了什么？（未完）"},{"contents":" 参考： https://www.bilibili.com/video/BV15V411n7fM?p=2\u0026amp;spm_id_from=pageDriver\u0026amp;vd_source=2ff613424b86c58a71ba91b7304ffe9b\n总览 mutex 的数据结构：\ntype Mutex struct { state int32 sema uint32 } mutex 零值可用，代表一个未锁定的锁。\nstate 存储的是互斥锁的状态，加锁和解锁都是通过 cas 来实现的 加锁：atomic.CompareAndSwapInt32(\u0026amp;m.state, 0, mutexLocked)，其中 mutexLocked = 1 解锁：atomic.AddInt32(\u0026amp;m.state, -mutexLocked)，代表将 state 变为 0，因为只有加锁状态下才能解锁，如果对一个未加锁的锁执行解锁操作会 panic，而加锁状态下的 state = 1，这个操作又是将 state - 1，所以会将 state 变为 0。\nsema 用作一个信号量，主要用作等待队列\nmutex 有两种模式：正常模式 和 饥饿模式\n在 正常模式 下，一个尝试加锁的 goroutine 会先自旋几次，尝试通过原子操作获得锁，若几次自旋后仍然不能获得锁，则通过信号量排队等待（？啥叫通过信号量排队，信号量是如何工作的？），所有的等待者会按照先入先出的顺序排队。\n但是当锁释放，第一个等待者被唤醒后并不会直接拥有锁，而是要和后来者竞争，后来者也就是新的 goroutine，它们处于自旋状态，且尚未排队，这些后来者因为占有了 CPU，且数量可能不少，而队列中被唤醒的 goroutine 只有一个，所以被唤醒的 goroutine 大概率抢不到锁，这种情况下，它会重新插入到队列的头部，而不是尾部。\n当一个 goroutine 本次加锁等待时间超过了 1ms 后，它会把当前 mutex 从正常模式切换到 饥饿模式，在饥饿模式下，mutex 的所有权从执行 unlock 的 goroutine 直接传递给等待队列头部的 goroutine，后来者不会自旋，也不会尝试获得锁，而是直接到队尾排队等待。\n何时从饥饿模式切换到正常模式 当一个等待者获得锁之后，它会在以下情况将锁从饥饿模式切换到正常模式：\n它的等待时间小于 1ms 它是队列的最后一个元素，也就是除自己以外没有其他等待者了，自然也就没有饥饿的 goroutine 了。 正常模式下性能更好，但是可能会出现队列尾端的 goroutine 迟迟抢不到锁的情况，也就是尾端延迟，而饥饿模式下，goroutine 不再自旋，所有 goroutine 都要排队，严格的先来后到，可以有效解决尾端延迟问题，但是性能会差一些。\n","date":"2022年07月12日","permalink":"/posts/go-mutex-yuan-ma/","summary":"参考： https://www.bilibili.com/video/BV15V411n7fM?p=2\u0026amp;spm_id_from=pageDriver\u0026amp;vd_source=2ff613424b86c58a71ba91b7304ffe9b\n总览 mutex 的数据结构：","title":"go mutex 原理"},{"contents":"chan 原理（人话版） 说明：这里只是用大白话来阐述 chan 的大致原理，不保证严谨性，也不考虑一些繁枝细节（比如 sudog，gopark，goready），仅作为一个简单的参考理解，如果想深入理解 chan 的底层实现，请自行参阅其他博客\nchan 底层有两个队列：sendq 和 recvq，这两个队列保存的都是被阻塞的 goroutine（简称 g），顾名思义，sendq 里面保存的是因为发送操作而被阻塞的 g，而 recvq 保存的是因为接收操作被阻塞的 g，比如：\nfunc main() { ch := make(chan int) ch \u0026lt;- 1 } 因为没有接收者，所以 ch \u0026lt;- 1 这个操作会被阻塞，那么就会把执行这个 chan 的 g 给放到 sendq 中\n当执行 发送 操作时，会先看看 recvq 中有没有因为接收操作而被阻塞的 g，如果有的话，那么出队 g，并将数据拷贝给它，之后再将其唤醒；\n同理，当执行 接收 操作时，会先看看 sendq 中有没有因为发送而被阻塞的 g，如果有，则执行和上面相同的操作。\n比如：\nfunc main() { ch := make(chan int) ch \u0026lt;- 1 go func() { v := \u0026lt;-ch }() } ch \u0026lt;- 1 这个操作对应的 main goroutine 被阻塞，会放到 ch 的 sendq 中，之后的 \u0026lt;-ch 会检查 ch 的 sendq，发现里面有元素，那么就将这个元素取出来，并将值拷贝给接收者 v，并唤醒 main goroutine ，让其不再是阻塞状态。\n用过 chan 的同学都知道这是一个并发安全的数据结构，那么这里是如何实现的呢？其实也是加锁，chan 的底层有一个 mutex 锁来对其他字段进行保护。\n此外，上面提到的发送和接收操作都是基于无缓存的 chan，如果是有缓存的 chan，那么流程会有所不同。\n缓存（简称 buf）在底层其实是一个 环形队列，这是一个非常经典的数据结构，这里就不多阐述了，发送和接收的流程如下：\n发送：\n先看看 recvq 中有没有元素，有的话直接按照之前的操作，将数据拷贝给对应的接收者，就不需要经过缓存这一部分了 如果 recvq 中没有元素，则先检查缓存是否已满，则将数据拷贝到缓存中；如果缓存已满，那么将当前的 g 阻塞并放到 sendq 中 接收：\n如果 sendq 中有元素，此时又分为两种情况： 无缓存 chan，直接取出 sendq 中的队头 g，并将 g 的数据拷贝给当前的接收者，并唤醒该 g 有缓存 chan，先从缓存中取出一个数据（通过 recvx 取出），并拷贝给接收者，此时缓存中相当于空了一个位置（环形队列，可以用覆盖值代替显式删除），所以可以取出 sendq 中的队头元素，放到这个空位里 如果 sendq 为空： 看缓存里有没有值，有的话则取出，并执行拷贝操作 缓存中没有值，将当前执行接收操作的 g 阻塞并放到 recvq 中 buf 底层是一个数组，通过两个变量 sendx 和 recvx 来完成 环形 的操作，通过下图来进行说明：\n附：部分源码注释 hchan // channel 的数据结构 type hchan struct { // 环形队列（buf）中元素的个数 qcount uint // total data in the queue // 环形队列容量，即可以存放的元素个数 // 即 make(chan int, 2) 的第二个参数，如果不指定则为 0 dataqsiz uint // size of the circular queue // 指针，指向环形队列头节点 buf unsafe.Pointer // points to an array of dataqsiz elements // 队列中每个元素的大小 elemsize uint16 // chan 是否关闭 closed uint32 // 队列中的元素数据类型 elemtype *_type // element type // 队列已发送位置的索引 sendx uint // send index // 队列已接收位置的索引 recvx uint // receive index // sendq 和 recvq 存储了当前 Channel 由于缓冲区空间不足而阻塞的 Goroutine 列表， // 这些等待队列使用双向链表 runtime.waitq 表示： // type waitq struct { //\tfirst *sudog //\tlast *sudog // } // // 链表中所有的元素都是 runtime.sudog 结构： // 等待读消息的 goroutine 队列 // 存储试图从 channel 接收数据(\u0026lt;-ch)的阻塞 goroutines recvq waitq // list of recv waiters // 等待写消息的 goroutine 队列 // 存储试图发送数据(ch\u0026lt;-)到 channel 的阻塞 goroutines sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } chansend // 发送数据到 channel 时，直观的理解是将数据放到 chan 的环形队列中，不过 go 做了一些优化： // 先判断是否有等待接收数据的 groutine，如果有，直接将数据发给 Groutine，唤醒 groutine， // 就不放入队列中了。当然还有另外一种情况就是：buf 如果满了，那就只能放到队列中等待，直到有 // 数据被取走才能发送。 // 核心逻辑： // 如果 recvq 不为空，从 recvq 中取出一个等待接收数据的 Groutine，将数据发送给该 Groutine // 如果 recvq 为空，才将数据放入 buf 中 // 如果 buf 已满，则将要发送的数据和当前的 Groutine 打包成 Sudog 对象放入 sendq， // 并将 groutine 置为等待状态 // ep 指向要发送数据的首地址 // c chan // block 是否阻塞 // callerpc 调用地址 // // 比较迷惑的是这个 block 参数，block 是为了实现如下代码的语义： // 因为加了 default，所以是非阻塞 // c := make(chan int) // // ... // select { // case \u0026lt;-c: // // ... // default: // // ... // } // // 上面这段代码被编译成对 selectnbsend 的调用： // if selectnbsend(c, v) { // ... foo //\t} else { // ... bar //\t} // // selectnbsend 的实现如下 // func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool) { // block 设置为 false // return chansend(c, elem, false, getcallerpc()) // 非阻塞的发送 // } func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\u0026quot;unreachable\u0026quot;) } if debugChan { print(\u0026quot;chansend: chan=\u0026quot;, c, \u0026quot;\\n\u0026quot;) } if raceenabled { racereadpc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(chansend)) } // Fast path: check for failed non-blocking operation without acquiring the lock. // // After observing that the channel is not closed, we observe that the channel is // not ready for sending. Each of these observations is a single word-sized read // (first c.closed and second full()). // Because a closed channel cannot transition from 'ready for sending' to // 'not ready for sending', even if the channel is closed between the two observations, // they imply a moment between the two when the channel was both not yet closed // and not ready for sending. We behave as if we observed the channel at that moment, // and report that the send cannot proceed. // // It is okay if the reads are reordered here: if we observe that the channel is not // ready for sending and then observe that it is not closed, that implies that the // channel wasn't closed during the first observation. However, nothing here // guarantees forward progress. We rely on the side effects of lock release in // chanrecv() and closechan() to update this thread's view of c.closed and full(). if !block \u0026amp;\u0026amp; c.closed == 0 \u0026amp;\u0026amp; full(c) { return false } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } // 加锁 lock(\u0026amp;c.lock) // 如果向已关闭的 channel 中发送数据，会引发 panic if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026quot;send on closed channel\u0026quot;)) } // blog1. 从接收队列中（存放等待接收的 goroutine）取出 goroutine， // 如果取到数据，就将数据传过去 // blog3. 如果目标 Channel 没有被关闭并且已经有处于读等待的 Goroutine， // 那么 runtime.chansend 会从接收队列 recvq 中取出最先陷入等待的 Goroutine // 并直接向它发送数据： // 如果 recvq 不为空，从 recvq 中取出一个等待接收数据的 Groutine，将数据发送给该 Groutine if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). // 找到一个等待的接收者。我们将要发送的值直接传递给接收者，绕过 chan buf（如果有） send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true // 返回 } // 到这里说明 recvq 为空，没有等待接收的 goroutine，此时会判断 hchan.buf 是否可用 // 对于有缓冲的 channel 来说，如果当前缓冲区 hchan.buf 有可用空间，那么会将数据拷贝至缓冲区 // 如果用户使用的是无缓冲 channel，则 c.dataqsiz = 0，不满足 c.qcount \u0026lt; c.dataqsiz 条件 if c.qcount \u0026lt; c.dataqsiz { // c.sendx 是已发送的索引位置，这个方法通过指针偏移找到索引位置 // 相当于 c.buf[c.sendx] // 计算出下一个可以存储数据的位置 // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { racenotify(c, c.sendx, nil) } // typememmove 会调用 memmove 方法，完成数据的拷贝工作，是用汇编实现的 // 将要发送的数据拷贝到 buf 中 typedmemmove(c.elemtype, qp, ep) // 发送索引号 + 1 c.sendx++ // 因为存储数据元素的结构是循环队列，所以当当前索引号已经到队末时，将索引号调整到队头 if c.sendx == c.dataqsiz { c.sendx = 0 } // 当前循环队列中存储元素数 + 1 c.qcount++ unlock(\u0026amp;c.lock) return true } // 走到这里，说明缓冲区也写满了 // 对于非阻塞的情况，直接返回 if !block { unlock(\u0026amp;c.lock) return false } // 代码执行到这里，意味着： // 1. 如果是有缓冲的 channel，则当前 hchan.buf 已满； // 2. 如果是无缓冲的 channel，则当前没有接收的 goroutine // 此时会将当前发送 goroutine 置于 sendq 中排队，并在运行时中挂起。 // Block on the channel. Some receiver will complete our operation for us. gp := getg() // 获取当前执行的 goroutine // 执行 runtime.acquireSudog 获取 runtime.sudog 结构并设置这一次阻塞发送的相关信息， // 例如发送的 Channel、是否在 select 中和待发送数据的内存地址等 mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil // 将要发送的数据和当前 goroutine 打包成 sudog 对象放入到 sendq 中 c.sendq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(\u0026amp;gp.parkingOnChan, 1) // gopark 将当前 goroutine 转为 waiting 态 // 在用户看来，向 channel 发送数据的代码语句会阻塞 gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // Ensure the value being sent is kept alive until the // receiver copies it out. The sudog has a pointer to the // stack object, but sudogs aren't considered as roots of the // stack tracer. KeepAlive(ep) // someone woke us up. if mysg != gp.waiting { throw(\u0026quot;G waiting list is corrupted\u0026quot;) } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) if closed { if c.closed == 0 { throw(\u0026quot;chansend: spurious wakeup\u0026quot;) } panic(plainError(\u0026quot;send on closed channel\u0026quot;)) } return true } send // send processes a send operation on an empty channel c. // The value ep sent by the sender is copied to the receiver sg. // The receiver is then woken up to go on its merry way. // Channel c must be empty and locked. send unlocks c with unlockf. // sg must already be dequeued from c. // ep must be non-nil and point to the heap or the caller's stack. // // send 在 recvq 不为空时调用，sg 就是从 recvq 中取出的 func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if raceenabled { if c.dataqsiz == 0 { racesync(c, sg) } else { // Pretend we go through the buffer, even though // we copy directly. Note that we need to increment // the head/tail locations only when raceenabled. racenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz } } if sg.elem != nil { // ep -\u0026gt; sg sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } // 使得之前在接收等待队列中的第一个 goroutine 的状态变为 runnable， // 这样 go 的调度器就可以重新让该 goroutine 得到执行。 goready(gp, skip+1) } chanrecv // chanrecv receives on channel c and writes the received data to ep. // ep may be nil, in which case received data is ignored. // If block == false and no elements are available, returns (false, false). // Otherwise, if c is closed, zeros *ep and returns (true, false). // Otherwise, fills in *ep with an element and returns (true, true). // A non-nil ep must point to the heap or the caller's stack. // // 在通道 c 上接收并将接收到的数据写入 ep，ep 可能为 nil，在这种情况下，接收到的数据会被忽略， // 如果 block == false 并且没有可用的元素，则返回 (false, false)， // 否则，如果 c 关闭，则将 ep 清零并返回 (true, false)。 // 否则，用一个元素填充 ep 并返回 (true, true)。 // 非 nil ep 必须指向堆或调用者的堆栈。 // block 貌似是用于 select func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { // raceenabled: don't need to check ep, as it is always on the stack // or is new memory allocated by reflect. if debugChan { print(\u0026quot;chanrecv: chan=\u0026quot;, c, \u0026quot;\\n\u0026quot;) } if c == nil { // 如果 c 为空且是非阻塞调用，那么直接返回 (false,false) if !block { return } // 阻塞调用直接等待 gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\u0026quot;unreachable\u0026quot;) } // Fast path: check for failed non-blocking operation without acquiring the lock. if !block \u0026amp;\u0026amp; empty(c) { // After observing that the channel is not ready for receiving, we observe whether the // channel is closed. // // Reordering of these checks could lead to incorrect behavior when racing with a close. // For example, if the channel was open and not empty, was closed, and then drained, // reordered reads could incorrectly indicate \u0026quot;open and empty\u0026quot;. To prevent reordering, // we use atomic loads for both checks, and rely on emptying and closing to happen in // separate critical sections under the same lock. This assumption fails when closing // an unbuffered channel with a blocked send, but that is an error condition anyway. if atomic.Load(\u0026amp;c.closed) == 0 { // Because a channel cannot be reopened, the later observation of the channel // being not closed implies that it was also not closed at the moment of the // first observation. We behave as if we observed the channel at that moment // and report that the receive cannot proceed. return } // The channel is irreversibly closed. Re-check whether the channel has any pending data // to receive, which could have arrived between the empty and closed checks above. // Sequential consistency is also required here, when racing with such a send. if empty(c) { // The channel is irreversibly closed and empty. if raceenabled { raceacquire(c.raceaddr()) } if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } lock(\u0026amp;c.lock) if c.closed != 0 \u0026amp;\u0026amp; c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } // 如果 sendq 中有 goroutine，则取出一个，将数据发送给它 if sg := c.sendq.dequeue(); sg != nil { // Found a waiting sender. If buffer is size 0, receive value // directly from sender. Otherwise, receive from head of queue // and add sender's value to the tail of the queue (both map to // the same buffer slot because the queue is full). // // 如果是无缓冲 chan，则直接从 sg 中接收数据， // 如果是有缓冲 chan，则接收 buf 中第一个元素的数据，并将 sg 中的数据放到 buf 的末尾 recv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true, true } // 到这里说明 sendq 中没有等待的 goroutine // 如果缓冲区中有数据，那么就从 buf 中获取 if c.qcount \u0026gt; 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } // 从缓冲区复制数据到 ep if ep != nil { typedmemmove(c.elemtype, ep, qp) } // typedmemclr 的作用是将 ep 指向的类型为 elemtype 的内存块置为 0 值。 // 这里就是将 buf[recvx] 置为 0，因为这里的元素已经拷贝给了 ep typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026amp;c.lock) return true, true } // 非阻塞，直接返回 if !block { unlock(\u0026amp;c.lock) return false, false } // 到这里说明即没有等待的 groutine，环形队列中也没有数据，则阻塞该 groutine， // 并将 goroutine 打包为 sudog 加入到 recevq 等待队列中 // no sender available: block on this channel. gp := getg() // 创建 sudog mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(\u0026amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) // someone woke us up if mysg != gp.waiting { throw(\u0026quot;G waiting list is corrupted\u0026quot;) } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg) return true, success } recv // recv processes a receive operation on a full channel c. // There are 2 parts: // 1) The value sent by the sender sg is put into the channel // and the sender is woken up to go on its merry way. // 2) The value received by the receiver (the current G) is // written to ep. // For synchronous channels, both values are the same. // For asynchronous channels, the receiver gets its data from // the channel buffer and the sender's data is put in the // channel buffer. // Channel c must be full and locked. recv unlocks c with unlockf. // sg must already be dequeued from c. // A non-nil ep must point to the heap or the caller's stack. // // recv 被 chanrecv() 调用，调用条件是 sendq 中有等待的 g // c: chan // sg: sendq 中的等待 goroutine // ep: 用来接收数据 func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { // 如果是无缓冲区 chan if c.dataqsiz == 0 { if raceenabled { racesync(c, sg) } if ep != nil { // copy data from sender // 将 sg（sendq 中的对象）中的数据拷贝到 ep 中 recvDirect(c.elemtype, sg, ep) } // 有缓冲区chan } else { // Queue is full. Take the item at the // head of the queue. Make the sender enqueue // its item at the tail of the queue. Since the // queue is full, those are both the same slot. // // 由于有发送者在等待，所以缓冲区一定是满的，因为 recv 的第二个参数 sudog // 传入的是被阻塞的发送者，在 chanrecv 中的表现是： // if sg := c.sendq.dequeue(); sg != nil { // recv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) //\treturn true, true // } // qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) } // copy data from queue to receiver // 先将 buf 中的第一个数据（recvx）移动到 ep if ep != nil { typedmemmove(c.elemtype, ep, qp) } // copy data from sender to queue // 因为已经将 buf 中的一个数据拷贝给 ep 了，等于此时的 ep 已经空了一个 // 位置了，所以可以将 sudog 中的元素放到这个空位置里 // 下面的逻辑也没有对 c.qcount 进行更新，因为移除了一个后又新添加了一个, // 相当于长度没有发生改变 typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz } sg.elem = nil gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } ","date":"2022年06月26日","permalink":"/posts/go-chan-yuan-ma/","summary":"chan 原理（人话版） 说明：这里只是用大白话来阐述 chan 的大致原理，不保证严谨性，也不考虑一些繁枝细节（比如 sudog，gopark，goready），仅作为一个简单的参考理解，如果想深入理解 chan 的底层实现，请自行参阅其他博客","title":"go chan 原理（人话版）"},{"contents":"127.0.0.1 127.0.0.1 是 回环地址，当报文的目的地址是回环地址时，依然会走一遍网络协议栈，但是不会经过物理网卡，而是经过一个名为 lo 的虚拟网卡（mac 上交 lo0），具体的流程是：\n从应用层到传输层再到网络层，到了网络层，系统会根据目的IP，在路由表中获取对应的路由信息，而这其中就包含选择哪个网卡把消息发出。\n当发现 目标IP是外网IP 时，会从\u0026quot;真网卡\u0026quot;发出。\n当发现 目标IP是回环地址 时，就会选择 本地网卡。\n本地网卡，其实就是个**\u0026ldquo;假网卡\u0026rdquo;**，它不像\u0026quot;真网卡\u0026quot;那样有个ring buffer什么的，\u0026ldquo;假网卡\u0026quot;会把数据推到一个叫 input_pkt_queue 的 链表 中。这个链表，其实是所有网卡共享的，上面挂着发给本机的各种消息。消息被发送到这个链表后，会再触发一个 软中断。\n专门处理软中断的工具人**\u0026ldquo;ksoftirqd\u0026rdquo;** （这是个内核线程），它在收到软中断后就会立马去链表里把消息取出，然后顺着数据链路层、网络层等层层往上传递最后给到应用程序。\n**之所以127.0.0.1叫本地回环地址，可以理解为，消息发出到这个地址上的话，就不会出网络，在本机打个转就又回来了。**所以断网，依然能 ping 通 127.0.0.1。\n以上内容摘自：\n硬核图解！断网了，还能ping通 127.0.0.1 吗？为什么？https://www.eet-china.com/mp/a67490.html\n问题记录：\nping 127.0.0.1 走不走物理网卡？\n网上的博客有说走的（https://blog.csdn.net/bandaoyu/article/details/87259631， 127.0.0.1 回环地址，不经过[链路层，物理层](网络接口层），在IP层就回去，依赖网卡，并受到网络防火墙和网卡相关的限制），也有说不走的，这时只能自己动手测试了，刚好有台闲置的 windows 笔记本可以拿来测试，执行 netsh interface set interface wlan diable 来关闭网卡，此时执行 ping 127.0.0.1，发现是可以 ping 通的，说明 127.0.0.1 是不走物理网卡的，但是会走 lo 虚拟网卡\nhttps://www.zhihu.com/question/43590414\nlocalhost localhost是个 域名 ，而不是一个 ip 地址。之所以我们经常把 localhost 与 127.0.0.1 认为是同一个东西，是因为我们使用的大多数电脑上都将 localhost 指向了127.0.0.1这个地址，在 linux 中这个文件的位置是 /etc/hosts ：\n$ cat /etc/hosts # Your system has configured 'manage_etc_hosts' as True. # As a result, if you wish for changes to this file to persist # then you will need to either # a.) make changes to the master file in /etc/cloud/templates/hosts.debian.tmpl # b.) change or remove the value of 'manage_etc_hosts' in # /etc/cloud/cloud.cfg or cloud-config from user-data # 127.0.1.1 primary primary 127.0.0.1 localhost 0.0.0.0 0.0.0.0 可以作为监听地址使用，代表监听本机的所有 IP 地址（如果一台机器上有多个网卡，就会有多个 IP 地址），可以做一个实践测试一下：\n实践 创建一块虚拟网卡（在虚拟机上）：\n$ ifconfig enp0s1:0 192.168.64.8 up 语句格式是：ifconfig [现有网卡名称]:[任意值,也可以为空] [虚拟网卡的 IP] up\n按照上面的格式，继续创建多块虚拟网卡：\n$ ifconfig enp0s1: 192.168.64.9 up $ ifconfig enp0s1:__ 192.168.64.10 up 查看现在的网卡情况：\n$ ifconfig enp0s1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.64.7 netmask 255.255.255.0 broadcast 192.168.64.255 inet6 fe80::5054:ff:fe97:e2bd prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; inet6 fd02:feab:6557:b7c6:5054:ff:fe97:e2bd prefixlen 64 scopeid 0x0\u0026lt;global\u0026gt; ether 52:54:00:97:e2:bd txqueuelen 1000 (Ethernet) RX packets 45586 bytes 61327352 (61.3 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 27086 bytes 2915213 (2.9 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 enp0s1:0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.64.8 netmask 255.255.255.0 broadcast 192.168.64.255 ether 52:54:00:97:e2:bd txqueuelen 1000 (Ethernet) enp0s1:: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.64.9 netmask 255.255.255.0 broadcast 192.168.64.255 ether 52:54:00:97:e2:bd txqueuelen 1000 (Ethernet) enp0s1:__: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.64.10 netmask 255.255.255.0 broadcast 192.168.64.255 ether 52:54:00:97:e2:bd txqueuelen 1000 (Ethernet) 用 Go 写一个 TCP server 进行测试：\npackage main import ( \u0026quot;net\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;flag\u0026quot; ) var host = flag.String(\u0026quot;h\u0026quot;, \u0026quot;127.0.0.1\u0026quot;, \u0026quot;listen host\u0026quot;) var port = flag.String(\u0026quot;p\u0026quot;, \u0026quot;8080\u0026quot;, \u0026quot;listen port\u0026quot;) func main() { flag.Parse() addr := fmt.Sprintf(\u0026quot;%v:%v\u0026quot;, *host, *port) l, err := net.Listen(\u0026quot;tcp\u0026quot;, addr) if err != nil { panic(err) } fmt.Printf(\u0026quot;listen in %v\\n\u0026quot;, addr) for { conn, err := l.Accept() if err != nil { fmt.Println(\u0026quot;accept error: \u0026quot;, err) continue } if _, err := conn.Write([]byte(\u0026quot;Hello\\n\u0026quot;)); err != nil { fmt.Println(\u0026quot;write error: \u0026quot;, err) continue } conn.Close() } } 运行程序（虚拟机上）：\n$ go run hello_tcpserver.go -h 0.0.0.0 listen in 0.0.0.0:8080 在 宿主机 上测试一下效果：\n$ nc 192.168.64.7 8080 Hello $ nc 192.168.64.8 8080 Hello $ nc 192.168.64.9 8080 Hello $ nc 192.168.64.10 8080 Hello 发现使用任意一个 IP 地址都可以访问到服务\n此外，有的博客里说 0.0.0.0 是 ping 不通的，但是在我的虚拟机上却并非如此：\n$ uname -a Linux primary 5.4.0-120-generic #136-Ubuntu SMP Fri Jun 10 13:46:10 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux $ ping 0.0.0.0 PING 0.0.0.0 (127.0.0.1) 56(84) bytes of data. 64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.299 ms 64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.091 ms 64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.076 ms 64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.108 ms ^C --- 0.0.0.0 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3244ms rtt min/avg/max/mdev = 0.076/0.143/0.299/0.090 ms 不知道什么情况\n监听 127.0.0.1 和 监听 0.0.0.0 有什么区别？\n监听 127.0.0.1 的话就只能在本地访问了，外部是无法访问的，localhost 当然也是一样的\n测试：\n$ go run hello_tcpserver.go -h localhost listen in localhost:8080 在宿主机上：\n$ nc -v 192.168.64.9 8080 nc: connectx to 192.168.64.9 port 8080 (tcp) failed: Connection refused 说到这里又让我想起了一件事：之前我部署过一个 Go HTTP 小程序到腾讯云服务器上，当时 listen 的地址就是 localhost，但是从外部依然能够访问到该服务，貌似是因为当时是用 docker 部署的，做了端口映射，所以才可以访问到\n","date":"2022年06月24日","permalink":"/posts/127.0.0.1_localhost_0.0.0.0/","summary":"127.0.0.1 127.0.0.1 是 回环地址，当报文的目的地址是回环地址时，依然会走一遍网络协议栈，但是不会经过物理网卡，而是经过一个名为 lo 的虚拟网卡（mac 上交 lo0），具体的流程是：","title":"127.0.0.1，localhost，0.0.0.0 的区别（包含虚拟网卡相关操作）"},{"contents":" 该文章仅作为本人笔记，不具备太大的参考价值\n该文章着重记录源码，没有对熔断器这一概念做过多理念上的说明解释\n该文章排版、思路较为混乱，后续可能会进行修改\n示例 官方示例 官方示例有点太简单了，完全无法体会到 熔断 这一概念\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;github.com/sony/gobreaker\u0026quot; ) var cb *gobreaker.CircuitBreaker func init() { var st gobreaker.Settings st.Name = \u0026quot;HTTP GET\u0026quot; st.ReadyToTrip = func(counts gobreaker.Counts) bool { failureRatio := float64(counts.TotalFailures) / float64(counts.Requests) return counts.Requests \u0026gt;= 3 \u0026amp;\u0026amp; failureRatio \u0026gt;= 0.6 } cb = gobreaker.NewCircuitBreaker(st) } // Get wraps http.Get in CircuitBreaker. func Get(url string) ([]byte, error) { body, err := cb.Execute(func() (interface{}, error) { resp, err := http.Get(url) if err != nil { return nil, err } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil { return nil, err } return body, nil }) if err != nil { return nil, err } return body.([]byte), nil } func main() { body, err := Get(\u0026quot;http://www.google.com/robots.txt\u0026quot;) if err != nil { log.Fatal(err) } fmt.Println(string(body)) } 我的示例 先写一个 http server，他会随机为请求返回 200 或者 500\npackage main import ( \u0026quot;math/rand\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;time\u0026quot; ) var canVisit bool func main() { rand.Seed(time.Now().Unix()) go func() { for { t := rand.Int63n(10) select { case \u0026lt;-time.After(time.Duration(t) * time.Second): canVisit = !canVisit } } }() http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { if canVisit { w.WriteHeader(http.StatusInternalServerError) w.Write([]byte(\u0026quot;server error\u0026quot;)) } else { w.WriteHeader(http.StatusOK) w.Write([]byte(\u0026quot;success\u0026quot;)) } }) if err := http.ListenAndServe(\u0026quot;:9000\u0026quot;, nil); err != nil { panic(err) } } 熔断器程序，会一直访问上面的 http server，返回 200 算请求成功，500 算失败，来观察熔断器的状态和效果：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/sony/gobreaker\u0026quot; ) var cb *gobreaker.CircuitBreaker func init() { var st gobreaker.Settings st.Name = \u0026quot;HTTP GET\u0026quot; st.ReadyToTrip = func(counts gobreaker.Counts) bool { failureRatio := float64(counts.TotalFailures) / float64(counts.Requests) return counts.Requests \u0026gt;= 3 \u0026amp;\u0026amp; failureRatio \u0026gt;= 0.5 } st.Timeout = time.Second * 10\t// 从开启切换到半开的时间 st.OnStateChange = func(name string, from, to gobreaker.State) { log.Printf(\u0026quot;state change: [%v] -\u0026gt; [%v]\\n\u0026quot;, from, to) } cb = gobreaker.NewCircuitBreaker(st) } func Get(url string) ([]byte, error) { body, err := cb.Execute(func() (interface{}, error) { resp, err := http.Get(url) if err != nil { return nil, err } defer resp.Body.Close() if resp.StatusCode \u0026gt;= 400 { return nil, fmt.Errorf(\u0026quot;[%v]%v\u0026quot;, resp.StatusCode, http.StatusText(resp.StatusCode)) } body, err := ioutil.ReadAll(resp.Body) if err != nil { return nil, err } return body, nil }) if err != nil { return nil, err } return body.([]byte), nil } func main() { for { body, err := Get(\u0026quot;http://localhost:9000\u0026quot;) if err != nil { log.Println(err) } log.Printf(\u0026quot;%v, %+v\\n\u0026quot;, cb.State(), cb.Counts()) fmt.Println(string(body)) time.Sleep(time.Millisecond * 500) } } 输出日志：\n2022/06/10 22:02:36 closed, {Requests:1 TotalSuccesses:1 TotalFailures:0 ConsecutiveSuccesses:1 ConsecutiveFailures:0} .... 2022/06/10 22:03:26 [500]Internal Server Error 2022/06/10 22:03:26 closed, {Requests:101 TotalSuccesses:51 TotalFailures:50 ConsecutiveSuccesses:0 ConsecutiveFailures:8} 2022/06/10 22:03:26 state change: [closed] -\u0026gt; [open] 2022/06/10 22:03:26 [500]Internal Server Error 2022/06/10 22:03:26 open, {Requests:0 TotalSuccesses:0 TotalFailures:0 ConsecutiveSuccesses:0 ConsecutiveFailures:0} 第二段日志记录了错误，此时的总请求次数是 101，总错误是 50，还没达到 ReadyToTrip 的条件（错误率 \u0026gt;= 50%），然后第三段日志，发现此时请求依然失败，此时总请求 102，总错误 51，达到了 50%，所以发生了状态转换，熔断器进行开启模式。\n后续应该会一直保持开启状态，直到 10s 后切换为半开状态，这里日志就没记录了。\n熔断器的 3 种状态 // State is a type that represents a state of CircuitBreaker. type State int // These constants are states of CircuitBreaker. // 熔断器的状态 const ( // StateClosed 表示关闭状态，此时所有请求都会通过 StateClosed State = iota // StateHalfOpen 半开状态，会根据情况变更为开启或者关闭状态 StateHalfOpen // StateOpen 开启状态，此时会拒绝所有请求 StateOpen ) 熔断器有三种状态：\n开启：所有请求都会拒绝 关闭：所有请求都会通过 半开启：开启状态自然不能一直维持，需要某个条件将其切换到一种中间状态：也就是这里的半开启状态。当开启状态持续了一段时间后，就会变更为半开启状态，表示可以做试探了，如果此时请求依然失败，则变更为开启状态；如果成功次数达到了我们的需求，则变更为关闭状态 计数器 计数器保存了熔断器的所有信息，具体属性如下：\n// Counts 保存请求的数量及其成功失败的次数。 // CircuitBreaker 在状态更改或关闭状态间隔时清除内部计数。 // Counts 会忽略在清除之前发送的请求的结果。 type Counts struct { Requests uint32 // 总请求次数 TotalSuccesses uint32 // 总成功次数 TotalFailures uint32 // 总失败次数 ConsecutiveSuccesses uint32 // 连续成功次数 ConsecutiveFailures uint32 // 连续失败次数 } 此外还提供了一些列更新计数的方法：\nfunc (c *Counts) onRequest() { c.Requests++ } func (c *Counts) onSuccess() { c.TotalSuccesses++ c.ConsecutiveSuccesses++ c.ConsecutiveFailures = 0 } func (c *Counts) onFailure() { c.TotalFailures++ c.ConsecutiveFailures++ c.ConsecutiveSuccesses = 0 } func (c *Counts) clear() { c.Requests = 0 c.TotalSuccesses = 0 c.TotalFailures = 0 c.ConsecutiveSuccesses = 0 c.ConsecutiveFailures = 0 } 熔断器 CircuitBreaker 定义了熔断器结构体，这也是最核心的结构体\n// CircuitBreaker is a state machine to prevent sending requests that are likely to fail. type CircuitBreaker struct { // 虚线内的属性和 Settings 中的相同，如果 Settings 中没有设置，则使用默认值来填充 // ================== name string // 比较迷的一个变量，源码里有两种情况： // 1. 请求总数（Requests） \u0026gt;= MaxRequests，那么会返回请求过多的错误 // 2. 连续成功次数（ConsecutiveSuccesses） \u0026gt;= MaxRequests，那么变更为关闭状态 // 1 在 beforeRequest() 函数中，2 在 afterRequest() -\u0026gt; onSuccess() 的 HalfOpen 分支中 // 也就是在请求前会确保请求总数不超过 MaxRequest，请求后如果处于半开状态且连续成功数 \u0026gt;= MaxRequest // 那么会变更为关闭状态 maxRequests uint32 // 关闭状态下定期清空计数的时间，如果为 0，则不清空 // 这里我不太明白清空计数的原因，在网上找了一个分析，意思是如果一直处于成功状态， // 那么计数的意义就不是很大，此外如果请求量过大可能会导致溢出，所以需要定期清空 interval time.Duration // 打开状态的持续时间，到时后会变更为半打开状态。 timeout time.Duration // 关闭状态下会调用该回调函数，如果返回 true，则进入打开状态 readyToTrip func(counts Counts) bool // 用来判断请求是否成功的回调函数 isSuccessful func(err error) bool // 发生状态变更时的回调函数 onStateChange func(name string, from State, to State) // ==================== mutex sync.Mutex state State generation uint64 counts Counts // 这个变量貌似有两种情况： // 1. 开启状态下，代表切换到半开启的绝对时间（time.Time 代表一个绝对时间） // 具体值是 time.Now + timeout // 2. 关闭状态下，代表进入下一个周期的绝对时间（进入下一个周期会清空计数） // 具体值是 time.Now + interval expiry time.Time } 其中的一部分值是通过配置类来指定的，如果不指定则会设置为默认配置：\nfunc NewCircuitBreaker(st Settings) *CircuitBreaker { cb := new(CircuitBreaker) cb.name = st.Name cb.onStateChange = st.OnStateChange if st.MaxRequests == 0 { cb.maxRequests = 1 } else { cb.maxRequests = st.MaxRequests } if st.Interval \u0026lt;= 0 { cb.interval = defaultInterval } else { cb.interval = st.Interval } if st.Timeout \u0026lt;= 0 { cb.timeout = defaultTimeout } else { cb.timeout = st.Timeout } if st.ReadyToTrip == nil { cb.readyToTrip = defaultReadyToTrip } else { cb.readyToTrip = st.ReadyToTrip } if st.IsSuccessful == nil { cb.isSuccessful = defaultIsSuccessful } else { cb.isSuccessful = st.IsSuccessful } cb.toNewGeneration(time.Now()) return cb } 配置类 type Settings struct { // 熔断器的名称 Name string // MaxRequests 是 CircuitBreaker 半开时允许通过的最大请求数。 // 如果 MaxRequests 为 0，则 CircuitBreaker 只允许 1 个请求。 // FIXME 比较迷的一个变量，源码里有两种情况： // 1. 请求总数（Requests） \u0026gt;= MaxRequests，那么会返回请求过多的错误 // 2. 连续成功次数（ConsecutiveSuccesses） \u0026gt;= MaxRequests，那么变更为关闭状态 // 1 在 beforeRequest() 函数中，2 在 afterRequest() -\u0026gt; onSuccess() 的 HalfOpen 分支中 // 也就是在请求前会确保请求总数不超过 MaxRequest，请求后如果处于半开状态且连续成功数 \u0026gt;= MaxRequest // 那么会变更为关闭状态 MaxRequests uint32 // Interval 是熔断器处于关闭状态时，定期清除内部 Counts 的时间。 // 如果 Interval 小于或等于 0，CircuitBreaker 在关闭状态期间不会清除内部计数。 // FIXME 这个东西暂时没发现用处何在 // 在网上找了一个分析，意思是如果一直处于成功状态，那么计数的意义就不是很大， // 需要定期清空，不然可能会溢出 Interval time.Duration // Timeout 是打开状态的持续时间，到时后会变更为半打开状态。 // 如果 Timeout 小于或等于 0，则将 CircuitBreaker 的超时值设置为 60 秒。 Timeout time.Duration // 每当请求在关闭状态下失败时，就会调用 ReadyToTrip，参数传递的是 Counts 的副本。 // 如果 ReadyToTrip 返回 true，CircuitBreaker 将进入打开状态。 // 如果 ReadyToTrip 为 nil，则使用默认 ReadyToTrip。 // 当连续失败次数超过 5 次时，默认 ReadyToTrip 返回 true。 ReadyToTrip func(counts Counts) bool // OnStateChange 是熔断器状态变更时的回调函数 OnStateChange func(name string, from State, to State) // IsSuccessful 判断请求是否成功，传入的 err 是执行用户请求函数后返回的。 // （也就是 CircuitBreaker.Execute 的参数 req） // 如果 IsSuccessful 返回 true， 则说明请求发生了错误，否则说明没有错误。 // 如果 IsSuccessful 为 nil， 则使用默认 IsSuccessful，该默认函数的逻辑是： // if err == nil { return true } IsSuccessful func(err error) bool } 默认配置：\nconst defaultInterval = time.Duration(0) * time.Second const defaultTimeout = time.Duration(60) * time.Second func defaultReadyToTrip(counts Counts) bool { return counts.ConsecutiveFailures \u0026gt; 5 } func defaultIsSuccessful(err error) bool { return err == nil } 状态转换 一共有一下几种状态转换：\n关闭 -\u0026gt; 开启 开启 -\u0026gt; 半开 半开 -\u0026gt; 开启 半开 -\u0026gt; 关闭 gobreaker 定义了一些方法来完成上面的状态转换，顾名思义，如下所示：\nCircuitBreaker.onSuccess 这个函数定义了 半开 -\u0026gt; 关闭 的状态转换 ，是通过 cb.setState 来完成的\n// 熔断器请求成功时调用该函数 func (cb *CircuitBreaker) onSuccess(state State, now time.Time) { switch state { case StateClosed: // 如果此时是关闭状态，则更新计数 cb.counts.onSuccess() case StateHalfOpen: // 半开状态 cb.counts.onSuccess() // 更新计数 // 连续成功总数超过了设置的 maxRequests，变更为关闭状态 if cb.counts.ConsecutiveSuccesses \u0026gt;= cb.maxRequests { cb.setState(StateClosed, now) } } } CircuitBreaker.onFailure 这个函数定义了 **半开 -\u0026gt; 开启 ** 和 关闭 -\u0026gt; 开启 的状态转换\n// 熔断器请求失败时调用该函数 func (cb *CircuitBreaker) onFailure(state State, now time.Time) { switch state { // 关闭状态下请求失败了 case StateClosed: cb.counts.onFailure() // 更新计数 // 如果回调函数 readyToTrip 返回 true // 因为一次失败可能不足以直接判定为需要熔断，所以可能失败多次后才会返回 true // 比如官方示例中设置的回调函数是： // st.ReadyToTrip = func(counts gobreaker.Counts) bool { //\tfailureRatio := float64(counts.TotalFailures) / float64(counts.Requests) //\treturn counts.Requests \u0026gt;= 3 \u0026amp;\u0026amp; failureRatio \u0026gt;= 0.6 //\t} // 可以看到这里需要请求次数大于3，且总失败率大于等于 60% 才会返回 true if cb.readyToTrip(cb.counts) { cb.setState(StateOpen, now) // 变更熔断器为开启状态 } case StateHalfOpen: // 半开状态下失败了，变更为开启状态 cb.setState(StateOpen, now) } } CircuitBreaker.currentState 这个函数定义了 开启 -\u0026gt; 半开 的状态转换：\n// currentState 返回熔断器当前的状态，now 用来判断是否发生了状态变更 func (cb *CircuitBreaker) currentState(now time.Time) (State, uint64) { // func toNewGeneration // case StateClosed: // if cb.interval == 0 { // cb.expiry = zero // } else { // cb.expiry = now.Add(cb.interval) // } switch cb.state { // 如上面的注释代码所示，如果 cb.interval 为 0，那么 cb.expiry 会设置为 zero， // 此时下面的 if 条件就不满足了，关闭状态下也不会调用 cb.toNewGeneration 来清空计数 // 如果设置了 cb.interval，那么会设置 cb.expiry 的时间，如果处于关闭状态且达到了 // expiry 的时间，就会调用 cb.toNewGeneration 来清空计数 case StateClosed: if !cb.expiry.IsZero() \u0026amp;\u0026amp; cb.expiry.Before(now) { cb.toNewGeneration(now) } case StateOpen: // 超过了 expiry 的时间，可以切换到半开状态了 if cb.expiry.Before(now) { cb.setState(StateHalfOpen, now) } } return cb.state, cb.generation } 核心方法 Execute 这个方法就是熔断器的执行方法，用户传入一个函数，熔断器会根据当前状态进行相应的操作\nfunc (cb *CircuitBreaker) Execute(req func() (interface{}, error)) (interface{}, error) { generation, err := cb.beforeRequest() if err != nil { return nil, err } defer func() { e := recover() if e != nil { cb.afterRequest(generation, false) panic(e) } }() result, err := req() cb.afterRequest(generation, cb.isSuccessful(err)) return result, err } 可以看到其中调用了用户传入的函数，此外还有两个函数：beforeRequest() 和 afterRequest() 代表执行请求前和执行请求后，这两个函数内部会更新熔断器的计数以及状态\nbeforeRequest func (cb *CircuitBreaker) beforeRequest() (uint64, error) { cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() state, generation := cb.currentState(now) // 如果熔断器处于开启状态，直接返回错误，因为该方法在 Execute 中先于用户请求执行， // 且逻辑是有 err 直接 return，所以不会执行之后的代码，具体查看 Execute，下面是截取的部分： // generation, err := cb.beforeRequest() //\tif err != nil { //\treturn nil, err //\t} if state == StateOpen { return generation, ErrOpenState // 请求前如果处于半开状态，会进行限流操作 } else if state == StateHalfOpen \u0026amp;\u0026amp; cb.counts.Requests \u0026gt;= cb.maxRequests { return generation, ErrTooManyRequests } cb.counts.onRequest() // 更新计数 return generation, nil } afterRequest func (cb *CircuitBreaker) afterRequest(before uint64, success bool) { cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() state, generation := cb.currentState(now) if generation != before { return } // 更新状态和计数 if success { cb.onSuccess(state, now) } else { cb.onFailure(state, now) } } 这两个函数又调用了同一个函数 cb.currentState\ncurrentState // currentState 返回熔断器当前的状态，now 用来判断是否需要执行某些操作，这些操作包括： // 1. 关闭状态下清空计数（如果设置了 interval 且达到了清空时间） // 2. 开启状态转换为半开启状态（到达了转换时间） func (cb *CircuitBreaker) currentState(now time.Time) (State, uint64) { // func toNewGeneration // case StateClosed: //\tif cb.interval == 0 { //\tcb.expiry = zero //\t} else { //\tcb.expiry = now.Add(cb.interval) //\t} switch cb.state { // 如上面的注释代码所示，如果 cb.interval 为 0，那么 cb.expiry 会设置为 zero， // 此时下面的 if 条件就不满足了，关闭状态下也不会调用 cb.toNewGeneration 来清空计数 // 如果设置了 cb.interval，那么会设置 cb.expiry 的时间，如果处于关闭状态且达到了 // expiry 的时间，就会调用 cb.toNewGeneration 来清空计数 case StateClosed: if !cb.expiry.IsZero() \u0026amp;\u0026amp; cb.expiry.Before(now) { cb.toNewGeneration(now) } case StateOpen: // 超过了 expiry 的时间，可以切换到半开状态了 if cb.expiry.Before(now) { cb.setState(StateHalfOpen, now) } } return cb.state, cb.generation } 这个函数又调用了另一个函数 toNewGeneration\ntoNewGeneration // 进入一个新周期，会清空计数，并对 cb.expiry 进行更新 // 该函数会在 setState、currentState、NewCircuitBreaker 调用 func (cb *CircuitBreaker) toNewGeneration(now time.Time) { cb.generation++ cb.counts.clear() var zero time.Time switch cb.state { case StateClosed: if cb.interval == 0 { cb.expiry = zero } else { cb.expiry = now.Add(cb.interval) } case StateOpen: cb.expiry = now.Add(cb.timeout) // 设置 open -\u0026gt; halfOpen 的绝对时间 default: // StateHalfOpen cb.expiry = zero } } 总结 光看文档感觉熔断器并不是一个多么复杂的东西，但是等实际阅读代码才发现还是有点小复杂的，何况这个开源库还只是对熔断器的一个最精简实现，还有其他的更复杂的实现库，阅读这个源码主要有一些不清楚的点在于，里面的属性 CircuitBreaker.expiry 和 CircuitBreaker.maxRequests 在不同情况下有不同的意义，导致看的有点头疼，状态的转换看着也有点绕，还有一开始对 CircuitBreaker.interval 这个属性的意义也不太理解\n完整源码注释 https://github.com/youseebiggirl/gobreaker_annotation\n// Package gobreaker implements the Circuit Breaker pattern. // See https://msdn.microsoft.com/en-us/library/dn589784.aspx. package gobreaker import ( \u0026quot;errors\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) // State is a type that represents a state of CircuitBreaker. type State int // These constants are states of CircuitBreaker. // 熔断器的状态 const ( // StateClosed 表示关闭状态，此时所有请求都会通过 StateClosed State = iota // StateHalfOpen 半开状态，会根据情况变更为开启或者关闭状态 StateHalfOpen // StateOpen 开启状态，此时会拒绝所有请求 StateOpen ) var ( // ErrTooManyRequests is returned when the CB state is half open and the requests count is over the cb maxRequests // 该错误在状态为半开且请求数超过 maxRequests 时返回 ErrTooManyRequests = errors.New(\u0026quot;too many requests\u0026quot;) // ErrOpenState is returned when the CB state is open // 该错误在状态为开启时返回 ErrOpenState = errors.New(\u0026quot;circuit breaker is open\u0026quot;) ) // String implements stringer interface. // String 继承了 stringer 接口，相当于自定义了 fmt.Println(State) 的输出 func (s State) String() string { switch s { case StateClosed: return \u0026quot;closed\u0026quot; case StateHalfOpen: return \u0026quot;half-open\u0026quot; case StateOpen: return \u0026quot;open\u0026quot; default: return fmt.Sprintf(\u0026quot;unknown state: %d\u0026quot;, s) } } // Counts holds the numbers of requests and their successes/failures. // CircuitBreaker clears the internal Counts either // on the change of the state or at the closed-state intervals. // Counts ignores the results of the requests sent before clearing. // Counts 保存请求的数量及其成功失败的次数。 // CircuitBreaker 在状态更改或关闭状态间隔时清除内部计数。 // Counts 会忽略在清除之前发送的请求的结果。 type Counts struct { Requests uint32 // 总请求次数 TotalSuccesses uint32 // 总成功次数 TotalFailures uint32 // 总失败次数 ConsecutiveSuccesses uint32 // 连续成功次数 ConsecutiveFailures uint32 // 连续失败次数 } func (c *Counts) onRequest() { c.Requests++ } func (c *Counts) onSuccess() { c.TotalSuccesses++ c.ConsecutiveSuccesses++ c.ConsecutiveFailures = 0 } func (c *Counts) onFailure() { c.TotalFailures++ c.ConsecutiveFailures++ c.ConsecutiveSuccesses = 0 } func (c *Counts) clear() { c.Requests = 0 c.TotalSuccesses = 0 c.TotalFailures = 0 c.ConsecutiveSuccesses = 0 c.ConsecutiveFailures = 0 } // Settings configures CircuitBreaker: // // Name is the name of the CircuitBreaker. // // MaxRequests is the maximum number of requests allowed to pass through // when the CircuitBreaker is half-open. // If MaxRequests is 0, the CircuitBreaker allows only 1 request. // // Interval is the cyclic period of the closed state // for the CircuitBreaker to clear the internal Counts. // If Interval is less than or equal to 0, the CircuitBreaker doesn't clear internal Counts during the closed state. // // Timeout is the period of the open state, // after which the state of the CircuitBreaker becomes half-open. // If Timeout is less than or equal to 0, the timeout value of the CircuitBreaker is set to 60 seconds. // // ReadyToTrip is called with a copy of Counts whenever a request fails in the closed state. // If ReadyToTrip returns true, the CircuitBreaker will be placed into the open state. // If ReadyToTrip is nil, default ReadyToTrip is used. // Default ReadyToTrip returns true when the number of consecutive failures is more than 5. // // OnStateChange is called whenever the state of the CircuitBreaker changes. // // IsSuccessful is called with the error returned from a request. // If IsSuccessful returns true, the error is counted as a success. // Otherwise the error is counted as a failure. // If IsSuccessful is nil, default IsSuccessful is used, which returns false for all non-nil errors. type Settings struct { // 熔断器的名称 Name string // MaxRequests 是 CircuitBreaker 半开时允许通过的最大请求数。 // 如果 MaxRequests 为 0，则 CircuitBreaker 只允许 1 个请求。 // FIXME 比较迷的一个变量，源码里有两种情况： // 1. 请求总数（Requests） \u0026gt;= MaxRequests，那么会返回请求过多的错误 // 2. 连续成功次数（ConsecutiveSuccesses） \u0026gt;= MaxRequests，那么变更为关闭状态 // 1 在 beforeRequest() 函数中，2 在 afterRequest() -\u0026gt; onSuccess() 的 HalfOpen 分支中 // 也就是在请求前会确保请求总数不超过 MaxRequest，请求后如果处于半开状态且连续成功数 \u0026gt;= MaxRequest // 那么会变更为关闭状态 MaxRequests uint32 // Interval 是熔断器处于关闭状态时，定期清除内部 Counts 的时间。 // 如果 Interval 小于或等于 0，CircuitBreaker 在关闭状态期间不会清除内部计数。 // FIXME 这个东西暂时没发现用处何在 // 在网上找了一个分析，意思是如果一直处于成功状态，那么计数的意义就不是很大， // 需要定期清空，不然可能会溢出 Interval time.Duration // Timeout 是打开状态的持续时间，到时后会变更为半打开状态。 // 如果 Timeout 小于或等于 0，则将 CircuitBreaker 的超时值设置为 60 秒。 Timeout time.Duration // 每当请求在关闭状态下失败时，就会调用 ReadyToTrip，参数传递的是 Counts 的副本。 // 如果 ReadyToTrip 返回 true，CircuitBreaker 将进入打开状态。 // 如果 ReadyToTrip 为 nil，则使用默认 ReadyToTrip。 // 当连续失败次数超过 5 次时，默认 ReadyToTrip 返回 true。 ReadyToTrip func(counts Counts) bool // OnStateChange 是熔断器状态变更时的回调函数 OnStateChange func(name string, from State, to State) // IsSuccessful 判断请求是否成功，传入的 err 是执行用户请求函数后返回的。 // （也就是 CircuitBreaker.Execute 的参数 req） // 如果 IsSuccessful 返回 true， 则说明请求发生了错误，否则说明没有错误。 // 如果 IsSuccessful 为 nil， 则使用默认 IsSuccessful，该默认函数的逻辑是： // if err == nil { return true } IsSuccessful func(err error) bool } // CircuitBreaker is a state machine to prevent sending requests that are likely to fail. type CircuitBreaker struct { // 虚线内的属性和 Settings 中的相同，如果 Settings 中没有设置，则使用默认值来填充 // ================== name string // 比较迷的一个变量，源码里有两种情况： // 1. 请求总数（Requests） \u0026gt;= MaxRequests，那么会返回请求过多的错误 // 2. 连续成功次数（ConsecutiveSuccesses） \u0026gt;= MaxRequests，那么变更为关闭状态 // 1 在 beforeRequest() 函数中，2 在 afterRequest() -\u0026gt; onSuccess() 的 HalfOpen 分支中 // 也就是在请求前会确保请求总数不超过 MaxRequest，请求后如果处于半开状态且连续成功数 \u0026gt;= MaxRequest // 那么会变更为关闭状态 maxRequests uint32 // 关闭状态下定期清空计数的时间，如果为 0，则不清空 // 这里我不太明白清空计数的原因，在网上找了一个分析，意思是如果一直处于成功状态， // 那么计数的意义就不是很大，此外如果请求量过大可能会导致溢出，所以需要定期清空 interval time.Duration // 打开状态的持续时间，到时后会变更为半打开状态。 timeout time.Duration // 关闭状态下会调用该回调函数，如果返回 true，则进入打开状态 readyToTrip func(counts Counts) bool // 用来判断请求是否成功的回调函数 isSuccessful func(err error) bool // 发生状态变更时的回调函数 onStateChange func(name string, from State, to State) // ==================== mutex sync.Mutex state State generation uint64 counts Counts // 这个变量貌似有两种情况： // 1. 开启状态下，代表切换到半开启的绝对时间（time.Time 代表一个绝对时间） // 具体值是 time.Now + timeout // 2. 关闭状态下，代表进入下一个周期的绝对时间（进入下一个周期会清空计数） // 具体值是 time.Now + interval expiry time.Time } // TwoStepCircuitBreaker is like CircuitBreaker but instead of surrounding a function // with the breaker functionality, it only checks whether a request can proceed and // expects the caller to report the outcome in a separate step using a callback. type TwoStepCircuitBreaker struct { cb *CircuitBreaker } // NewCircuitBreaker returns a new CircuitBreaker configured with the given Settings. func NewCircuitBreaker(st Settings) *CircuitBreaker { cb := new(CircuitBreaker) cb.name = st.Name cb.onStateChange = st.OnStateChange if st.MaxRequests == 0 { cb.maxRequests = 1 } else { cb.maxRequests = st.MaxRequests } if st.Interval \u0026lt;= 0 { cb.interval = defaultInterval } else { cb.interval = st.Interval } if st.Timeout \u0026lt;= 0 { cb.timeout = defaultTimeout } else { cb.timeout = st.Timeout } if st.ReadyToTrip == nil { cb.readyToTrip = defaultReadyToTrip } else { cb.readyToTrip = st.ReadyToTrip } if st.IsSuccessful == nil { cb.isSuccessful = defaultIsSuccessful } else { cb.isSuccessful = st.IsSuccessful } cb.toNewGeneration(time.Now()) return cb } // NewTwoStepCircuitBreaker returns a new TwoStepCircuitBreaker configured with the given Settings. func NewTwoStepCircuitBreaker(st Settings) *TwoStepCircuitBreaker { return \u0026amp;TwoStepCircuitBreaker{ cb: NewCircuitBreaker(st), } } const defaultInterval = time.Duration(0) * time.Second const defaultTimeout = time.Duration(60) * time.Second func defaultReadyToTrip(counts Counts) bool { return counts.ConsecutiveFailures \u0026gt; 5 } func defaultIsSuccessful(err error) bool { return err == nil } // Name returns the name of the CircuitBreaker. func (cb *CircuitBreaker) Name() string { return cb.name } // State returns the current state of the CircuitBreaker. func (cb *CircuitBreaker) State() State { cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() state, _ := cb.currentState(now) return state } // Counts returns internal counters func (cb *CircuitBreaker) Counts() Counts { cb.mutex.Lock() defer cb.mutex.Unlock() return cb.counts } // Execute runs the given request if the CircuitBreaker accepts it. // Execute returns an error instantly if the CircuitBreaker rejects the request. // Otherwise, Execute returns the result of the request. // If a panic occurs in the request, the CircuitBreaker handles it as an error // and causes the same panic again. func (cb *CircuitBreaker) Execute(req func() (interface{}, error)) (interface{}, error) { // 执行请求前 generation, err := cb.beforeRequest() if err != nil { return nil, err } defer func() { e := recover() if e != nil { cb.afterRequest(generation, false) panic(e) } }() result, err := req() // 执行请求后 cb.afterRequest(generation, cb.isSuccessful(err)) return result, err } // Name returns the name of the TwoStepCircuitBreaker. func (tscb *TwoStepCircuitBreaker) Name() string { return tscb.cb.Name() } // State returns the current state of the TwoStepCircuitBreaker. func (tscb *TwoStepCircuitBreaker) State() State { return tscb.cb.State() } // Counts returns internal counters func (tscb *TwoStepCircuitBreaker) Counts() Counts { return tscb.cb.Counts() } // Allow checks if a new request can proceed. It returns a callback that should be used to // register the success or failure in a separate step. If the circuit breaker doesn't allow // requests, it returns an error. func (tscb *TwoStepCircuitBreaker) Allow() (done func(success bool), err error) { generation, err := tscb.cb.beforeRequest() if err != nil { return nil, err } return func(success bool) { tscb.cb.afterRequest(generation, success) }, nil } func (cb *CircuitBreaker) beforeRequest() (uint64, error) { cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() state, generation := cb.currentState(now) // 如果熔断器处于开启状态，直接返回错误，因为该方法在 Execute 中先于用户请求执行， // 且逻辑是有 err 直接 return，所以不会执行之后的代码，具体查看 Execute，下面是截取的部分： // generation, err := cb.beforeRequest() //\tif err != nil { //\treturn nil, err //\t} if state == StateOpen { return generation, ErrOpenState // 请求前如果处于半开状态，会进行限流操作 } else if state == StateHalfOpen \u0026amp;\u0026amp; cb.counts.Requests \u0026gt;= cb.maxRequests { return generation, ErrTooManyRequests } cb.counts.onRequest() // 更新计数 return generation, nil } func (cb *CircuitBreaker) afterRequest(before uint64, success bool) { cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() state, generation := cb.currentState(now) if generation != before { return } // 更新状态和计数 if success { cb.onSuccess(state, now) } else { cb.onFailure(state, now) } } // 熔断器请求成功时调用该函数 func (cb *CircuitBreaker) onSuccess(state State, now time.Time) { switch state { case StateClosed: // 如果此时是关闭状态，则更新计数 cb.counts.onSuccess() case StateHalfOpen: // 半开状态 cb.counts.onSuccess() // 更新计数 // 连续成功总数超过了设置的 maxRequests，变更为关闭状态 if cb.counts.ConsecutiveSuccesses \u0026gt;= cb.maxRequests { cb.setState(StateClosed, now) } } } // 熔断器请求失败时调用该函数 func (cb *CircuitBreaker) onFailure(state State, now time.Time) { switch state { // 关闭状态下请求失败了 case StateClosed: cb.counts.onFailure() // 更新计数 // 如果回调函数 readyToTrip 返回 true // 因为一次失败可能不足以直接判定为需要熔断，所以可能失败多次后才会返回 true // 比如官方示例中设置的回调函数是： // st.ReadyToTrip = func(counts gobreaker.Counts) bool { //\tfailureRatio := float64(counts.TotalFailures) / float64(counts.Requests) //\treturn counts.Requests \u0026gt;= 3 \u0026amp;\u0026amp; failureRatio \u0026gt;= 0.6 //\t} // 可以看到这里需要请求次数大于3，且总失败率大于等于 60% 才会返回 true if cb.readyToTrip(cb.counts) { cb.setState(StateOpen, now) // 变更熔断器为开启状态 } case StateHalfOpen: // 半开状态下失败了，变更为开启状态 cb.setState(StateOpen, now) } } // currentState 返回熔断器当前的状态，now 用来判断是否需要执行某些操作，这些操作包括： // 1. 关闭状态下清空计数（如果设置了 interval 且达到了清空时间） // 2. 开启状态转换为半开启状态（到达了转换时间） func (cb *CircuitBreaker) currentState(now time.Time) (State, uint64) { // func toNewGeneration // case StateClosed: //\tif cb.interval == 0 { //\tcb.expiry = zero //\t} else { //\tcb.expiry = now.Add(cb.interval) //\t} switch cb.state { // 如上面的注释代码所示，如果 cb.interval 为 0，那么 cb.expiry 会设置为 zero， // 此时下面的 if 条件就不满足了，关闭状态下也不会调用 cb.toNewGeneration 来清空计数 // 如果设置了 cb.interval，那么会设置 cb.expiry 的时间，如果处于关闭状态且达到了 // expiry 的时间，就会调用 cb.toNewGeneration 来清空计数 case StateClosed: if !cb.expiry.IsZero() \u0026amp;\u0026amp; cb.expiry.Before(now) { cb.toNewGeneration(now) } case StateOpen: // 超过了 expiry 的时间，可以切换到半开状态了 if cb.expiry.Before(now) { cb.setState(StateHalfOpen, now) } } return cb.state, cb.generation } func (cb *CircuitBreaker) setState(state State, now time.Time) { if cb.state == state { return } prev := cb.state cb.state = state cb.toNewGeneration(now) // 设置新状态后更新计数 if cb.onStateChange != nil { cb.onStateChange(cb.name, prev, state) } } // 进入一个新周期，会清空计数，并对 cb.expiry 进行更新 // 该函数会在 setState、currentState、NewCircuitBreaker 调用 func (cb *CircuitBreaker) toNewGeneration(now time.Time) { cb.generation++ cb.counts.clear() var zero time.Time switch cb.state { case StateClosed: if cb.interval == 0 { cb.expiry = zero } else { cb.expiry = now.Add(cb.interval) } case StateOpen: cb.expiry = now.Add(cb.timeout) // 设置 open -\u0026gt; halfOpen 的绝对时间 default: // StateHalfOpen cb.expiry = zero } } ","date":"2022年06月11日","permalink":"/posts/gobreaker/","summary":"该文章仅作为本人笔记，不具备太大的参考价值\n该文章着重记录源码，没有对熔断器这一概念做过多理念上的说明解释\n该文章排版、思路较为混乱，后续可能会进行修改\n示例 官方示例 官方示例有点太简单了，完全无法体会到 熔断 这一概念","title":"go 熔断器 gobreaker 源码阅读"},{"contents":"引子 最近买了块机械硬盘作为仓库盘，用硬盘盒连接到 mac，但是这块硬盘貌似有休眠机制（不确定），如果一段时间没有读写操作就会停转，等到需要操作时才会唤醒，唤醒会有比较明显的磁盘启动声音，并且需要等待 5s 左右才能使用。\n但蛋疼的是不知道是系统原因还是什么，每次硬盘休眠后即便我没有对其进行操作仍然会自行唤醒，导致我总是能频繁听到硬盘的启停声，我还专门观察了一下，就算我躺在床上玩手机，完全不碰电脑，这块硬盘仍然会频繁的启停，先不说这烦人的声音，网上查了下，貌似这样频繁启停还会损害硬盘健康，还不如一直通电运行，为了解决这个问题，我打算写个小程序，期间遇到了一些问题，特此记录一下。\n问题1 读不出文件内容 具体流程是先在这块硬盘上创建一个文件，并获取 fd，然后用这个 fd 写入一段文字，再在一个死循环里不断读取这个 fd，我想的是这样不断读取硬盘，来达到禁止休眠的效果，结果发现写入成功，但是读直接报 EOF，后来研究发现，貌似是写入后导致 fd 偏移量改变移到末尾，此时再读取就会直接报 EOF，所以在读取前需要调用 Seek 来重置偏移量。\npackage main import ( \u0026quot;io\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;time\u0026quot; ) func main() { p := \u0026quot;/Volumes/4t/DO_NOT_SLEEP\u0026quot; f, err := os.OpenFile(p, os.O_CREATE|os.O_RDWR, 0777) if err != nil { panic(err) } defer f.Close() _, err = f.WriteString(\u0026quot;123\u0026quot;) if err != nil { panic(err) } log.Println(\u0026quot;DO_NOT_SLEEP is ready\u0026quot;) // 注意：上面的写入操作会改变偏移量，导致后续的读取操作为 EOF // 需要调用 Seek 来重置偏移量，这里设置为 SEEK_SET + 0 也就是从头开始读 f.Seek(0, os.SEEK_SET) b := make([]byte, 10) for { n, err := f.Read(b) if err != nil { log.Println(err) if err == io.EOF { break } continue } log.Println(n, string(b[:n])) f.Seek(0, os.SEEK_SET) time.Sleep(time.Second * 3) } } 问题2 虽然在不断读取，但硬盘依然会休眠 还是上面的程序，我跑起来以后发现终端在不断打印读取到的内容，本以为 ok 了，结果过了几分钟硬盘又给休眠了，最离谱的是此时终端还在不断打印日志。。。这是为什么？难不成 fd 打开后，会把文件内容缓存，之后直接从缓存中读？我又尝试了在一个死循环里不断调用 os.OpenFile，但是然并卵，磁盘依然会休眠，这个问题还有待研究，不知道是 go runtime 的设计问题，还是 linux fd 的机制问题\n解决 既然读解决不了问题，那我只能试试写了，感觉写问题应该不大，其实我一开始就是想用写操作来解决问题的，但是了解到读操作对硬盘是无损害的，而写操作会损害硬盘寿命，所以才尝试用读操作解决，\n大致思路也很简单，就是不断向这个文件中写入文本即可，这里我把 open 也放到了死循环中，因为我不知道在死循环中操作同一个 fd 会不会出现上面的硬盘依然休眠的问题\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;time\u0026quot; ) func main() { p := \u0026quot;/Volumes/4t/DO_NOT_SLEEP\u0026quot; for { f, err := os.OpenFile(p, os.O_CREATE|os.O_RDWR, 0777) if err != nil { panic(err) } f.WriteString(\u0026quot;123\u0026quot;) log.Println(\u0026quot;write ok\u0026quot;) f.Close() time.Sleep(time.Second * 5) } } 可能的原因 无意间看到一盘 页缓存 的文章，里面说系统会把磁盘里的内容放到缓存中，下次如果访问同样的内容，则不需要读取硬盘，而是直接从缓存中读取即可，这样就避免了 IO 的开销，提高了性能。如果是这样的话，那么问题2 就解释的通了\n","date":"2022年06月08日","permalink":"/posts/go_seek/","summary":"引子 最近买了块机械硬盘作为仓库盘，用硬盘盒连接到 mac，但是这块硬盘貌似有休眠机制（不确定），如果一段时间没有读写操作就会停转，等到需要操作时才会唤醒，唤醒会有比较明显的磁盘启动声音，并且需要等待 5s 左右才能使用。","title":"go 文件操作的\"坑\"之 Seek()"},{"contents":"引子 做过开发的同学都遇到过给程序进行配置的场景，比如非常常见的配置文件，将 Mysql 的地址、端口、用户名、密码等信息写到一个配置文件里，然后程序内部通过读取这个配置文件来获取信息，防止了硬编码信息到程序内部，这样如果发生变动只需要修改配置文件即可，无需重新编译整个程序；另一种常见的方式是运行程序时指定参数来进行配置，比如 Go 标准库里的 flag 包就可以方便的进行参数指定，然后运行时只要使用诸如 go run main.go -a 123 -b 456 这样的方式，便可读取到配置信息，比如在这个例子里传递了两个参数：a=123，b=456\n那么在 k8s 中如何对容器进行配置呢？\n在 docker 中定义命令与参数 容器中运行的完整指令由两部分组成：命令和参数，比如 ./main -a 123，main 就是命令，-a 就是参数，在 Dockerfile 中分别对应的是 ENTRYPOINT（命令）和 CMD（参数）。\n实战：\n这个 image 会调用 fortuneloop.sh：\nFROM ubuntu:latest RUN apt-get update ; apt-get -y install fortune # 安装 fortune ADD fortuneloop.sh /bin/fortuneloop.sh ENTRYPOINT [\u0026quot;/bin/fortuneloop.sh\u0026quot;] CMD [\u0026quot;10\u0026quot;] # 每隔 10s 输出一次 这里指定了 ENTRYPOINT 和 CMD，等同于命令 /bin/fortuneloop.sh 10\n附：fortuneloop.sh 文件内容如下：\n#!/bin/bash trap \u0026quot;exit\u0026quot; SIGINT INTERVAL=$1 # 读取第一个参数，比如执行 fortuneloop.sh 10，那么 $1=10 echo Configured to generate new fortune every $INTERVAL seconds mkdir -p /var/htdocs while : do echo $(date) Writing fortune to /var/htdocs/index.html /usr/games/fortune \u0026gt; /var/htdocs/index.html # 执行 fortune 命令，将输出的内容写入到 index.html sleep $INTERVAL # 睡眠指定的时间 done 接下来构建并运行：\n# 构建镜像 $ docker build -t stdoutt/fortune-args-arm64 . # 运行 $ docker run -it stdoutt/fortune-args-arm6 # 观察效果 # 每隔 10s 就会打印一次 Configured to generate new fortune every 10 seconds Sun Sep 25 02:23:01 UTC 2022 Writing fortune to /var/htdocs/index.html Sun Sep 25 02:23:11 UTC 2022 Writing fortune to /var/htdocs/index.html 因为在 Dockerfile 的 CMD 中指定值为 10，所以默认会每隔 10 秒打印一次，也可以自己指定值将默认值覆盖掉：\n$ docker run -it stdoutt/fortune-args-arm6 1 Configured to generate new fortune every 1 seconds Sun Sep 25 02:24:55 UTC 2022 Writing fortune to /var/htdocs/index.html Sun Sep 25 02:24:56 UTC 2022 Writing fortune to /var/htdocs/index.html Sun Sep 25 02:24:57 UTC 2022 Writing fortune to /var/htdocs/index.html 现在每间隔 1s 就会打印一次\n在 k8s 中定义命令与参数 在 k8s 中定义容器时，可以覆盖掉镜像的 ENTRYPOINT 和 CMD，通过 command 和 args 来指定，类似一下这种形式：\nkind: Pod spec: containers: - image: some/image command: [\u0026quot;/bin/command\u0026quot;] # 等同于 Dockerfile 中的 ENTRYPOINT args: [\u0026quot;argl\u0026quot;, \u0026quot;arg2\u0026quot;, \u0026quot;arg3\u0026quot;] # 等同于 Dockerfile 中的 CMD 其中 command 等同于 Dockerfile 中的 ENTRYPOINT，args 等同于 Dockerfile 中的 CMD\n实战 yaml 如下：\napiVersion: v1 kind: Pod metadata: name: fortune2s spec: containers: - image: stdoutt/fortune-args-arm64 # 注意此 image 仅适用于 arm64 args: [\u0026quot;2\u0026quot;] # 等同于 Dockerfile 中的 CMD name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 测试效果：\n# 创建 pod $ kubectl apply -f fortune-pod-args.yaml pod/fortune2s created # 开启端口转发 $ kubectl port-forward fortune2s 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 # 每隔 2s （和 args 中定义的一样）curl 一次，因为 args 也定义为 2s，代表每 2s 写入一次谚语，所以 # 每次 curl 都应该输出不同的内容 $ while true;do curl localhost:8080;sleep 2s;done Wagner's music is better than it sounds. -- Mark Twain You plan things that you do not even attempt because of your extreme caution. You never hesitate to tackle the most difficult problems. The lovely woman-child Kaa was mercilessly chained to the cruel post of the warrior-chief Beast, with his barbarian tribe now stacking wood at her nubile feet, when the strong clear voice of the poetic and heroic Handsomas roared, 'Flick your Bic, crisp that chick, and you'll feel my steel through your last meal!' -- Winning sentence, 1984 Bulwer-Lytton bad fiction contest. 在容器定义中指定环境变量 Dockerfile\nFROM ubuntu:latest RUN apt-get update ; apt-get -y install fortune ADD fortuneloop.sh /bin/fortuneloop.sh ENTRYPOINT [\u0026quot;/bin/fortuneloop.sh\u0026quot;] fortuneloop.sh\n#!/bin/bash trap \u0026quot;exit\u0026quot; SIGINT echo Configured to generate new fortune every $INTERVAL seconds mkdir -p /var/htdocs while : do echo $(date) Writing fortune to /var/htdocs/index.html /usr/games/fortune \u0026gt; /var/htdocs/index.html sleep $INTERVAL # 读取 INTERVAL 这个环境变量 done fortune-pod-env.yaml\napiVersion: v1 kind: Pod metadata: name: fortune-env spec: containers: - image: stdoutt/fortune-env-arm64 # 注意此 image 仅适用于 arm64 env: # 在这里设置环境变量，用于 fortuneloop.sh 的读取 - name: INTERVAL value: \u0026quot;5\u0026quot; name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 测试效果：\n$ kubectl apply -f fortune-env.yaml pod/fortune-env created $ kubectl get po NAME READY STATUS RESTARTS AGE fortune2s 2/2 Running 0 84m fortune-env 2/2 Running 0 18s $ kubectl port-forward fortune-env 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 $ while true;do curl localhost:8080;sleep 5s;done # sleep 同样的时间 It may or may not be worthwhile, but it still has to be done. When I reflect upon the number of disagreeable people who I know who have gone to a better world, I am moved to lead a different life. -- Mark Twain, \u0026quot;Pudd'nhead Wilson's Calendar\u0026quot; # 这里额外测试一下，环境变量里设置的值是 5，也就是每 5s 生成一句新谚语并写入 index.html， # 但是这里每隔 1s 就 curl 一次，会发现前 5 此输出的内容都相同 $ while true;do curl localhost:8080;sleep 1s;done You will be surrounded by luxury. You will be surrounded by luxury. You will be surrounded by luxury. You will be surrounded by luxury. You will be surrounded by luxury. Q:\tWhat happens when four WASPs find themselves in the same room? A:\tA dinner party. ConfigMap 上面的方式都是直接在 pod 中硬编码的，就像 - name: INTERVAL value: \u0026quot;5\u0026quot; 这样，如果之后需要更改 value 的值，那么需要修改 pod 的 yaml，然后删除旧的重新创建新的（这里还待验证），比较麻烦。\nConfigMap 将配置相关的内容抽取出来做成了一个单独的部分，然后 pod 再引用 ConfigMap 即可（又是万能的解决方法：加一层中间层），相比较直接在 pod spec 中的硬编码而言更加灵活。\n命令 使用 create 命令而不是 yaml 来创建一个 configmap（之后简称为 cm），通过 --from-literal 来指定键值对（kv），形式为：\u0026ndash;from-literal=[key]=[value]\n$ kubectl create configmap fortune-config --from-literal=sleep-interval=25 configmap/fortune-config created 通过指定多个 --from-literal 来添加多个 kv：\n$ kubectl create configmap myconfigmap --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two configmap/myconfigmap created 查看 cm 的 yaml：\n$ kubectl get cm myconfigmap -o yaml apiVersion: v1 data: bar: baz foo: bar one: two kind: ConfigMap metadata: creationTimestamp: \u0026quot;2022-06-07T11:39:32Z\u0026quot; name: myconfigmap namespace: default resourceVersion: \u0026quot;73938\u0026quot; uid: c9814e7e-f3f9-4149-8530-74ea963420d9 发现其实定义的 kv 都在 data 里，metadata 里唯一需要明确指定的是 name，所以通过 yaml 的形式来创建 cm 也比较简单\n如果添加两个同样的 key 会怎样？\n$ kubectl create configmap fortune-config1 --from-literal=sleep-interval=25 --from-literal=sleep-interval=30 error: cannot add key \u0026quot;sleep-interval\u0026quot;, another key by that name already exists in Data for ConfigMap \u0026quot;fortune-config1\u0026quot; 可以看到直接创建失败了\ncm 还可以将某个文件的内容作为 value 值，使用 --from-file 来引用文件：\n$ kubectl create cm my-config --from-file=conf.conf configmap/my-config created conf.conf 内容如下：\na:a a:a b:b c:c d:d 查看该 cm 的 yaml：\n$ kubectl get cm my-config -o yaml apiVersion: v1 data: conf.conf: | a:a b:b c:c d:d kind: ConfigMap metadata: creationTimestamp: \u0026quot;2022-06-07T12:58:09Z\u0026quot; name: my-config namespace: default resourceVersion: \u0026quot;77327\u0026quot; uid: 2937d396-8692-4c16-8798-236fc4015299 发现这里是用文件名来做 key 的，value 就是文件的内容，只不过不是常规的 key:value 形式，而是在 key: 后多加了一个 |，之后才是 value\n同样也可以通过指定多个 --from-file 来引用多个文件作为 cm 的条目，比如（这里我还专门测试了一下该 flag 是否对文件后缀有要求，我传入了一个 txt，这不是常见的配置文件后缀）：\n$ kubectl create cm my-config1 --from-file=conf.conf --from-file conf.txt configmap/my-config1 created 查看 yaml：\n$ kubectl get cm my-config1 -o yaml apiVersion: v1 data: conf.conf: | a:a b:b c:c d:d conf.txt: | 大数据断开了撒娇漏打卡所经历的卡所经历的 kind: ConfigMap metadata: creationTimestamp: \u0026quot;2022-06-07T13:13:16Z\u0026quot; name: my-config1 namespace: default resourceVersion: \u0026quot;77980\u0026quot; uid: 3332d608-4184-48bc-8481-bd78ccf8b312 貌似这里是直接简单粗暴的把文件内容作为 value 了，至于文件后缀是什么，cm 不会去考虑，不过想想确实也只能这么做，毕竟配置文件后缀非常多，conf、yaml、toml 等等，可能还有一些冷门的格式，如果要专门维护一个允许的后缀列表显然太麻烦了，也没啥太大必要。\n上面的 key 都是直接使用文件名，如果想自行指定，可以使用类似下面的语句：\n$ kubectl create cm my-config1 --from-file=myconf=conf.conf 这里设置 key 为 myconf，也就是说格式是：\u0026ndash;from-file=[自定义 key 名]=[value]\n给容器传递 ConfigMap 作为环境变量 fortune-pod-env-configmap.yaml\napiVersion: v1 kind: Pod metadata: name: fortune-env-from-configmap spec: containers: - image: stdoutt/fortune-env-arm64 # 注意该 image 仅适用于 arm64 env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 如果引用的 ConfigMap 不存在会怎样 测试：\n$ kubectl apply -f fortune-pod-env-configmap.yaml pod/fortune-env-from-configmap created $ kubectl get po NAME READY STATUS RESTARTS AGE fortune2s 2/2 Running 0 117m fortune-env 2/2 Running 0 33m fortune-env-from-configmap 1/2 CreateContainerConfigError 0 2m27s 发现当前 pod 的状态是 CreateContainerConfigError，这是肯定的，env 会从名为 keyfortune-config 的 configMap 中读取，但是现在没有创建这个 configMap，所以当然无法正常启动\n查看 describe 也会发现有相应的警告：\nWarning Failed 80s (x7 over 2m28s) kubelet Error: configmap \u0026quot;fortune-config\u0026quot; not found 需要创建一个 configMap 来解决：\n# 创建一个 configMap，同时向其中写入 sleep-interval=5 $ kubectl create configmap fortune-config --from-literal=sleep-interval=5 configmap/fortune-config created # 在创建完 configMap 之后，之前的 pod 立马成功运行了 $ kubectl get po NAME READY STATUS RESTARTS AGE fortune2s 2/2 Running 0 121m fortune-env 2/2 Running 0 37m fortune-env-from-configmap 2/2 Running 0 7m3s 也就是说如果引用的 ConfigMap 不存在，那么 pod 会创建失败，当所需要的 ConfigMap 被创建后，这个失败的 pod 会自动重启，无须重新创建 pod。\n$ kubectl port-forward fortune-env 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 $ while true;do curl localhost:8080;sleep 5s;done You'll feel much better once you've given up hope. When I reflect upon the number of disagreeable people who I know who have gone to a better world, I am moved to lead a different life. -- Mark Twain, \u0026quot;Pudd'nhead Wilson's Calendar\u0026quot; 给容器传递 ConfigMap 作为参数 pod.spec.containers.args 不能直接引用 ConfigMap 中的条目，但是可以”曲线救国“，将 ConfigMap 中的条目设置为一个环境变量，然后再让 args 来引用这个环境变量，便可达到效果。\nfortune-pod-args-configmap.yaml\napiVersion: v1 kind: Pod metadata: name: fortune-args-from-configmap spec: containers: - image: stdoutt/fortune-args-arm64 # 注意该 image 仅适用于 arm64 env: - name: INTERVAL # 这个环境变量引用了 fortune-config 中的 sleep-interval 条目 valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval args: [\u0026quot;$(INTERVAL)\u0026quot;] # 引用 INTERVAL 这个环境变量 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 测试效果：\n$ kubectl apply -f fortune-pod-args-configmap.yaml pod/fortune-args-from-configmap created $ kubectl get po NAME READY STATUS RESTARTS AGE fortune2s 2/2 Running 0 7h13m fortune-env 2/2 Running 0 5h49m fortune-env-from-configmap 2/2 Running 0 5h18m fortune-args-from-configmap 2/2 Running 0 5s $ kubectl port-forward fortune-env 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 $ while true;do curl localhost:8080;sleep 5s;done What good is an obscenity trial except to popularize literature? -- Nero Wolfe, \u0026quot;The League of Frightened Men\u0026quot; They spell it \u0026quot;da Vinci\u0026quot; and pronounce it \u0026quot;da Vinchy\u0026quot;. Foreigners always spell better than they pronounce. -- Mark Twain Give your very best today. Heaven knows it's little enough. 使用 ConfigMap 卷 前面提到的环境变量或者是参数的方式比较适用于配置数量较少的情况，如果配置数量过多，那么就不太适合了，此外还有一些特殊的配置文件格式，比如 nginx ，也不太适合。（待修改）\n还可以将 ConfigMap 挂载（感觉挂载这个词有点不太合适） 做成一个 卷，然后pod 中的容器可以将这个卷挂载到自己的某个目录下，来完成配置，比如一个 nginx 容器可以将卷挂载到自己的 /etc/nginx/conf.d 目录下，就可以读取卷中的配置信息。\nconfigMap卷会将 ConfigMap 中的每个条目均暴露成一个文件。 运行在容器中的进程可通过读取文件内容获得对应的条目值 。\n实践 创建一个文件夹 configmap-files：\n$ mkdir configmap-files 然后向里面写入下面的这两个文件：\nmy-nginx-config.conf\nserver { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval\n25 查看文件夹：\n$ tree . ├── my-nginx-config.conf └── sleep-interval 创建一个 cm，将这个文件夹作为 value\n$ kubectl create configmap fortune-config --from-file=configmap-files error: failed to create configmap: configmaps \u0026quot;fortune-config\u0026quot; already exists 发现之前以及存在同名的 cm 了，看来重复创建 cm 不能覆盖更新，必须要删除旧的才能创建新的：\n$ kubectl get cm NAME DATA AGE kube-root-ca.crt 1 21h fortune-config 1 6h4m $ kubectl delete cm fortune-config configmap \u0026quot;fortune-config\u0026quot; deleted $ kubectl create configmap fortune-config --from-file=configmap-files configmap/fortune-config created # 查看 cm $ kubectl get cm fortune-config -o yaml apiVersion: v1 data: my-nginx-config.conf: | server { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25 kind: ConfigMap metadata: creationTimestamp: \u0026quot;2022-09-25T11:05:49Z\u0026quot; name: fortune-config namespace: default resourceVersion: \u0026quot;12886\u0026quot; uid: c9f5ee8a-9078-46cc-955a-3ec72c6ceb62 发现该 cm 有两个条目，条目名就是目标文件夹下的每个文件的文件名，value 就是文件内容\n接下来定义一个 pod，这个 pod 里的 nginx 容器会将 configMap 卷挂载到自己的 /etc/nginx/conf.d 目录下，而 nginx 会自动将该路径下的所有以 conf 为后缀的文件嵌入到默认配置文件中，而 configMap 卷里面刚好有我们定义的开启了 gzip 的配置文件 my-nginx-config.conf，nginx 读取这个文件来开启 gzip，而卷中的另一个文件 sleep-interval 因为不是以 conf 为后缀的，所以 nginx 不会将其内容嵌入到配置文件。\nfortune-configmap-volume\napiVersion: v1 kind: Pod metadata: name: fortune-configmap-volume spec: containers: - image: stdoutt/fortune-env-arm64 env: - name: INTERVAL # 从 fortune-config 这个 configmap 中读取 sleep-interval，用其值填充环境变量 INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d # 将 config 卷挂载到该路径下 readOnly: true - name: config mountPath: /tmp/whole-fortune-config-volume # 将 config 卷挂载到该路径下 readOnly: true ports: - containerPort: 80 name: http protocol: TCP volumes: - name: html emptyDir: {} - name: config # 定义一个名为 config 的卷，卷引用了 fortune-config 这个 cm 里的内容，会将每个条目做成一个文件 configMap: name: fortune-config 测试效果：\n$ kubectl apply -f fortune-pod-configmap-volume.yaml $ kubectl get po NAME READY STATUS RESTARTS AGE fortune-configmap-volume 2/2 Running 0 13s $ kubectl port-forward fortune-configmap-volume 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 $ curl -H \u0026quot;Accept-Encoding: gzip\u0026quot; -I localhost:8080 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Sun, 25 Sep 2022 11:38:16 GMT Content-Type: text/html Last-Modified: Sun, 25 Sep 2022 11:38:06 GMT Connection: keep-alive ETag: W/\u0026quot;63303d9e-135\u0026quot; Content-Encoding: gzip # 这里说明已经开启了 gzip 压缩，配置文件已经生效 验证挂载：\n前面提到过 ConfigMap 卷会将 config 中的每个条目单独做成一个文件，我们实际验证一下：\n# 进入 web-server 这个容器 $ kubectl exec -it fortune-configmap-volume -c web-server -- /bin/sh # 看一下挂载路径 / # ls /tmp/whole-fortune-config-volume/ my-nginx-config.conf sleep-interval / # ls /etc/nginx/conf.d my-nginx-config.conf sleep-interval # 发现 configMap 卷全部都被挂载了，且里面有两个文件，每个文件对应 cm 中的一个条目 # 输出一下文件内容 / # cat /etc/nginx/conf.d/sleep-interval 25 / # cat /tmp/whole-fortune-config-volume/my-nginx-config.conf server { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } 只挂载 ConfigMap 中的部分条目 上面的例子存在一个问题：sleep-interval 这个配置文件也被挂载到了 nginx 的目录下，但是该配置文件与 nginx 是没任何关系的，为了解决这个问题，可以用指定 items 的方式，如下：\nfortune-pod-configmap-volume-with-items\napiVersion: v1 kind: Pod metadata: name: fortune-configmap-volume-with-items spec: containers: - image: stdoutt/fortune-env-arm64 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d/ readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config items: - key: my-nginx-config.conf # 引用 cm 中的 my-nginx-config.conf 这条条目 path: gzip.conf # 将这条条目的内容放到一个名为 gzip.conf 的文件中 测试效果：\n$ kubectl apply -f fortune-pod-configmap-volume-with-items $ kubectl get po NAME READY STATUS RESTARTS AGE fortune2s 2/2 Running 0 10h fortune-env 2/2 Running 0 8h fortune-env-from-configmap 2/2 Running 0 8h fortune-args-from-configmap 2/2 Running 0 171m fortune-configmap-volume 2/2 Running 0 92m fortune-configmap-volume-with-items 2/2 Running 0 14s $ kubectl exec -it fortune-configmap-volume-with-items -c web-server -- /bin/sh / # ls /etc/nginx/ conf.d/ fastcgi_params modules/ scgi_params fastcgi.conf mime.types nginx.conf uwsgi_params / # ls /etc/nginx/conf.d gzip.conf / # cat /etc/nginx/conf.d/gzip.conf server { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } 可以看到挂载的目录 /etc/nginx/conf.d 下只有 gzip.conf 这一个文件了，之前的 sleep-interval 文件已经没有了，并且 gzip.conf 文件的内容就是 configMap 中 my-nginx-config.conf 这个条目的内容。\n热更新 使用环境变量或者命令行参数作为配置源的弊端在于无法在进程运行时更新配置，也就是说如果配置信息发生了变化，需要重启应用来重新读取，达到更新的效果。 而将 ConfigMap 暴露为卷可以达到配置热更新的效果，也就是无须重新创建 pod 或者重启容器。\n使用 kubectl edit 修改 ConfigMap：\n$ kubectl edit cm fortune-config # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this f # reopened with the relevant failures. # apiVersion: v1 data: my-nginx-config.conf: | server { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25 kind: ConfigMap metadata: creationTimestamp: \u0026quot;2022-09-25T11:05:49Z\u0026quot; name: fortune-config namespace: default resourceVersion: \u0026quot;12886\u0026quot; uid: c9f5ee8a-9078-46cc-955a-3ec72c6ceb62 ~ ~ ~ - /tmp/kubectl-edit-4270724913.yaml 12/29 41% 和 vim 一样，按下 i，将 gzip on 修改为 gzip off，然后 esc，输入 :wq，退出并保存，输出 configmap/fortune-config edited\n查看修改是否生效：\n$ kubectl exec -it fortune-configmap-volume-with-items -c web-server -- cat /etc/nginx/conf.d/gzip.conf server { listen 80; server_name www.kubia-example.com; gzip off; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } 发现修改已经生效\n配置文件已经修改，继续测试一下 nginx 这边是否生效：\n$ kubectl port-forward fortune-configmap-volume 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 $ curl -H \u0026quot;Accept-Encoding: gzip\u0026quot; -I localhost:8080 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Sun, 25 Sep 2022 13:27:14 GMT Content-Type: text/html Last-Modified: Sun, 25 Sep 2022 13:26:55 GMT Connection: keep-alive ETag: W/\u0026quot;6330571f-c6\u0026quot; Content-Encoding: gzip 发现依然输出了 Content-Encoding: gzip ，这是因为虽然修改了配置文件，但是 nginx 并不会监听配置文件并作出响应，所以不会更新 gzip 的状态将其 off，需要手动通知 nginx ，告诉它配置文件发生了变化，需要你重新读取一下：\n$ kubectl exec -it fortune-configmap-volume-with-items -c web-server -- nginx -s reload 2022/09/25 13:30:40 [notice] 39#39: signal process started $ curl -H \u0026quot;Accept-Encoding: gzip\u0026quot; -I localhost:8080 HTTP/1.1 200 OK Server: nginx/1.23.1 Date: Sun, 25 Sep 2022 13:32:37 GMT Content-Type: text/html Content-Length: 27 Last-Modified: Sun, 25 Sep 2022 13:32:37 GMT Connection: keep-alive ETag: \u0026quot;63305875-1b\u0026quot; Accept-Ranges: bytes 此时发现 Content-Encoding: gzip 已经没有了，表示 nginx 这边也已经生效了\n整个过程都没有重启或者重建过 pod，但是最终也达到了更新配置的效果，所以可以做到热更新。\n但是对 k8s 而言，它只是保证在更新 ConfigMap 后同步修改其对应的 ConfigMap 卷，但是对于引用这个 ConfigMap 的应用，k8s 就管不到了，如果应用不会对配置文件的修改作出响应，或者类似 nginx 提供一个重新载入的命令，那么可能最终还是需要重启或者重建 pod（个人猜测）\n","date":"2022年06月07日","permalink":"/posts/k8s-configmap/","summary":"引子 做过开发的同学都遇到过给程序进行配置的场景，比如非常常见的配置文件，将 Mysql 的地址、端口、用户名、密码等信息写到一个配置文件里，然后程序内部通过读取这个配置文件来获取信息，防止了硬编码信息到程序内部，这样如果发生变动只需要修改配置文件即可，无需重新编译整个程序；另一种常见的方式是运行程序时指定参数来进行配置，比如 Go 标准库里的 flag 包就可以方便的进行参数指定，然后运行时只要使用诸如 go run main.","title":"k8s ConfigMap"},{"contents":"主流的垃圾回收算法 引用计数法 根据对象自身的引用计数来回收，当引用计数归零时进行回收，但是计数频繁更新会带来更多开销，且无法解决循环引用的问题。\n优点：简单直接，回收速度快 缺点：需要额外的空间存放计数，无法处理循环引用的情况； 标记清除法 标记出所有不需要回收的对象，在标记完成后统一回收掉所有未被标记的对象。\n优点：简单直接，速度快，适合可回收对象不多的场景 缺点：会造成不连续的内存空间（内存碎片），导致有大的对象创建的时候，明明内存中总内存是够的，但是空间不是连续的造成对象无法分配，此外还需要 STW 来确保标记清除过程不被外部影响（比如 gc 前脚刚把一个对象标记为可达，用户程序后脚就释放了它）； （内存碎片) STW（Stop The World，Start The World）\n通常意义上指的是从Stop The World到Start The World这一段时间间隔。垃圾回收过程中为了保证准确性、防止无止境的内存增长等问题而不可避免的需要停止赋值器进一步操作对象图以完成垃圾回收。STW时间越长，对用户代码造成的影响越大。\n标记整理法 这种算法用于解决 标记清除法 的内存碎片问题，它的标记阶段与标记清除法相同，但是会在完成标记工作后，移动非垃圾数据，使它们尽可能紧凑的放在内存中。 优点：解决了内存碎片的问题 缺点：性能低，因为在移动对象的时候不仅需要移动对象还要维护对象的引用地址，可能需要对内存经过几次扫描才能完成 复制回收法 和 标记整理法 一样，这种算法也是为了解决内存碎片问题，也会移动数据，它把堆内存划分为两个相等的空间 From 和 To，程序执行时使用 From 空间，垃圾回收执行时，会扫描 From 空间，把能追踪到的数据复制到 To 空间，当所有有用的数据都复制到 To 空间后，把 From 和 To 空间的角色交换一下，原来的 From 空间可以全部回收作为新的 To 空间；原来的 To 空间因为保存的都是有用的数据，所以可以作为新的 From 空间。 优点：解决了内存碎片的问题，每次清除针对的都是整块内存，但是因为移动对象需要耗费时间，效率低于标记清除法； 缺点：只有一半的堆内存可以使用，导致堆内存使用效率低。为了提高内存的使用率，通常会和其他垃圾回收算法搭配使用，只在一部分堆内存中使用复制回收， 分代式 将对象根据存活时间的长短进行分类，存活时间小于某个值的为年轻代，存活时间大于某个值的为老年代，永远不会参与回收的对象为永久代。并根据分代假设（如果一个对象存活时间不长则倾向于被回收，如果一个对象已经存活很长时间则倾向于存活更长时间）对对象进行回收。\n三色标记法 到这里才是真正的重头戏，因为 Go 采用的正是这种 GC 算法。\n三色标记法将对象的颜色分为了黑、灰、白，三种颜色。\n黑色：该对象已经被标记过了，且该对象下的属性也全部都被标记过了（程序所需要的对象）； 灰色：该对象已经被标记过了，但该对象下的属性没有全被标记完（GC需要从此对象中去寻找垃圾）； 白色：该对象没有被标记过（垃圾对象）； 在垃圾收集器开始工作时，从 GC Roots 开始进行遍历访问，访问步骤可以分为下面几步：\nGC Roots 根对象会被标记成灰色； 然后从灰色集合中获取对象，将其标记为黑色，将该对象引用到的对象标记为灰色； 重复步骤2，直到没有灰色集合可以标记为止； 结束后，剩下的没有被标记的白色对象即为 GC Roots 不可达，可以进行回收。 ❓gc root 节点具体指哪些？ 说法1，来源： https://www.bilibili.com/video/BV1n5411H7qS?spm_id_from=333.999.0.0，2:30 秒\n可以把栈、数据段（.data，全局变量）上的数据对象作为 root\n说法 2，来源：https://www.luozhiyun.com/archives/475\nGC 执行根节点的标记，这包括扫描所有的栈、全局对象以及不在堆中的运行时数据结构。\n说法 3，来源：https://www.bookstack.cn/read/qcrao-Go-Questions/spilt.2.GC-GC.md\n根对象在垃圾回收的术语中又叫做根集合，它是垃圾回收器在标记过程时最先检查的对象，包括：\n全局变量：程序在编译期就能确定的那些存在于程序整个生命周期的变量。 执行栈：每个 goroutine 都包含自己的执行栈，这些执行栈上包含栈上的变量及指向分配的堆内存区块的指针。 寄存器：寄存器的值可能表示一个指针，参与计算的这些指针可能指向某些赋值器分配的堆内存区块。 前面提到过，这类标记清理算法都需要 STW 来保证垃圾回收过程不受影响，那么如果没有 STW 会发生什么问题呢？\n问题1：多标\n通俗的说，就是一个本该被回收的对象（白色），却被错误的标记成了非白色，导致本轮 gc 没有正确回收，导致产生浮动垃圾。\n比如下面这张图： 在步骤2中，gc 将 D 标记为了黑色，将 D 的可达对象 E 标记为了灰色，但在执行步骤3前，用户程序断开了 D 到 E 的引用（比如从 D = E 变成 D = null），此时 E 及其引用的 F，G 都会变为垃圾，因为它们都不再可达了，但是因为 D 已经标记成黑色，不会再次对其进行扫描了，这会导致 gc 继续标记，将 E 从灰色变为黑色，E 的引用对象 F 和 G 标记为灰色，最后将 F 和 G 标记为黑色。至此，该轮 gc 结束，E，F，G 都被错误的标记为了黑色而没有被及时回收。\n❓写屏障可以解决上面这种情况吗？\n问题2：漏标\n通俗的说，就是把一个有用的对象给错误的回收掉了，也就是悬挂指针问题。\n比如下面这张图： 在步骤2中，E 已经变为灰色，但在 gc 进一步标记之前，用户程序断开了 E 到 G 的引用，同时已经标记为黑色的 D 新增了对 G 的引用，之后 gc 标记时发现 E 到 G 已经不可达，便不会将 G 标记为灰色，而因为 D 已经是黑色不会再次扫描，所以也不会对 G 进行标记，最终，G 依然保持着白色，导致最后被 gc 清理。\n现在问题来了，用 STW 会导致 gc 过程中整个程序不可用，不用 STW 又会出现内存错误，有没有什么办法可以尽可能的让用户程序和 gc 交替（并发）执行，减少 STW 造成的影响，同时又不会出现内存错误呢？比如 增量式回收，就是将 gc 过程分多次完成，也就是用户程序与垃圾回收交替执行，如下图： 只要满足以下条件，就可以避免之前的内存问题了：\n强三色不变式 黑色对象不会引用白色对象，称为 强三色不变式，比如在 问题2：漏标 中，如果避免黑色对象 D 到白色对象 G 的引用，就不会出现错误回收的问题了。\n弱三色不变式 黑色对象可以引用白色对象，前提是可以通过一个灰色对象抵达该白色对象，比如 A（黑）引用了 B（白色），而 C（灰色） 引用了 D（白色），D （白色）又引用了 B（白色），这种情况可以通过 C -\u0026gt; D -\u0026gt; B 抵达 B，这样 B 虽然被一个黑色对象引用，但是最终还是可达，不会被遗漏，这种情况称为 弱三色不变性 。\n实现强弱三色不变式的通常做法是建立 读/写屏障。\n写屏障 写屏障 会在写操作中插入指令，目的是把对象的修改通知到垃圾回收器，比如当一个黑色对象尝试引用一个白色对象时，就会触发写屏障，此时为了保证 强三色不变式，就会将被引用的白色对象变为灰色，或者将引用白色对象的黑色对象变为灰色，这种写屏障属于 插入写屏障 ；又或者是一个灰色对象尝试去除对一个白色对象的引用，为了保证 弱三色不变式，会将这个白色对象变为灰色，这种写屏障属于 删除写屏障。\n插入写屏障的缺点 **插入写屏障 **虽然实现非常简单并且也能保证强三色不变性，但是它也有明显的缺点。因为栈上的对象在垃圾收集中也会被认为是根对象，所以为了保证内存的安全，必须：\n为栈上的对象增加写屏障 或者\n在标记阶段完成重新对栈上的对象进行扫描 这两种方法各有各的缺点，前者会大幅度增加写入指针的额外开销，后者重新扫描栈对象时需要暂停程序，垃圾收集算法的设计者需要在这两者之间做出权衡。\n删除写屏障的缺点 而 删除写屏障 会存在 回收效率低 的问题，因为当删除灰色对象到白色对象的引用时，会直接将白色对象赋为灰色，但是这个白色对象可能本身是垃圾，可以在本轮被清除了，但是却逃过了本轮的清理。比如：\n黑 \u0026mdash;\u0026ndash;\u0026gt; 灰 \u0026mdash;\u0026ndash;\u0026gt; 白\n如果在扫描到白之前，断开灰到白的连接，此时因为白是整个调用链的最后一个对象，所以将会成为垃圾，但是因为删除写屏障的存在，会将该白色对象变为灰色，导致本轮 GC 未被清理。\n读屏障 读屏障 适用于移动式垃圾回收，非移动式垃圾回收天然的不需要读屏障。像复制式回收这样会移动数据来避免碎片化，那么 gc 和用户程序交替执行时，读数据便也不那么安全了，比如下图： 回收器已经将 A 从 From 复制到 To 空间了，之后交替执行的用户程序却读取了 From 空间中的老对象，并且在对象 B 中引用了这个老对象 A，而后续回收器复制 B 到 To 空间后，B 的新副本持有的依然是 A 的老对象指针，当 From 空间整体被回收时，因为 B 指向的依然是 From 空间的 A，所以访问便会出错。这种情况下，就需要建立读屏障，确保用户程序不会访问到老对象，例如在检测到引用对象已经存在新副本时，转而读取 To 空间的新副本。\nGo GC Go 的 GC 结合了插入写屏障和删除写屏障的优点，称之为 混合写屏障，它的规则如下：\nGC 开始将栈上的对象全部扫描并标记为黑色； GC 期间，任何在栈上创建的新对象，均为黑色； 被删除的堆对象标记为灰色； 被添加的堆对象标记为灰色； Go 团队在最终实现时，没有为所有栈上的指针写操作启用写屏障，而是当发生栈上的写操作时，将栈标记为灰色，但此举产生了 灰色赋值器，将会需要标记终止阶段 STW 时对这些栈进行 重新扫描（v1.7 版本之前）。\n一些例子：\n看到这里我有一个疑问，这里就算不把对象 7 变成灰色，貌似也不会影响什么，因为它的上游对象对象 4 依然是灰色而不是黑色，所以会继续扫描到对象 7，然后再继续扫描到对象 6，为什么会说对象 6 被保护呢？\n然后我想了一下，这样做是为了防止这种情况的发生：在对象 4 准备扫描对象 7之前，用户程序断开了对象 4 到对象 7 的引用，导致此时的对象 7 依然是白色，如果这时黑色对象 10 再引用白色对象 7，为了保证强三色不变性，就需要将对象 7 变为灰色。\n参考（抄袭） Go语言GC实现原理及源码分析 https://www.luozhiyun.com/archives/475\nGolang垃圾回收(GC)介绍 https://liangyaopei.github.io/2021/01/02/golang-gc-intro/\nhttps://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/\nhttps://www.bilibili.com/video/BV1n5411H7qS?spm_id_from=333.999.0.0\n","date":"2022年06月01日","permalink":"/posts/go-gc/","summary":"主流的垃圾回收算法 引用计数法 根据对象自身的引用计数来回收，当引用计数归零时进行回收，但是计数频繁更新会带来更多开销，且无法解决循环引用的问题。\n优点：简单直接，回收速度快 缺点：需要额外的空间存放计数，无法处理循环引用的情况； 标记清除法 标记出所有不需要回收的对象，在标记完成后统一回收掉所有未被标记的对象。","title":"go gc 白话版"},{"contents":"简介 Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers 和 deployments 等都是属于某一个 namespace 的（默认是default），而 node, persistentVolumes 等则不属于任何 namespace。\nNamespace 常用来隔离不同的用户，比如 Kubernetes 自带的服务一般运行在 kube-system namespace 中。\nNamespace 适用于存在很多跨多个团队或项目的用户的场景。例如，如果使用命名空间来划分应用程序生命周期环境（如开发、测试、生产），则可以在每个环境中维护利用同样的名称维护相同对象的副本。\n同一 namespace 内的资源名称要唯一，但跨 namespace 时没有这个要求，通俗的说，就是两个资源如果名称相同，但是所在的命名空间不同是可以的。\n疑问：\nlabel（标签）也可以对资源进行分类，它和 namespace 有什么区别？\n由于每个对象都可以有多个标签，因此这些对象组可以重叠。另外，当在集群中工作(例如通过 kubectl )时，如果没有明确指定标签选择器，我们总能看到所有对象。而 namespace 可以将资源分割为完全独立不重叠的组，我个人的理解是，namespace 提供了比 label 更严格的资源分类，其主要针对的是不同用户间的划分，而 label 则倾向于诸如对同一软件不同版本这种情况进行划分。\n使用 查看所有的 namespace $ kubectl get ns NAME STATUS AGE default Active 24d kube-system Active 24d kube-public Active 24d kube-node-lease Active 24d Kubernetes 会创建四个初始 namespace：\ndefault 如果资源没有明确指定其所属的 namespace，则会默认使用该 namespace kube-system Kubernetes 系统创建对象所使用的 namespace kube-public 这个 namespace 是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。 这个 namespace 主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。 这个 namespace 的公共方面只是一种约定，而不是要求。 kube-node-lease 此 namespace 用于与各个节点相关的 租约（Lease）对象。 节点租期允许 kubelet 发送心跳，由此控制面能够检测到节点故障。 指定 namespace 可以将 namespace 作为筛选/属性，比如下面的查询 pod 操作使用 namespace 作为筛选条件\n$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE helm-install-traefik-crd-q62cm 0/1 Completed 0 24d helm-install-traefik-l2wcr 0/1 Completed 1 24d svclb-traefik-f4w7g 2/2 Running 12 (50m ago) 24d traefik-df4ff85d6-2rbl2 1/1 Running 15 (50m ago) 24d coredns-d76bd69b-7bkb4 1/1 Running 18 (50m ago) 24d local-path-provisioner-6c79684f77-qjq26 1/1 Running 7 (50m ago) 24d metrics-server-7cd5fcb6b7-b72lz 1/1 Running 25 (50m ago) 24d 也可以使用 \u0026ndash;namespace 命令，如果不指定则默认为 default\n也可以在创建资源的时候为其指定 namespace：\n$ kubectl create -f kubia-manual.yam1 -n custom-namespace pod ”kubia-manual\u0026quot; created 创建 namespace $ kubectl create ns ns-test namespace/ns-test created 删除 namespace $ kubectl delete ns ns-test namespace \u0026quot;ns-test\u0026quot; deleted Warning: 这会删除名字空间下的 所有内容 ！\nnamespace 与 DNS 当你创建服务时，Kubernetes 会创建相应的 DNS 条目。 此条目的格式为 \u0026lt;服务名称\u0026gt;.\u0026lt;名字空间名称\u0026gt;.svc.cluster.local。 这意味着如果容器使用 \u0026lt;服务名称\u0026gt;，它将解析为名字空间本地的服务。 这对于在多个名字空间（如开发、暂存和生产）中使用相同的配置非常有用。 如果要跨名字空间访问，则需要使用完全限定的域名（FQDN）。\n","date":"2022年05月31日","permalink":"/posts/k8s-namespace/","summary":"简介 Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers 和 deployments 等都是属于某一个 namespace 的（默认是default），而 node, persistentVolumes 等则不属于任何 namespace。","title":"k8s namespace"},{"contents":"简介 errgroup 是 WaitGroup 的强化版，其在 WaitGroup 的基础上添加了错误处理的功能：如果一组 goroutine 中的某一个发生了错误，那么后续的所有 gorouine 都不会被执行。\n示例 示例 1 一共开启 10 个 goroutine 执行任务，其中第三个 goroutine 会出错，使用 errgroup 可以保证第三个之后的所有 goroutine 都不会执行\nfunc main() { group, ctx := errgroup.WithContext(context.Background()) count := 50 for i := 0; i \u0026lt; count; i++ { i := i // 闭包重新捕获变量 i group.Go(func() error { select { case \u0026lt;-ctx.Done(): log.Printf(\u0026quot;[%d] group 里有一个任务失败了，所以这个任务不会执行\\n\u0026quot;, i) return nil default: } if i == 3 { log.Printf(\u0026quot;[%d] error\\n\u0026quot;, i) return fmt.Errorf(\u0026quot;[%d] error\u0026quot;, i) } log.Printf(\u0026quot;[%d] success\\n\u0026quot;, i) return nil }) time.Sleep(time.Millisecond * 10) } if err := group.Wait(); err != nil { log.Println(err) } } // Output: // 2022/05/29 00:02:51 [0] success // 2022/05/29 00:02:51 [1] success // 2022/05/29 00:02:51 [2] success // 2022/05/29 00:02:51 [3] error // 2022/05/29 00:02:51 [4] group 里有一个任务失败了，所以这个任务不会执行 // 2022/05/29 00:02:51 [5] group 里有一个任务失败了，所以这个任务不会执行 // 2022/05/29 00:02:51 [6] group 里有一个任务失败了，所以这个任务不会执行 // 2022/05/29 00:02:51 [7] group 里有一个任务失败了，所以这个任务不会执行 // 2022/05/29 00:02:51 [8] group 里有一个任务失败了，所以这个任务不会执行 // 2022/05/29 00:02:51 [9] group 里有一个任务失败了，所以这个任务不会执行 //2022/05/29 00:02:51 [3] error 示例 2 来源：https://lailin.xyz/post/go-training-week3-errgroup.html\n因为我是跳过文章前面的内容直接看示例的，所以一开始有点看不懂注释写的意思，后来翻了一下源码才明白，对注释针对自己的简介进行了一些修改。\n这个示例的大致意思是：使用 errgroup 开启了 3 个 goroutine，第一个运行 HTTP 服务，第二个监听 shutdown 接口用来关闭 HTTP 服务，第三个监听 ctrl+c 和 kill 这两个信号。这三个中的任何一个发生错误退出，都会让另外两个 goroutine 也退出（准确的说，运行 HTTP 服务的 goroutine 是不会主动退出的，只有第二个能将其结束，另外两个都可以主动结束，第二个执行 curl localhost:8080/shutdown 可以触发，第三个使用 ctrl+c 或者 kill 都可以触发）\nfunc main() { // WithContext() 里面用 context.Cancel 包装了传入的 context，并返回包装后 // 的 ctx，而另一个返回值 cancel 则是放到了 errgroup.Group 里面 // 如果 g.Go(func() error) 里面的 func 返回了错误，那么会调用 Group.cancel // 将包装的 ctx 取消 g, ctx := errgroup.WithContext(context.Background()) mux := http.NewServeMux() mux.HandleFunc(\u0026quot;/ping\u0026quot;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026quot;pong\u0026quot;)) }) // 模拟单个服务错误退出 serverOut := make(chan struct{}) mux.HandleFunc(\u0026quot;/shutdown\u0026quot;, func(w http.ResponseWriter, r *http.Request) { serverOut \u0026lt;- struct{}{} }) server := http.Server{ Handler: mux, Addr: \u0026quot;:8080\u0026quot;, } // g1 // g1 退出了所有的协程都能退出么？ // g1 退出后, context 将不再阻塞（这话第一眼没看懂，context 还会被阻塞？），g2, g3 都会随之退出 // 修改版注释：g1 只有在 g2 调用 Shutdown() 后才会退出，同时返回一个 error // 之后 ctx 被 cancel，其他 goroutine 的 select 都会走 case \u0026lt;-ctx.Done() 分支， // 从而达到退出的目的（因为 select 没有加 default 分支，所以会被阻塞，直到有一个 case // 满足条件，所以上面说的 context 将不再阻塞，实际应该是 select 不再阻塞） // 然后 main 函数中的 g.Wait() 退出，所有协程都会退出 g.Go(func() error { // g2 的 Shutdown 会让这里停止阻塞并返回一个 err // 之后 ctx 会被 cancel 掉，g3 也会停止 if err := server.ListenAndServe(); err != nil { log.Println(\u0026quot;g1 error: \u0026quot;, err) return err } return nil }) // g2 // g2 退出了所有的协程都能退出么？ // g2 退出时，调用了 shutdown，g1 会退出 // g2 退出后, context 将不再阻塞，g3 会随之退出 // 修改：g2 退出后, g1 cancel 掉了 ctx，g3 的 select 将不再阻塞，g3 会随之退出 // 然后 main 函数中的 g.Wait() 退出，所有协程都会退出 g.Go(func() error { select { case \u0026lt;-ctx.Done(): log.Println(\u0026quot;errgroup exit...\u0026quot;) case \u0026lt;-serverOut: // curl localhost:8080/shutdown log.Println(\u0026quot;server will out...\u0026quot;) } cancelCtx, cancel := context.WithCancel(context.Background()) // 这里不是必须的，但是如果使用 _ 的话静态扫描工具会报错，加上也无伤大雅 defer cancel() log.Println(\u0026quot;shutting down server...\u0026quot;) // Shutdown() 会停止 g1，同时 g1 的 ListenAndServe() 会返回一个 error if err := server.Shutdown(cancelCtx); err != nil { log.Println(\u0026quot;g2 error: \u0026quot;, err) return err } return nil }) // g3 // g3 捕获到 os 退出信号将会退出 // g3 退出了所有的协程都能退出么？ // g3 退出后, context 将不再阻塞，g2 会随之退出 // 修改：g3 收到信号返回一个 error 并退出，ctx 会被 cancel 掉，然后 g2 的 select // 停止阻塞，执行下面的流程 // g2 退出时，调用了 shutdown，g1 会退出 // 然后 main 函数中的 g.Wait() 退出，所有协程都会退出 g.Go(func() error { quit := make(chan os.Signal, 0) signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM) select { case \u0026lt;-ctx.Done(): log.Println(\u0026quot;g3: ctx is done\u0026quot;) return ctx.Err() case sig := \u0026lt;-quit: return errors.Errorf(\u0026quot;get os signal: %v\u0026quot;, sig) } }) fmt.Printf(\u0026quot;errgroup exiting: %+v\\n\u0026quot;, g.Wait()) } 示例 3 errgroup 还可以限制同时开启的 goroutine 数量，其实本质也是使用 WaitGroup + channel 来实现的。\n下面的代码保证最多只开启 2 个 goroutine，其他的都会阻塞等待，值得一提的是，Group 即便是零值也是可以直接使用的，只是没有了 cancel 的能力。\nfunc main() { eg := errgroup.Group{} count := 20 mostRunning := 2 eg.SetLimit(mostRunning) for i := 0; i \u0026lt; count; i++ { i := i eg.Go(func() error { log.Println(i) time.Sleep(300 * time.Millisecond) return nil }) } eg.Wait() } 源码分析 Group // A Group is a collection of goroutines working on subtasks that are part of // the same overall task. // // A zero Group is valid and does not cancel on error. // // Group 是一组 goroutines，它们处理属于同一整体任务的子任务。零值是有效的，并且不会因错误而取消。 // 因为零值的 cancel 是 nil，所以自然不会被执行 type Group struct { cancel func()\t// 发生错误时会调用该函数，将 WithContext 创建的 ctx 取消 wg sync.WaitGroup sem chan token\t// 带缓存的 chan，用来限制可以创建的 goroutine 数量，token 是 struct{} 的别名 errOnce sync.Once\t// 确保只有第一次发生错误时才执行某些操作 err error\t// 记录第一次发生的错误 } WithContext // WithContext returns a new Group and an associated Context derived from ctx. // // The derived Context is canceled the first time a function passed to Go // returns a non-nil error or the first time Wait returns, whichever occurs // first. // // WithContext 返回一个 Group 和一个从 ctx 派生的关联上下文。派生的 Context 在 Go 函数第一次返 // 回错误或 Wait 第一次返回时被取消，以先发生者为准。 func WithContext(ctx context.Context) (*Group, context.Context) { // 用 WithCancel 包装了传入的 ctx ctx, cancel := context.WithCancel(ctx) // 将 cancel 保存到 Cancel 属性 return \u0026amp;Group{cancel: cancel}, ctx } Go // Go calls the given function in a new goroutine. // It blocks until the new goroutine can be added without the number of // active goroutines in the group exceeding the configured limit. // // The first call to return a non-nil error cancels the group; its error will be // returned by Wait. // // Go 在一个新的 goroutine 中调用给定的函数。如果当前运行的 goroutine 数量达到了 limit 的上限，则会阻 // 塞，直到可以添加。如果调用 f 返回了错误，则会取消该组（也就是调用 Group.cancel，只有第一次发生错误时 // 会执行该操作），Wait 会返回它的错误。 func (g *Group) Go(f func() error) { if g.sem != nil { g.sem \u0026lt;- token{}\t// 如果当前开启的 goroutine 数量大于 limit 则会被阻塞 } g.wg.Add(1) go func() { defer g.done() if err := f(); err != nil { // 只执行一次，保证只有第一次发生错误时执行 g.errOnce.Do(func() { g.err = err\t// 保存错误，Wait() 会返回这个错误 if g.cancel != nil { g.cancel()\t// cancel 掉 WithContext() 创建的 ctx } }) } }() } SetLimit // SetLimit limits the number of active goroutines in this group to at most n. // A negative value indicates no limit. // // Any subsequent call to the Go method will block until it can add an active // goroutine without exceeding the configured limit. // // The limit must not be modified while any goroutines in the group are active. // // SetLimit 将这个组中的活动 goroutine 的数量限制为最多 n。负值表示没有限制。 // 对 Go 方法的任何后续调用都将阻塞，直到它可以添加一个活动的 goroutine 而不会 // 超过配置的限制。当组中的任何 goroutine 处于活动状态时，不得修改限制。 func (g *Group) SetLimit(n int) { if n \u0026lt; 0 { g.sem = nil return } if len(g.sem) != 0 { panic(fmt.Errorf(\u0026quot;errgroup: modify limit while %v goroutines in the group are still active\u0026quot;, len(g.sem))) } g.sem = make(chan token, n)\t// 使用带缓存的 chan } TryGo // TryGo calls the given function in a new goroutine only if the number of // active goroutines in the group is currently below the configured limit. // // The return value reports whether the goroutine was started. // // 仅当组中当前活动的 goroutine 的数量低于配置的限制时，TryGo 才会在新的 goroutine 中调用给定的函数。 // 返回值报告 goroutine 是否已启动。 func (g *Group) TryGo(f func() error) bool { if g.sem != nil { select { case g.sem \u0026lt;- token{}: // Note: this allows barging iff channels in general allow barging. default: return false } } g.wg.Add(1) go func() { defer g.done() if err := f(); err != nil { g.errOnce.Do(func() { g.err = err if g.cancel != nil { g.cancel() } }) } }() return true } Wait // Wait blocks until all function calls from the Go method have returned, then // returns the first non-nil error (if any) from them. // // 阻塞等待，直到来自 Go 方法的所有函数调用都返回，然后从它们返回第一个非零错误（如果有）。 func (g *Group) Wait() error { g.wg.Wait() if g.cancel != nil { g.cancel() } return g.err } 总的来看，源码还是比较简单的，也比较容易阅读。\n","date":"2022年05月28日","permalink":"/posts/errgroup/","summary":"简介 errgroup 是 WaitGroup 的强化版，其在 WaitGroup 的基础上添加了错误处理的功能：如果一组 goroutine 中的某一个发生了错误，那么后续的所有 gorouine 都不会被执行。","title":"Go errgroup 使用"},{"contents":"一个 echo server 示例：\npackage main import ( \u0026quot;log\u0026quot; . \u0026quot;syscall\u0026quot; ) const eventNum = 10 func init() { log.SetFlags(log.Lshortfile | log.Ldate) } func main() { sfd, err := Socket(AF_INET, SOCK_STREAM, 0) if err != nil { panic(err) } //log.Printf(\u0026quot;listen fd: %v\\n\u0026quot;, sfd) if err := Bind(sfd, \u0026amp;SockaddrInet4{Port: 8080, Addr: [4]byte{127, 0, 0, 1}}); err != nil { panic(err) } if err := Listen(sfd, 1024); err != nil { panic(err) } kfd, err := Kqueue() if err != nil { panic(err) } change := Kevent_t{ Ident: uint64(sfd), Filter: EVFILT_READ, Flags: EV_ADD | EV_ENABLE, Fflags: 0, Data: 0, Udata: nil, } _, err = Kevent(kfd, []Kevent_t{change}, nil, nil)\t// 添加监听事件，第三个参数传空 if err != nil { panic(err) } for { events := make([]Kevent_t, eventNum) // 获取就绪事件，第二个参数传空 readyEventNum, err := Kevent(kfd, nil, events, nil) // 无限等待直到有事件产生 //fmt.Printf(\u0026quot;readyEventNum: %v\\n\u0026quot;, readyEventNum) if err != nil \u0026amp;\u0026amp; err != EINTR { panic(err) } for i := 0; i \u0026lt; readyEventNum; i++ { event := events[i] log.Printf(\u0026quot;event fd: %v \\n\u0026quot;, event.Ident) efd := int(event.Ident) if efd == sfd { nfd, _, err := Accept(sfd) if err != nil { log.Println(err) continue } log.Printf(\u0026quot;accept event, fd: %v\\n\u0026quot;, nfd) if err := SetNonblock(nfd, true); err != nil { log.Println(err) continue //panic(err) } change := Kevent_t{ Ident: uint64(nfd), Filter: EVFILT_READ, Flags: EV_ADD | EV_ENABLE, Fflags: 0, Data: 0, Udata: nil, } // 添加监听事件，第三个参数传空 _, err = Kevent(kfd, []Kevent_t{change}, nil, nil) if err != nil { panic(err) } } else { log.Printf(\u0026quot;read event\\n\u0026quot;) b := make([]byte, 1024) _, err := Read(efd, b) if err != nil { log.Println(err) Close(efd) continue } _, err = Write(efd, b) if err != nil { log.Println(err) Close(efd) continue } } } } } 踩坑记录 错误版本：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; . \u0026quot;syscall\u0026quot; ) const eventNum = 10 func init() { log.SetFlags(log.Lshortfile | log.Ldate) } // BUG 记录： // 1. 客户端断开连接后，服务端仍然会产生读事件 // 客户端断开连接后，服务端的 Read 会发生 connection reset by peer 错误并进入错误处理分支， Close 掉这 // 个连接，但是因为错误处理采用的是 continue 而不是 break（因为可能会同时产生多个事件，也就是 kevent 的返 // 回值，如果使用 break，会导致后面的事件全部被放弃处理，因为多个事件中的某一个事件产生错误，而直接跳过后续事 // 件的处理，显然是不合理的），会重新进入外层的死循环，又因为会继续产生读事件，导致 Kevent 函数成功返回，进 // 入到读事件分支，进行 Read 操作，此时会报错 bad file descriptor，continue 到最外层死循环，如此反复， // 直到循环很多次以后会抛出一个 panic func main() { sfd, err := Socket(AF_INET, SOCK_STREAM, 0) if err != nil { panic(err) } log.Printf(\u0026quot;listen fd: %v\\n\u0026quot;, sfd) if err := Bind(sfd, \u0026amp;SockaddrInet4{Port: 8080, Addr: [4]byte{127, 0, 0, 1}}); err != nil { panic(err) } if err := Listen(sfd, 1024); err != nil { panic(err) } var ( changes = make([]Kevent_t, 0) // 监听列表 events = make([]Kevent_t, eventNum) // 发生的事件列表，len 不能为 0 ) kfd, err := Kqueue() if err != nil { panic(err) } readEvent := Kevent_t{ Ident: uint64(sfd), Filter: EVFILT_READ, Flags: EV_ADD | EV_ENABLE, Fflags: 0, Data: 0, Udata: nil, } changes = append(changes, readEvent) // BUG1 解决步骤1 ：新增下面这个函数调用 // _, err = Kevent(kfd, changes, nil, nil) // if err != nil { // panic(err) // } for { // BUG1 解决步骤2： // 这里的第二个参数必须传 nil，如果传 changes 就会出现 BUG1 的情况 readyEventNum, err := Kevent(kfd, changes, events, nil) // 无限等待直到有事件产生 fmt.Printf(\u0026quot;readyEventNum: %v\\n\u0026quot;, readyEventNum) if err != nil \u0026amp;\u0026amp; err != EINTR { panic(err) } log.Printf(\u0026quot;events: %v \\n\u0026quot;, events) log.Printf(\u0026quot;changes: %v \\n\u0026quot;, changes) log.Printf(\u0026quot;readyEventNum: %v\\n\u0026quot;, readyEventNum) for i := 0; i \u0026lt; readyEventNum; i++ { event := events[i] log.Printf(\u0026quot;event fd: %v \\n\u0026quot;, event.Ident) efd := int(event.Ident) if efd == sfd { nfd, _, err := Accept(sfd) if err != nil { log.Println(err) continue } log.Printf(\u0026quot;accept event, fd: %v\\n\u0026quot;, nfd) if err := SetNonblock(nfd, true); err != nil { log.Println(err) continue //panic(err) } changes = append(changes, Kevent_t{ Ident: uint64(nfd), Filter: EVFILT_READ, Flags: EV_ADD | EV_ENABLE, Fflags: 0, Data: 0, Udata: nil, }) // BUG1 解决步骤3 ：新增下面这个函数调用 // _, err = Kevent(kfd, changes, nil, nil) // if err != nil { // panic(err) // } } else /*if event.Filter == EVFILT_READ*/ { log.Printf(\u0026quot;read event\\n\u0026quot;) b := make([]byte, 1024) _, err := Read(efd, b) if err != nil { log.Println(err) Close(efd) continue } _, err = Write(efd, b) if err != nil { log.Println(err) Close(efd) continue } } } } } 正确版本：\n按照注释上的标注进行修改：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; . \u0026quot;syscall\u0026quot; ) const eventNum = 10 func init() { log.SetFlags(log.Lshortfile | log.Ldate) } func main() { sfd, err := Socket(AF_INET, SOCK_STREAM, 0) if err != nil { panic(err) } log.Printf(\u0026quot;listen fd: %v\\n\u0026quot;, sfd) if err := Bind(sfd, \u0026amp;SockaddrInet4{Port: 8080, Addr: [4]byte{127, 0, 0, 1}}); err != nil { panic(err) } if err := Listen(sfd, 1024); err != nil { panic(err) } var ( changes = make([]Kevent_t, 0) // 监听列表 events = make([]Kevent_t, eventNum) // 发生的事件列表，len 不能为 0 ) kfd, err := Kqueue() if err != nil { panic(err) } readEvent := Kevent_t{ Ident: uint64(sfd), Filter: EVFILT_READ, Flags: EV_ADD | EV_ENABLE, Fflags: 0, Data: 0, Udata: nil, } changes = append(changes, readEvent) // BUG1 解决步骤1 ：新增下面这个函数调用 _, err = Kevent(kfd, changes, nil, nil) if err != nil { panic(err) } for { // BUG1 解决步骤2： // 这里的第二个参数必须传 nil，如果传 changes 就会出现 BUG1 的情况 readyEventNum, err := Kevent(kfd, nil, events, nil) // 无限等待直到有事件产生 //fmt.Printf(\u0026quot;readyEventNum: %v\\n\u0026quot;, readyEventNum) if err != nil \u0026amp;\u0026amp; err != EINTR { panic(err) } //log.Printf(\u0026quot;events: %v \\n\u0026quot;, events) //log.Printf(\u0026quot;changes: %v \\n\u0026quot;, changes) //log.Printf(\u0026quot;readyEventNum: %v\\n\u0026quot;, readyEventNum) for i := 0; i \u0026lt; readyEventNum; i++ { event := events[i] //log.Printf(\u0026quot;event fd: %v \\n\u0026quot;, event.Ident) efd := int(event.Ident) if efd == sfd { nfd, _, err := Accept(sfd) if err != nil { log.Println(err) continue } log.Printf(\u0026quot;accept event, fd: %v\\n\u0026quot;, nfd) if err := SetNonblock(nfd, true); err != nil { log.Println(err) continue //panic(err) } changes = append(changes, Kevent_t{ Ident: uint64(nfd), Filter: EVFILT_READ, Flags: EV_ADD | EV_ENABLE, Fflags: 0, Data: 0, Udata: nil, }) // BUG1 解决步骤3 ：新增下面这个函数调用 _, err = Kevent(kfd, changes, nil, nil) if err != nil { panic(err) } } else /*if event.Filter == EVFILT_READ*/ { log.Printf(\u0026quot;read event\\n\u0026quot;) b := make([]byte, 1024) _, err := Read(efd, b) if err != nil { log.Println(err) Close(efd) continue } _, err = Write(efd, b) if err != nil { log.Println(err) Close(efd) continue } } } } } 原因：\n没研究出来\n暂时见解：\n貌似 kqueue 这个函数是 epoll 的 epoll_ctl 和 epoll_wait 的结合体，主要是它的第二个和第三个参数（Go 的系统调用需要的参数和原生 API 略有不同，但影响不大，只是不用提供两个数组的长度而已），第二个参数是要监听的事件集合，第三个参数用来存放已经准备就绪的事件，所以如果想要添加监听事件，就将第三个参数传 null；如果想获取已经就绪的事件，那么就给第二个参数传 null，具体看开头的 echo server 代码的那几行调用 Kqueue 的注释。看了一些正确的例子，包括官方的 FreeBSD man 文档，基本都是这么写的。\n对比 epoll\npackage main import ( \u0026quot;log\u0026quot; . \u0026quot;syscall\u0026quot; ) func init() { log.SetFlags(log.Lshortfile | log.Ltime) } func main() { sfd, err := Socket(AF_INET, SOCK_STREAM, 0) if err != nil { panic(err) } log.Printf(\u0026quot;listen fd: %v\\n\u0026quot;, sfd) if err := Bind(sfd, \u0026amp;SockaddrInet4{Port: 8080, Addr: [4]byte{127, 0, 0, 1}}); err != nil { panic(err) } if err := Listen(sfd, 1024); err != nil { panic(err) } epfd, err := EpollCreate(10) if err != nil { panic(err) } if err := EpollCtl(epfd, EPOLL_CTL_ADD, sfd, \u0026amp;EpollEvent{ Events: EPOLLIN, Fd: int32(sfd), Pad: 0, }); err != nil { panic(err) } for { events := make([]EpollEvent, 20) n, err := EpollWait(epfd, events, -1) if err != nil { log.Println(err) continue } for i := 0; i \u0026lt; n; i++ { event := events[i] if event.Fd == int32(sfd) { nfd, _, err := Accept(sfd) if err != nil { log.Println(err) continue } if err := EpollCtl(epfd, EPOLL_CTL_ADD, nfd, \u0026amp;EpollEvent{ Events: EPOLLIN, Fd: int32(nfd), Pad: 0, }); err != nil { log.Println(err) continue } } else { b := make([]byte, 1024) _, err := Read(int(event.Fd), b) if err != nil { log.Println(err) Close(int(event.Fd)) continue } _, err = Write(int(event.Fd), b) if err != nil { log.Println(err) Close(int(event.Fd)) continue } } } } } ","date":"2022年05月28日","permalink":"/posts/kqueue/","summary":"一个 echo server 示例：\npackage main import ( \u0026quot;log\u0026quot; .","title":"kqueue"},{"contents":"为什么需要卷 pod 类似逻辑主机，在逻辑主机中运行的进程共享诸如 CPU、RAM、网络接口等资源。有时候人们会期望进程也能共享磁盘，但是因为 pod 中运行的是一个个容器，而每个容器都有自己独立的文件系统（因为文件系统来自于容器镜像），所以不能共享磁盘。有时候可能存在这样一种需求：pod 中某个容器存储的数据需要持久化，但是因为容器存在挂掉的可能，如果挂掉后会新创建一个 pod 来顶替，但是因为 pod 因为无法共享磁盘，导致无法继承先前 pod 里存储的数据，先前的那部分数据就永久丢失了。\nk8s 通过定义 卷 这个概念来解决这个问题，它们不像 pod 这样的顶级资源，而是被定义为了 pod 的一部分，并和 pod 共享相同的生命周期，也就是说，卷在 pod 启动时创建，在 pod 删除时销毁（书上是这么说的，但是感觉不太准确，不同类型的卷有不同的生命周期，像 emptyDir 就是和 pod 共享生命周期，而 hostPath 则不共享，就算 pod 删除了，这个卷也不会被删除）。\n卷类型 emptyDir 顾名思义，这个卷最初是空的，当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。运行在 pod 内的应用程序可以写入它需要的任何文件。\n实践：\nyaml 如下：（ ⚠️ 该 yaml 的镜像基于 arm64 架构，如果你的机器是 x86，需要将第一个 -image 更换为 luksa/fortune）\napiVersion: v1 kind: Pod metadata: name: fortune spec: containers: - image: stdoutt/fortune-arm64 name: html-generator volumeMounts: - name: html # 使用 name 为 html 的卷（该卷在 yaml 最后定义） mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} 这个 pod 的大致意思是：创建了 2 个容器，一个负责生成随机内容（使用 fortune 随机生成一句谚语）并写入到 /var/htdocs/index.html 文件里，另一个是 nginx 服务，负责将 /usr/share/nginx/html/index.html 里的内容显示出来（将第一个容器写入的谚语读取出来），所以这两个容器需要结合使用，一个负责写，另一个负责读，所以需要共用同一个卷来实现。\n为了实现上面的流程，需要创建一个卷，将这个卷分别挂载到两个容器的某个目录下，这样就可以达到共享一个卷的目的，在上面的 yaml 中，将卷挂载到了 fortune 容器的 /var/htdocs/ 目录下，然后这个容器会在这个目录下创建一个 index.html 并写入谚语（因为挂载的原因，所以相当在卷中创建了一个 index.html 文件）；同样还会将这个卷挂载到 nginx 容器下的 /usr/share/nginx/html 目录下，nginx 默认会读取这个目录下的 index.html 作为显示内容，前面提到过，fortune 容器会创建一个 index.html 文件到卷中，所以这个文件刚好就可以作为 nginx 的默认显示内容，值得注意的是，yaml 里还对 nginx 容器设置了 readOnly: true 属性，表示该容器只能从卷中读取，不能写入。\n以上涉及到 linux 挂载（mount）的知识，这个概念感觉有点绕，（我总是把挂载、被挂载搞混），我也是初学掌握的不是很好，所以上面的内容说的比较绕，也可能不正确，用我个人的理解，简单的用几句话概括就是，将卷挂载到了一个容器的 /var/htdocs 下和另一个容器的 /usr/share/nginx/html 容器下，然后这两个路径就相当于共享了，第一个容器在 /var/htdocs 中创建一个文件，另一个容器可以同样在 /usr/share/nginx/html 目录下读取到。\n因为容器的文件系统是隔离的，正常来说两个容器是不能共享目录的，就相当于两台电脑，如果不通过一些特殊手段肯定是不能共享目录的，所以为了解决这个问题，就需要使用上面的 卷 来实现。\n接下来就可以使用 kubectl apply -f 进行创建了，为了测试效果，可以使用端口转发：\n$ kubectl port-forward fortune 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 然后使用 curl 就可以了：\n$ curl http://localhost:8080 Q:\tHow many psychiatrists does it take to change a light bulb? A:\tOnly one, but it takes a long time, and the light bulb has to really want to change. ❓这个 emptyDir 实际路径在哪里？\nemptyDir 不仅可以使用常规的硬盘作为存储介质，也可以使用内存作为存储介质（linux 中的 tmpfs 也是类似的东西），只需要在 yaml 中指定 mediun 即可：\nvolumes: - name: html emptyDir: medium: Memory # 存储在内存中 hostPath 这里复用 emptyDir 部分的 yaml，只是将 volume 的类型更换为了 hostPath：\napiVersion: v1 kind: Pod metadata: name: fortune spec: containers: - image: stdoutt/fortune-arm64 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html hostPath: path: /data type: Directory 执行上面的 yaml，创建 pod，然后看看 hostPath 卷里的内容：\n$ cat /data/index.html Stay away from hurricanes for a while. $ cat /data/index.html A vivid and creative mind characterizes you. 也可以像 emptyDir 中演示的那样，开启端口转发，使用 curl 看看效果，这里就不记录了。\n测试一下 hostPath 类型的卷是否和 pod 共享生命周期：\n$ kubectl delete po fortune pod \u0026quot;fortune\u0026quot; deleted $ cat /data/index.html Today's weirdness is tomorrow's reason why. -- Hunter S. Thompson pod 被删除了，但是 /data 下的内容依然存在，所以 hostPath 卷不与 pod 共享生命周期，属于持久卷。\n持久卷 持久卷（pv）和持久卷声明（pvc） 一般来说，开发者不需要知道持久卷存储的类型和地址，如果开发者定义 pod 时需要手动指定存储地址，那么会导致高耦合，解决方法就是计算机的通用解决方法：套一层中间层，pvc 就是这个中间层，开发者只需要将自己的需求（容量需求、访问模式）定义为 pvc 提交给 k8s，k8s 将找到可匹配的持久卷并将其绑定到 pvc。\nPVC 与 PV 是一一对应关系，不能一个 PVC 挂载多个 PV，也不能一个 PV 挂载多个 PVC。为应用配置存储时，需要声明一个存储需求声明（PVC），而Kubernetes会通过最佳匹配的方式选择一个满足PVC需求的PV，并与之绑定。所以从职责上PVC是应用所需要的存储对象，属于应用作用域。PV是存储平面的存储对象，属于整个存储域。\nPVC只有绑定了PV之后才能被Pod使用，而PVC绑定PV的过程即是消费PV的过程，这个过程是有一定规则的，以下规则都满足的PV才能被PVC绑定：\nVolumeMode：被消费PV的VolumeMode需要和PVC一致。 AccessMode：被消费PV的AccessMode需要和PVC一致。 StorageClassName：如果PVC定义了此参数，PV必须有相关的参数定义才能进行绑定。 LabelSelector：通过标签（labels）匹配的方式从PV列表中选择合适的PV绑定。 Size：被消费PV的capacity必须大于或者等于PVC的存储容量需求才能被绑定。 在 k3s 上实践 pv 和 pvc pvc.yaml\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: local-path-pvc namespace: default spec: accessModes: - ReadWriteOnce storageClassName: local-path resources: requests: storage: 50Mi 创建并查看：\n$ kubectl apply -f pvc.yaml persistentvolumeclaim/local-path-pvc created $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-path-pvc Pending local-path 102s $ kubectl get pv No resources found 发现 pvc 的状态是 pending，查看 describe：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal WaitForFirstConsumer 11s (x9 over 2m3s) persistentvolume-controller waiting for first consumer to be created before binding 意思是 pvc 创建好了，等待某个消费者来绑定它，所以处于 pending 状态，接下来创建一个 pod 作为消费者，来绑定这个 pvc：\npvc_consumer_pod.yaml\napiVersion: v1 kind: Pod metadata: name: volume-test namespace: default spec: containers: - name: volume-test image: nginx:stable-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: volv mountPath: /data ports: - containerPort: 80 volumes: - name: volv persistentVolumeClaim: claimName: local-path-pvc 运行：\n$ kubectl apply -f pvc_consumer_pod.yaml pod/volume-test created 此时再查看 pvc 状态：\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-path-pvc Bound pvc-0e7db0b6-fa32-4c2e-ac0c-53a354ead8a4 50Mi RWO local-path 5m41s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-0e7db0b6-fa32-4c2e-ac0c-53a354ead8a4 50Mi RWO Delete Bound default/local-path-pvc local-path 41m 发现 pvc 已经处于 Bound 状态了，此外 pv 也已经创建好了\n接下来就可以测试一下效果了，首先需要先进入容器：\n$ k3s kubectl get po NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 9h volume-test 1/1 Running 0 5m49s $ kubectl exec -it volume-test -- /bin/sh / # ls bin docker-entrypoint.sh media root sys data etc mnt run tmp dev home opt sbin usr docker-entrypoint.d lib proc srv var 因为 pvc_consumer_pod.yaml 中，我们定义的挂载目录的 /data，所以我们要 cd 到容器的 /data 目录，然后向这个目录中写入一些内容，再看看其 pvc 绑定的 pv 是否会同步拥有该内容\n# 此时接着上面的 exec，也就是在容器内部执行这些命令 / # cd data /data # echo \u0026quot;hello, local PV\u0026quot; \u0026gt; pvc-test /data # cat pvc-test hello, local PV 在 local PV 查看是否同样有此文件，在 k3s 中，pv 存储在 /var/lib/rancher/k3s/storage 目录下，k3s 的 pv 相关内容可以 参考这篇\n# 退出容器内部，回到主机 $ ls /var/lib/rancher/k3s/storage/ pvc-0e7db0b6-fa32-4c2e-ac0c-53a354ead8a4_default_local-path-pvc $ cd /var/lib/rancher/k3s/storage/pvc-0e7db0b6-fa32-4c2e-ac0c-53a354ead8a4_default_local-path-pvc $ ls pvc-test # 在容器内部写入的内容，同步到了这里 $ cat pvc-test hello, local PV 结果说明测试成功\n如果两个 pod 绑定到同一个 pvc 上会怎样？ 继续沿用之前的 pvc_consumer_pod.yaml，做一些修改：\n$ cp pvc_consumer_pod.yaml pvc_consumer_pod1.yaml 修改后：\napiVersion: v1 kind: Pod metadata: name: volume-test1 # 此处修改 namespace: default spec: containers: - name: volume-test1 # 此处修改 image: nginx:stable-alpine imagePullPolicy: IfNotPresent volumeMounts: - name: volv mountPath: /data ports: - containerPort: 80 volumes: - name: volv persistentVolumeClaim: claimName: local-path-pvc 创建：\n$ kubectl apply -f pvc_consumer_pod1.yaml pod/volume-test1 created $ kubectl get po NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 10h volume-test 1/1 Running 0 25m volume-test1 1/1 Running 0 32s 发现创建成功了\n还是按照之前的流程，先 exec 进容器看看：\n$ kubectl exec -it volume-test1 -- /bin/sh / # ls bin docker-entrypoint.sh media root sys data etc mnt run tmp dev home opt sbin usr docker-entrypoint.d lib proc srv var / # cd data/ /data # ls pvc-test /data # cat pvc-test hello, local PV 发现 /data 下已经有内容了，而且就是之前 volume-test 这个 pod 写入的内容，尝试向这个文件里新写入一部分内容：\n/data # echo \u0026quot;hello again\u0026quot; \u0026gt;\u0026gt; pvc-test /data # cat pvc-test hello, local PV hello again 查看 local PV：\n$ cat /var/lib/rancher/k3s/storage/pvc-0e7db0b6-fa32-4c2e-ac0c-53a354ead8a4_default_local-path-pvc/pvc-test hello, local PV hello again 查看第一个容器 volume-test :\n$ kubectl exec -it volume-test -- /bin/sh / # cat /data/pvc-test hello, local PV hello again 发现二者的内容都同步了\n如此看来，多个 pod 可以绑定同一个 pvc 来达到共享文件的效果，pv 和 pvc 是一对一绑定的，但是 pod 和 pvc 是可以多对一绑定的。\n删除 pvc ","date":"2022年05月20日","permalink":"/posts/k8s-juan/","summary":"为什么需要卷 pod 类似逻辑主机，在逻辑主机中运行的进程共享诸如 CPU、RAM、网络接口等资源。有时候人们会期望进程也能共享磁盘，但是因为 pod 中运行的是一个个容器，而每个容器都有自己独立的文件系统（因为文件系统来自于容器镜像），所以不能共享磁盘。有时候可能存在这样一种需求：pod 中某个容器存储的数据需要持久化，但是因为容器存在挂掉的可能，如果挂掉后会新创建一个 pod 来顶替，但是因为 pod 因为无法共享磁盘，导致无法继承先前 pod 里存储的数据，先前的那部分数据就永久丢失了。","title":"k8s 卷"},{"contents":" 来源：https://chinese.freecodecamp.org/news/how-make-a-windows-10-usb-using-your-mac-build-a-bootable-iso-from-your-macs-terminal/\n准备 windows iso 文件 一个大小至少 8G 的 U 盘 mac 上安装好 homebrew 步骤 1 查看 U 盘的名称：\n$ diskutil list 该命令会列出电脑上所有的磁盘设备，可以通过大小来确认哪个是你插入的 U 盘，比如我的是：\n/dev/disk8 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *31.0 GB disk8 1: Windows_NTFS WINDOWS10 31.0 GB disk8s1 那么 disk8 就是我的 U 盘的名称了。\n步骤 2 格式化 U 盘：\n$ diskutil eraseDisk FAT32 \u0026quot;WINDOWS10\u0026quot; MBR disk8 这里的 disk8 要替换成你的 u 盘名称，\u0026ldquo;WINDOWS10\u0026rdquo; 是格式化后对硬盘的重命名，可以指定为任意值。\nStarted erase on disk8 Unmounting disk Creating the partition map Waiting for partitions to activate Formatting disk8s1 as MS-DOS (FAT32) with name WINDOWS10 512 bytes per physical sector /dev/rdisk8s1: 60507232 sectors in 1890851 FAT32 clusters (16384 bytes/cluster) bps=512 spc=32 res=32 nft=2 mid=0xf8 spt=32 hds=255 hid=2048 drv=0x80 bsec=60536832 bspf=14773 rdcl=2 infs=1 bkbs=6 Mounting disk Finished erase on disk8 提示以上信息说明格式化成功。\n步骤 3 挂载 windows iso 文件：\n$ hdiutil mount /Volumes/c/Win10_2004_Chinese\\(Simplified\\)_x64.iso hdiutil: mount failed - 资源暂时不可用 mout 后面是你的 iso 所在路径\n也可以双击 iso 文件来进行挂载，因为我之前已经双击挂载过了，所以这里提示“资源暂时不可用”\n挂载完成后，finder 的位置会显示一个名为 CCCOMA_X64FRE_ZH-CN_DV9 的文件（也可能是 CCCOMA_X64FRE_EN-US_DV9 ，取决于 iso 文件），也可以在 /Volumes 目录下找到，比如我的是 /Volumes/CCCOMA_X64FRE_ZH-CN_DV9（可以通过 ls /Volumes 查看），这个路径后面会用到\n步骤 4 复制文件到 u 盘，Windows 10 ISO 中的一个文件 install.wim 现在太大而无法复制到 FAT-32 格式的 USB 驱动器，需要单独复制它。\n首先运行此命令以复制除该文件之外的所有内容：\n$ rsync -vha --exclude=sources/install.wim /Volumes/CCCOMA_X64FRE_ZH-CN_DV9/* /Volumes/WINDOWS10 （这里将 /Volumes/CCCOMA_X64FRE_ZH-CN_DV9/ 替换为你的目录，参考步骤 3，/Volumes/WINDOWS10 替换为你的 u 盘名，参考步骤 2）\n然后继续创建你要将文件写入的目录：\n$ mkdir /Volumes/WINDOWS10/sources 如果提示 File exists 说明该文件夹已存在，可以继续执行下面的步骤。\n使用 Homebrew 使用此终端命令安装名为 wimlib 的工具，该工具用于分割 install.wim 为 2 个小于 4G 的文件，\n$ brew install wimlib 然后运行这个命令：\n$ wimlib-imagex split /Volumes/CCCOMA_X64FRE_EN-US_DV9/sources/install.wim /Volumes/WIN10/sources/install.swm 3800 同样的，需要将 /Volumes/CCCOMA_X64FRE_EN-US_DV9/ 这部分替换为你的路径，以及 /Volumes/WIN10/ 这部分。之后需要等待一会，直到提示以下信息：\nSplitting WIM: 4193 MiB of 4193 MiB (100%) written, part 2 of 2 Finished splitting \u0026quot;/Volumes/CCCOMA_X64FRE_ZH-CN_DV9/sources/install.wim\u0026quot; # 制作完成 步骤 5 至此启动盘已经制作完成，弹出 u 盘，插入到要安装 windows 的电脑，并使用 u 盘启动便可以看到 windows 的安装界面了。\n总结 在 mac 上制作 windows 启动盘还是有点小麻烦的，不像 windows 上直接使用软碟通那么简单方便，网上也有不少教程是过时错误的，比如使用 balenaEtcher ，这个软件自己都会弹出警告，无法刻录 windows，还有使用命令行的，要将 u 盘格式化为 MS-DOS 格式，因为 MS-DOS 格式无法一次写入 4G 以上的文件，会提示 File too large 错误，如果格式化为 ExFAT，虽然可以成功写入，但是 bios 无法识别出 U 盘。其实本文使用的方法也是命令行，只是对 4G 文件进行了特殊处理，总的来看这个方法简单有效，感谢这位大佬的教程。\n","date":"2022年05月17日","permalink":"/posts/mac-shang-zhi-zuo-windows/","summary":"来源：https://chinese.freecodecamp.org/news/how-make-a-windows-10-usb-using-your-mac-build-a-bootable-iso-from-your-macs-terminal/\n准备 windows iso 文件 一个大小至少 8G 的 U 盘 mac 上安装好 homebrew 步骤 1 查看 U 盘的名称：","title":"在 mac 上制作 windows10 启动盘，亲测可用"},{"contents":" 参考：\nhttps://www.zsythink.net/archives/category/%e8%bf%90%e7%bb%b4%e7%9b%b8%e5%85%b3/iptables/page/2\n规则相关命令 查看规则 # 查看 filter 表的所有规则，-t 后面跟表名 $ iptables -t filter -L # 查看 raw 表的所有规则 $ iptables -t raw -L # 查看 filter 表（-t 缺省时，默认为 filter 表）的 INPUT 链上的所有规则 $ iptables -L INPUT # 加 -v 可以输出更详细的信息 $ iptables -vL INPUT # 加 -n 会直接输出 IP 地址，不加的话会继续转换，比如 0.0.0.0 会转换为 anywhere $ iptables -nL INPUT # 为输出的规则列表添加序号 $ iptables --line -L INPUT 操作规则 清空规则 # 清空 filter 表 INPUT 链中的规则 $ iptables -F INPUT 添加规则 $ iptables -t filter -I INPUT -s 192.168.64.1 -j DROP 上面的命令表示：在 filter 表的 INPUT 链中添加一条规则：丢弃（DROP）源地址为 192.168.64.1 的报文\n每个参数的意思：\n-t：指定要操作的表，此处指定了操作 filter 表，不使用 -t 选项指定表时，默认为操作 filter 表\n-I：指明将 ”规则” 插入至哪个链中，即添加规则\n-s：指明“匹配条件”中的”源地址”，如果报文的源地址属于-s对应的地址，那么报文则满足匹配条件\n-j：指明当”匹配条件”被满足时，所对应的动作\n测试：\n上面添加的 192.168.64.1是我的宿主机 ip，现在用宿主机来访问一下这台设置了规则的虚拟机：\n$ ping 192.168.64.5 # ping 虚拟机 PING 192.168.64.5 (192.168.64.5): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 发现已经无法访问了。（之后发现 ssh 给断了，虚拟机也进不去了，用的是 multipass😅）\n添加规则的同时可以指定其插入的位置，不同的位置会影响规则的执行。具体的例子可以参考 https://www.zsythink.net/archives/1517，大致流程是：已经设置了一条 drop 规则，如果追加一条相同 ip，但是 accept 的规则，发现依然无法 ping 通，因为 drop 在前 accept 在后；但是如果将 accept 插入到头部，那么 INPUT 链会先执行 accept 规则，数据包就会被放行，因为报文已经被放行了，所以，即使第二条 drop 规则能够匹配到刚才”放行”的报文，也没有机会再对刚才的报文进行丢弃操作了。\n# -A 追加一条规则（append），也就是添加到末尾 $ iptables -A INPUT -s 192.168.64.1 -j ACCEPT # -I 添加到头部 $ iptables -I INPUT -s 192.168.64.1 -j ACCEPT # 插入到规则的下标 2 处 $ iptables -I INPUT 2 -s 192.168.64.1 -j ACCEPT 删除规则 根据规则编号删除（指定 \u0026ndash;line ）\n$ iptables -t filter -D INPUT 3 上面的命令会删除 filter 表的 INPUT 链的第 3 条规则\n根据匹配条件删除\n$ iptables -D INPUT -s 192.168.64.1 -j ACCEPT 上述命令表示删除 INPUT 链中源地址为 192.168.64.1，动作为 ACCEPT 的规则\n清空链上的规则\n# 清空 filter 表 INPUT 链中的规则 $ iptables -F INPUT 清空表的所有规则\n# 不指定链名 $ iptables -F 自定义链 自定义链用于对规则的管理，这里我有一个疑问，iptables 的 表 已经是对规则的一种分类管理了，为什么还需要自定义链呢？我暂时的理解是，虽然表已经进行了分类，但是粒度还是比较粗，比如 filter 表只是负责过滤功能，但是过滤功能可能还需要进行进一步分类，比如有针对 HTTP 的过滤规则，有针对 SSH 的过滤规则，\n创建 实验的两台机器 IP 为：192.168.1.110 和 192.168.1.103，在 110 上设置一条自定义链，规则是阻止 103 这台机器来访问 8080 端口，8080 运行了一个 web 程序，访问会返回 “Hello”\n首先创建一条名为 IN_WEB 的自定义链\n$ iptables -t filter -N IN_WEB 为该链设置规则，对 源地址 为 192.168.1.103 的连接进行 REJECT （拒绝）处理\n$ iptables -t filter -I IN_WEB -s 192.168.1.103 -j REJECT 查看 filter 中的规则，使用 IN_WEB 进行过滤\n$ iptables -t filter --line -nvL IN_WEB Chain IN_WEB (0 references) num pkts bytes target prot opt in out source destination 1 0 0 REJECT all -- * * 192.168.1.103 0.0.0.0/0 reject-with icmp-port-unreachable 自定义链不能直接使用，需要将其添加到一条 默认链 中， 将该链添加到 INPUT 链中，这代表该条链用于接收到数据时\n$ iptables -I INPUT -p tcp --dport 8080 -j IN_WEB 查看\n$ iptables -t filter --line -nvL IN_WEB Chain IN_WEB (1 references) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 192.168.1.103 0.0.0.0/0 reject-with icmp-port-unreachable 此时使用 103 的机器访问 110 的 8080 端口：\n$ curl 192.168.1.110:8080 curl: (7) Failed to connect to 192.168.1.110 port 8080 after 37 ms: Connection refused 报错 “Connection refused”（连接被拒绝），这个错误见得也比较多了，当访问的端口没有被程序监听时，也会报这个错误。此时说明拦截规则已经成功。\n110 的 9090 端口上也运行了一个 web 程序，访问一下看看是不是只拦截了 8080 端口：\n$ curl 192.168.1.110:9090 Hello% 9090 可以正常访问，说明拦截规则成功。\n删除 删除自定义链需要满足两个条件\n1、自定义链没有被引用\n2、自定义链中没有任何规则\n自定义链被放置在 INPUT 上，首先查看 INPUT 的所有规则，并删除属于自定义链的规则\n$ iptables -nvL INPUT # 查看 INPUT 链上的所有规则 Chain INPUT (policy ACCEPT 4397 packets, 972K bytes) pkts bytes target prot opt in out source destination 0 0 IN_WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 7753 1508K KUBE-ROUTER-INPUT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kube-router netpol - 4IA2OSFRMVNDXBVV */ 0 0 IN_WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 1 64 IN_WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:5555 $ iptables -D INPUT 7 # 删除规则，数字是上面列表的序号，根据自己的实际情况来指定 $ iptables --line-number -vL INPUT # 使用 --line-numbers 可以显示规则的编号，这样删除起来更方便 Chain INPUT (policy ACCEPT 2702 packets, 895K bytes) num pkts bytes target prot opt in out source destination 1 5865 1470K KUBE-ROUTER-INPUT all -- any any anywhere anywhere /* kube-router netpol - 4IA2OSFRMVNDXBVV */ 2 2702 895K KUBE-FIREWALL all -- any any anywhere anywhere 3 2702 895K KUBE-NODEPORTS all -- any any anywhere anywhere /* kubernetes health check service ports */ 4 0 0 KUBE-EXTERNAL-SERVICES all -- any any anywhere anywhere ctstate NEW /* kubernetes externally-visible service portals */ $ iptables -F IN_WEB # 清空 filter 表上的 IN_WEB 链上的所有规则（不指定 -t 时默认为 filter 表） $ iptables -X IN_WEB # 删除链 $ iptables -t filter --line -nvL IN_WEB # 再次查看该链，发现已经无法输出信息了，说明删除成功 iptables v1.8.7 (nf_tables): chain `IN_WEB' in table `filter' is incompatible, use 'nft' tool. 负载均衡 https://vflong.github.io/sre/network/2020/02/23/turning-iptables-into-a-tcp-load-balancer.html\n实践 DNT 转发 流程：有两台机器 192.168.64.4 （简称 4）和 192.168.64.1（简称 1），4 上的 8080 和 9090 端口运行了两个 tcp 程序，连接到 8080 时会返回消息： ”8080“，连接到 9090 时会返回消息 ”9090“，这里通过 iptables 设置 nat 转发，将访问 8080 的连接转发到 9090，也就是 DNAT（D 代表 dst，目标），修改报文的目标地址。\n添加规则：\n$ iptables -t nat -A PREROUTING -p tcp -d 192.168.64.4 --dport 8080 -j DNAT --to 192.168.64.4:9090 在 nat 表的 PREROUTING 链中添加一条规则，将目标地址为 192.168.64.4，协议为 tcp 的请求转发（DNAT）到 192.168.64.4 （本机）的 9090 端口\n查看规则：\n$ iptables -t nat -nL PREROUTING DNAT tcp -- 0.0.0.0/0 192.168.64.4 tcp dpt:8080 to:192.168.64.4:9090 在 1 上执行 nc：\n$ nc 192.168.64.4 8080 9090 访问的是 8080 端口，返回的消息是 9090，说明转发成功。\n这两个程序还会记录访问的 IP 地址，查看日志：\n$ cat 8080 $ cat 9090 2022/05/19 15:41:44 192.168.64.1:63310 8080 的日志为空，说明该请求被 iptables 拦截 后转发给了 9090\n附代码：\npackage main import ( \u0026quot;net\u0026quot; \u0026quot;log\u0026quot; ) func main() { l, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;:8080\u0026quot;) assert(err) for { conn, err := l.Accept() assert(err) log.Println(conn.RemoteAddr()) conn.Write([]byte(\u0026quot;8080\u0026quot;)) } } func assert(err error) { if err != nil { panic(err) } } package main import ( \u0026quot;net\u0026quot; \u0026quot;log\u0026quot; ) func main() { l, err := net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;:9090\u0026quot;) assert(err) for { conn, err := l.Accept() assert(err) log.Println(conn.RemoteAddr()) conn.Write([]byte(\u0026quot;9090\u0026quot;)) } } func assert(err error) { if err != nil { panic(err) } } 使用 nohup 部署：\n$ go build tcp_8080.go $ go build tcp_9090.go $ nohup ./tcp_8080 \u0026gt; 8080 \u0026amp; $ nohup ./tcp_9090 \u0026gt; 9090 \u0026amp; ","date":"2022年05月16日","permalink":"/posts/iptables/","summary":"参考：\nhttps://www.zsythink.net/archives/category/%e8%bf%90%e7%bb%b4%e7%9b%b8%e5%85%b3/iptables/page/2\n规则相关命令 查看规则 # 查看 filter 表的所有规则，-t 后面跟表名 $ iptables -t filter -L # 查看 raw 表的所有规则 $ iptables -t raw -L # 查看 filter 表（-t 缺省时，默认为 filter 表）的 INPUT 链上的所有规则 $ iptables -L INPUT # 加 -v 可以输出更详细的信息 $ iptables -vL INPUT # 加 -n 会直接输出 IP 地址，不加的话会继续转换，比如 0.","title":"linux iptables"},{"contents":"deplopyment 用于 pod 的更新相关操作。\n如何更新 pod 更新是一个非常常见的场景，比如：当前 pod 运行的是某个 v1 版本的镜像，一个月后该镜像发布了 v2 版本，此时想将所有的 pod 更新到 v2 版本，该怎么做呢？\n比较容易想到的以下几种方式：\n先删除所有旧版本的 pod，再创建新版本的 pod 实践如下：\napiVersion: v1 kind: ReplicationController metadata: name: kubia-v1 spec: replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs --- apiVersion: v1 kind: Service metadata: name: kubia spec: selector: app: kubia ports: - port: 80 targetPort: 8080 创建一个基于 luksa/kubia:v1 的镜像的 pod，同时创建一个 service\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-v1-td826 1/1 Running 0 4m16s kubia-v1-k8hz4 1/1 Running 0 4m16s kubia-v1-7kpmg 1/1 Running 0 4m16s $ kubectl get rc NAME DESIRED CURRENT READY AGE kubia-v1 3 3 3 5m $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 37h kubia ClusterIP 10.43.191.168 \u0026lt;none\u0026gt; 80/TCP 5m3s $ curl kubia This is v1 running in pod kubia-v1-7kpmg\t测试访问，输出表示当前运行的是 v1 版本\n现在来进行升级操作，将 luksa/kubia:v1 升级为 luksa/kubia:v2。\n修改 yaml：\nspec: containers: - image: luksa/kubia:v2 执行 yaml：\n$ kubectl apply -f kubia-rc-and-service-v1.yaml replicationcontroller/kubia-v1 configured service/kubia unchanged $ curl 10.43.191.168 This is v1 running in pod kubia-v1-7kpmg 发现输出的还是 v1，可能是因为 rc 已经有 3 个 pod 了，修改 image 不会令其删除旧的并创建新的，那就只好手动删除旧 pod 了：\n$ kubectl delete po -l app=kubia pod \u0026quot;kubia-v1-td826\u0026quot; deleted pod \u0026quot;kubia-v1-k8hz4\u0026quot; deleted pod \u0026quot;kubia-v1-7kpmg\u0026quot; deleted 此时删除了所有 app=kubia 的 pod（在我的机器上测试时，这个删除操作阻塞了将近 10s），rc 检测到其管理的 pod 数量不足所需的数量 3，那么就会新创建 3 个 pod，因为之前改掉了创建 template 中的镜像为 v2，所以现在 rc 创建出来的都是新版本的 pod 了：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-v1-b5lkr 1/1 Running 0 37s kubia-v1-tq7nh 1/1 Running 0 37s kubia-v1-w6r6t 1/1 Running 0 37s $ curl 10.43.191.168 This is v2 running in pod kubia-v1-w6r6t 经过上面的一系列操作，终于完成了 pod 的升级，这种方式有一个问题：删除 pod 后到 rc 创建完成这段时间，是没有 pod 能够提供服务的，这就会导致程序在一段时间内不可用，这对于一些应用是无法接收的。\n先创建新版本的 pod，再删除所有旧版本的 pod 先将镜像退回至 v1 版本，采用和上面相同的方式，这里就不贴具体操作了。\n复制一份之前的 yaml，在该基础上进行修改：\n$ cp kubia-rc-and-service-v1.yaml kubia-rc-and-service-v2.yaml 修改为：\napiVersion: v1 kind: ReplicationController metadata: name: kubia-v2 # 修改 spec: replicas: 3 template: metadata: name: kubia labels: app: kubia-v2 # 修改 spec: containers: - image: luksa/kubia:v2 # 修改 name: nodejs --- apiVersion: v1 kind: Service metadata: name: kubia spec: selector: app: kubia-v2 # 修改 ports: - port: 80 targetPort: 8080 修改之处已经标出，注意这里还更新了 service 的选择器，让其为 app=kubia-v2 的 pod 提供对外服务。\n$ kubectl apply -f kubia-rc-and-service-v2.yaml replicationcontroller/kubia-v2 created service/kubia configured $ kubectl get rs No resources found in default namespace. $ kubectl get rc NAME DESIRED CURRENT READY AGE kubia-v1 3 3 3 68m kubia-v2 3 3 3 18s $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-v1-7pxtm 1/1 Running 0 11m kubia-v1-zt5pg 1/1 Running 0 11m kubia-v1-gnssp 1/1 Running 0 11m kubia-v2-46w9g 1/1 Running 0 20s kubia-v2-66wqr 1/1 Running 0 20s kubia-v2-bvjqc 1/1 Running 0 20s 此时访问 service：\n$ curl 10.43.191.168 This is v2 running in pod kubia-v2-dj8cf 发现已经切换到了 v2\n后续再删除 v1 的 pod：\n$ kubectl get rc NAME DESIRED CURRENT READY AGE kubia-v1 3 3 3 76m kubia-v2 3 3 3 98s $ kubectl delete rc kubia-v1 replicationcontroller \u0026quot;kubia-v1\u0026quot; deleted $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-v2-dj8cf 1/1 Running 0 2m9s kubia-v2-4q5tv 1/1 Running 0 2m9s kubia-v2-rbbfd 1/1 Running 0 2m9s kubia-v1-gnssp 1/1 Terminating 0 19m kubia-v1-7pxtm 1/1 Terminating 0 19m kubia-v1-zt5pg 1/1 Terminating 0 19m 等待一会，这些 Terminating 状态的 pod 就会消失了。\n使用这种方式可以做到不停机更新，但是需要更多的硬件资源，因为在短时间内会同时运行 2 倍的 pod。\n交替执行，删一个旧的，创建一个新的（滚动更新） 这种方式比较繁琐，就不演示了，大致的思路是：和 先创建新版本的 pod，再删除所有旧版本的 pod 中的做法一样，再创建一个用于管理 v2 的 rc（这里称为 rc-v2，之前的称为 rc-v1），但是 replicas 置为 0，此时有两个 rc：rc-v1，replicas 为 3，rc-v2，replicas 为 0，然后开始交替执行，rc-v1 缩容为 2，rc-v2 扩容到 1，v1 缩容到 1，v2 扩容到 2，以此类推，来完成升级。\n这种方式避免了前面两种同时创建、同时删除的缺点， 但是其也有自己的缺点：较为繁琐，且容易出错，一套流程下来要敲非常多的命令。\n好在强大的 k8s 提供了指令来完成 pod 的更新操作：\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 ⚠️ 该命令已经被删除 ：error: unknown command \u0026ldquo;rolling-update\u0026rdquo; for \u0026ldquo;kubectl\u0026rdquo;\nhttps://stackoverflow.com/questions/65303683/why-kubectl-removed-command-rolling-update\nhttps://github.com/kubernetes/kubectl/commit/d3af7e08624bfa7c2f52714b47cfe96a52d15fc0\n大致原因是：该命令是在客户端执行的，而不是在 k8s 内部执行，相比之下客户端更容易遇到一些网络中断、终端异常退出等错误，这会导致 pod 和 rc 处于中间状态\n使用 deployment 进行更新 使用如下 yaml：\napiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: replicas: 3 selector: matchLabels: app: kubia template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs 创建：\n$ kubectl apply -f deployment.yaml --record Flag --record has been deprecated, --record will be removed in the future deployment.apps/kubia created 书上着重强调了：确保在创建时使用了 --record 选项。 这个选项会记录历史版本号， 在之后的操作中非常有用。然而这里直接提示该选项已被废弃，难道已经默认记录版本号了吗？\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-5f6cdb7bf7-4pn9b 1/1 Running 0 7m26s kubia-5f6cdb7bf7-kt8gp 1/1 Running 0 7m26s kubia-5f6cdb7bf7-f6w5g 1/1 Running 0 7m26s $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 2d kubia ClusterIP 10.43.191.168 \u0026lt;none\u0026gt; 80/TCP 11h $ curl 10.43.191.168 This is v1 running in pod kubia-5f6cdb7bf7-f6w5g 开始执行滚动升级：\n$ kubectl patch deployment kubia -p '{\u0026quot;spec\u0026quot;: {\u0026quot;minReadySeconds\u0026quot;: 10}}' deployment.apps/kubia patched $ kubectl set image deployment kubia nodejs=luksa/kubia:v2 # 更新镜像 这里通过 patch 为 这个 deployment 添加了一条 minReadySeconds: 10 的属性，这个属性的作用是：指定新创建的 pod 至少要成功运行多久之后，才能将其视为可用，在后面会详细说明这个属性。\n使用 set image 命令来进行更新，其中 nodejs 是 template.sepc.containers.name 指定的。之后更新就开始了，可以开一个新的 shell 观察升级过程：\n$ while true;do curl 10.43.191.168; sleep 0.5s;done This is v1 running in pod kubia-5f6cdb7bf7-f6w5g This is v2 running in pod kubia-88894bbd4-pbr5n This is v2 running in pod kubia-88894bbd4-h8mck This is v1 running in pod kubia-5f6cdb7bf7-f6w5g This is v1 running in pod kubia-5f6cdb7bf7-f6w5g This is v2 running in pod kubia-88894bbd4-w74nb This is v2 running in pod kubia-88894bbd4-pbr5n This is v1 running in pod kubia-5f6cdb7bf7-f6w5g This is v2 running in pod kubia-88894bbd4-h8mck This is v2 running in pod kubia-88894bbd4-h8mck This is v2 running in pod kubia-88894bbd4-w74nb This is v2 running in pod kubia-88894bbd4-h8mck This is v2 running in pod kubia-88894bbd4-h8mck This is v2 running in pod kubia-88894bbd4-w74nb This is v2 running in pod kubia-88894bbd4-pbr5n 可以看到，刚开始请求会打到 v1 或者 v2，到后面就全部是 v2 了，说明升级已经完成，同时也说明该命令使用的是 滚动更新 的方式，其实具体使用哪种方式是可以配置的，字段是 .spec.strategy.type，默认为 RollingUpdate，也就是滚动更新，还有一种 Recreate，采用的是先删除所有旧的，再创建新 pod 的方式。\n使用 deployment 进行回滚 为什么需要回滚 可能发布的新版本存在一些 bug，但是在更新之前没有发现，等发现以后整个系统已经更新成新版本了，此时就需要进行回滚操作了。\n实践 接着上面的操作，此时查看 rs，会发现之前的老 rs 没有被删除：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE kubia-88894bbd4 3 3 3 29m kubia-5f6cdb7bf7 0 0 0 87m $ kubectl describe kubia-5f6 ... Pod Template: Labels: app=kubia pod-template-hash=5f6cdb7bf7 Containers: nodejs: Image: luksa/kubia:v1 ... $ kubectl describe rs kubia-888 ... Pod Template: Labels: app=kubia pod-template-hash=88894bbd4 Containers: nodejs: Image: luksa/kubia:v2 ... 为什么要保留旧的 rs 呢？这里先卖个关子 为的是能够完成回滚操作\n开始升级，这里会更新到 v3 ，这是一个带有错误的版本，访问从 5 次开始就会返回一个 500 错误：\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v3 # 更新到 v3 deployment.apps/kubia image updated $ kubectl rollout status deployment kubia # 查看更新状态 Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 old replicas are pending termination... Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 old replicas are pending termination... deployment \u0026quot;kubia\u0026quot; successfully rolled out 在另一个终端查看访问 service 的结果：\n$ while true;do curl 10.43.191.168; sleep 0.5s;done This is v3 running in pod kubia-56ff78687-kc77b Some internal error has occurred! This is pod kubia-56ff78687-2b7tz This is v3 running in pod kubia-56ff78687-kc77b Some internal error has occurred! This is pod kubia-56ff78687-2b7tz This is v2 running in pod kubia-88894bbd4-h8mck Some internal error has occurred! This is pod kubia-56ff78687-2b7tz This is v3 running in pod kubia-56ff78687-mbqdw This is v3 running in pod kubia-56ff78687-mbqdw Some internal error has occurred! This is pod kubia-56ff78687-mbqdw This is v3 running in pod kubia-56ff78687-kc77b This is v2 running in pod kubia-88894bbd4-h8mck 可以看到有一些报错了：Some internal error has occurred! This is pod kubia-56ff78687-mbqdw\n对外提供一个有 bug 的 pod 显然是不行的，此时就需要执行回滚操作了：\n$ kubectl rollout undo deployment kubia deployment.apps/kubia rolled back 在另一个终端查看访问 service 的结果：\n$ while true;do curl 10.43.191.168; sleep 0.5s;done This is v2 running in pod kubia-88894bbd4-rvzvb This is v2 running in pod kubia-88894bbd4-rvzvb This is v2 running in pod kubia-88894bbd4-pxxwt This is v2 running in pod kubia-88894bbd4-pxxwt This is v2 running in pod kubia-88894bbd4-4wknk This is v2 running in pod kubia-88894bbd4-4wknk 发现已经回滚到 v2 版本了。\n查看更新记录：\n$ kubectl rollout history deployment kubia deployment.apps/kubia REVISION CHANGE-CAUSE 1 kubectl apply --filename=deployment.yaml --record=true 3 kubectl apply --filename=deployment.yaml --record=true 4 kubectl apply --filename=deployment.yaml --record=true 这里显示的和书中的有所不同，书中显示的是：\n$ kubectl rollout history deployment kubia deployments \u0026quot;kubia\u0026quot;: REVISION CHANGE-CAUSE 2 kubectl set image deployment kubia nodejs=luksa/kubia:v2 3 kubectl set image deployment kubia nodejs=luksa/kubia:v3 感觉书上这种显示的友好一些，而且我执行的明明是 kubectl set image deployment kubia nodejs=xxx ，为什么这里记录的是 kubectl apply \u0026ndash;filename=deployment.yaml \u0026ndash;record=true ？\n前面提到过，\u0026ndash;record 已经被废弃，难道新版本对 CHANGE-CAUSE 进行了一些变更吗？暂时还不知道啥情况\n可以通过指定版本来查看详细信息，这样就可以解决上面的问题了：\n$ kubectl rollout history deployment/kubia --revision=1 deployment.apps/kubia with revision #1 Pod Template: Labels:\tapp=kubia pod-template-hash=5f6cdb7bf7 Annotations:\tkubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true Containers: nodejs: Image:\tluksa/kubia:v1 Port:\t\u0026lt;none\u0026gt; Host Port:\t\u0026lt;none\u0026gt; Environment:\t\u0026lt;none\u0026gt; Mounts:\t\u0026lt;none\u0026gt; Volumes:\t\u0026lt;none\u0026gt; 此时就可以看到该版本对应的镜像了。\n至此已经经历了 v1 - v3 一共 3 个版本的迭代，有更新操作也有回滚操作，此时查看 rs：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE kubia-5f6cdb7bf7 0 0 0 17h kubia-88894bbd4 3 3 3 16h kubia-56ff78687 0 0 0 15h 会发现每个版本的 rs 依然存在，为的就是能执行回滚操作，顺带一提，回滚不仅仅是上个版本，还可以是任意一个版本，使用 kubectl rollout undo deployment kubia --to-revision=l 即可。在使用了 deployment 后，系统中的 rs 就不用我们去管理了，所以我们也不需要自己去手动修改、删除 rs 了。\n控制滚动升级速率 有两个属性可以用来控制升级速率：\nmaxSurge：升级过程中最多允许超出 replicas 的 pod 数量，值可以是一个百分比或者一个数字 maxUnavailable：升级过程中最多允许的不可用 pod 数量，值可以是一个百分比或者一个数字 这两个值定义在 .spec.strategy.rollingUpdate 中，下面对这两个属性进行展开说明。\n前面提到过滚动升级的流程，大致是使用两个 rs，一个管理旧版本 pod，一个管理新版本 pod，管理旧版本的 rs 不断缩容，管理新版本的 rs 不断扩容，最终完成更新。通过上面的两个属性就可以对整个流程的速率进行控制，举个例子：现在一共有 3 个 pod 需要升级，此时将 maxUnavailable 设置为 2，代表最多允许有 2 个 pod 不可用，那么此时缩容 rs 就可以缩到 1。引用官方的说明：\n最大不可用\n.spec.strategy.rollingUpdate.maxUnavailable 是一个可选字段，用来指定 更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是所需 Pods 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。 如果 .spec.strategy.rollingUpdate.maxSurge 为 0，则此值不能为 0。 默认值为 25%。\n例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。 新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容， 确保在更新期间可用的 Pods 总数在任何时候都至少为所需的 Pod 个数的 70%。\n最大峰值\n.spec.strategy.rollingUpdate.maxSurge 是一个可选字段，用来指定可以创建的超出期望 Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。 如果 MaxUnavailable 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。 此字段的默认值为 25%。\n例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod 的总数不超过所需 Pod 总数的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩容， 同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%。\n对于书上的表 9.2 我有点疑问，对这两个属性的描述都是：当把百分数转换成绝对值时，会将数字四舍五入，但是后面又举了个例子：\n由于在之前场景中，设置的期望副本数为 3，上述的两个属性都设置为 25%, maxSurge 允许最多 pod数量达到 4， 同时 maxUnavailable 不允许出现任何不可用的pod (也就是说三个pod\u0026amp;、须一直处于可运行状态)\n副本数为 3，maxUnavailable 设置为 25%，3*25% = 0.75，四舍五入等于 1，那么应该允许有 1 个 pod 不可用，为什么书上说的是不允许任何 pod 不可用？\n在看官方的文档：maxUnavailable 说的是：百分比值会转换成绝对数并去除小数部分\nmaxSurge 说的是：百分比值会通过向上取整转换为绝对数\n这显然和书上说的四舍五入有出入，那么到底实际情况如何呢？只有源码能揭晓答案了。\n对应的函数在 kubernetes/pkg/controller/deployment/util.deployment_util 下：\n// desired 代表副本数量 func ResolveFenceposts(maxSurge, maxUnavailable *intstrutil.IntOrString, desired int32) (int32, int32, error) { // maxSurge 没有设置，则使用默认值 0 // roundUp 设置为 true，代表向上取整 surge, err := intstrutil.GetScaledValueFromIntOrPercent(intstrutil.ValueOrDefault(maxSurge, intstrutil.FromInt(0)), int(desired), true) if err != nil { return 0, 0, err } // maxUnavailable 没有设置，则使用默认值 0 // roundUp 设置为 false，代表向下取整 unavailable, err := intstrutil.GetScaledValueFromIntOrPercent(intstrutil.ValueOrDefault(maxUnavailable, intstrutil.FromInt(0)), int(desired), false) if err != nil { return 0, 0, err } // 所以 surge 和 unavailable 采用的策略是不同的 // 比如 replicas 为 3，maxSurge 和 maxUnavailable 都设置为 25%， // 3 * 25 / 100 = 0.75，surge 采用向上取整，所以为 1，而 unavailable // 采用向下取整, 所以为 0 if surge == 0 \u0026amp;\u0026amp; unavailable == 0 { // Validation should never allow the user to explicitly use zero values for both maxSurge // maxUnavailable. Due to rounding down maxUnavailable though, it may resolve to zero. // If both fenceposts resolve to zero, then we should set maxUnavailable to 1 on the // theory that surge might not work due to quota. unavailable = 1 } return int32(surge), int32(unavailable), nil } 调用的 GetScaledValueFromIntOrPercent 函数：\n// intOrPercent 代表一个整数或者百分比字符串，total 代表总数量，roundUp 代表向上取整还是向下取整， // 如果 intOrPercent 是 int，那么直接返回，如果是百分比，则返回 total*百分比后的值（还需要进行取整处理） func GetScaledValueFromIntOrPercent(intOrPercent *IntOrString, total int, roundUp bool) (int, error) { if intOrPercent == nil { return 0, errors.New(\u0026quot;nil value for IntOrString\u0026quot;) } // 获取 intOrPercent 的值，该值可能是一个数字或者百分比 value, isPercent, err := getIntOrPercentValueSafely(intOrPercent) if err != nil { return 0, fmt.Errorf(\u0026quot;invalid value for IntOrString: %v\u0026quot;, err) } // 如果该值是百分比 if isPercent { // 如果设置了 roundUp（不知道咋翻译），则向上取整，比如 1.2 =\u0026gt; 2（不是四舍五入） if roundUp { value = int(math.Ceil(float64(value) * (float64(total)) / 100)) } else { // 否则向下取整，比如 1.5 =\u0026gt; 1 value = int(math.Floor(float64(value) * (float64(total)) / 100)) } } return value, nil } 测试一下：\nfunc TestResolveFenceposts1(t *testing.T) { maxSurge := intstr.FromString(\u0026quot;25%\u0026quot;) maxUnavailable := intstr.FromString(\u0026quot;25%\u0026quot;) var replicas int32 = 3 s, u, err := ResolveFenceposts(\u0026amp;maxSurge, \u0026amp;maxUnavailable, replicas) if err != nil { t.Error(err) } t.Log(s, u) } // Output: // 1 0 // 代表两个值都设置为 25% 的情况下，允许多出 1 个 pod，允许有 0 个 pod 不可用 所以书上说的四舍五入并不准确，官方文档才是正确的，不过最靠谱的还是看源码 （狗头）。\n实践 该 yaml 用来创建一个 replicas 为 3 的 deployment：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 运行：\n$ kubectl apply -f nginx-deployment.yaml --record deployment.apps/nginx-deployment created $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-7848d4b86f 3 3 3 6m57s # 更新 deployment 的镜像版本，1.91 是一个不存在的版本 $ kubectl set image deployment/nginx-deployment nginx=nginx:1.91 # 查看该 deployment 管理的 rs，发现多出来了一个 rs，其期望的 pod 数量为 1， # 但是因为 1.91 版本不存在，所以 READY 为 0，这个多出来的 rs 就是用来做滚动更新的 $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-7848d4b86f 3 3 3 6h54m nginx-deployment-b475d749b 1 1 0 7m8s # 查看更新后的 deployment $ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 1 3 6h59m # 查看 deployment 的详细信息，发现滚动策略为 25% max unavailable, 25% max surge，也就是允许多出 1 个 pod，允许 # 有 0 个 pod 不可用，所以会创建一个新的 pod，所以那个新创建出来的 rs 的 replicas 为 1 $ kubectl describe deploy nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 11 Oct 2022 17:21:35 +0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 4 kubernetes.io/change-cause: kubectl apply --filename=nginx-deployment.yaml --record=true Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.91 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-7848d4b86f (3/3 replicas created) NewReplicaSet: nginx-deployment-b475d749b (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 9m9s deployment-controller Scaled down replica set nginx-deployment-b475d749b to 0 Normal ScalingReplicaSet 6m9s (x2 over 13m) deployment-controller Scaled up replica set nginx-deployment-b475d749b to 1 升级过程中检测新版本是否可用 在回滚一节中说过，更新的新版本可能是一个无法工作的版本，此时需要回滚操作，那么有没有一种方法可以边更新边检测，发现不可用就停止更新呢？当然可以，使用一个额外的 minReadySeconds 属性 + 就绪探针就可以实现。\nminReadySeconds 属性指定新创建的 pod 至少要成功运行多久之后，才能将其视为可用。在 pod 可用之前，滚动升级的过程不会继续。当所有容器的就绪探针返回成功时，pod 就被标记为就绪状态。\n如果一个新的 pod 运行出错，就绪探针返回失败，或者在 minReadySeconds 时间内它的就绪探针出现了失败， 那么新版本的滚动升级将被阻止。\n之前的笔记有写过存活探针，这里的是就绪探针，二者有所区别，但区别不是特别大，这里简单摘要一下官方的文档：\nlivenessProbe（存活探针）\n指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。\nreadinessProbe（就绪探针）\n指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。\nstartupProbe（启动探针）\n指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其 重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。\n实践 yaml 如下：\napiVersion: apps/v1 kind: Deployment metadata: name: kubia spec: replicas: 3 minReadySeconds: 10 # pod 就绪后继续等待 10 秒 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 # 确保升级过程中 pod 被挨个替换 type: RollingUpdate template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v3 name: nodejs readinessProbe: # 就绪探针 periodSeconds: 1 # 探针每隔 1 秒执行一次 httpGet: path: / port: 8080 selector: matchLabels: app: kubia 这是一个“坏掉”的镜像：就绪探针检测不会成功，下面来执行 yaml：\n$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml deployment.apps/kubia configured apply 命令可以用 YAML 文件中声明的字段来更新 Deployment。不仅更新镜像，而且还添加了就绪探针，以及在 YAML 中添加或修改的其他声明。 如果新的 YAML 也包含 replicas 字段，当它与现有 Deployment 中的数量不一 致时，那么 apply 操作也会对 Dpeloymnet 进行扩容。\n此时查看升级过程：\n$ kubectl rollout status deployment kubia Waiting for deployment \u0026quot;kubia\u0026quot; rollout to finish: 1 out of 3 new replicas have been updated... # 很久以后... error: deployment \u0026quot;kubia\u0026quot; exceeded its progress deadline 整个流程卡在了更新第一个 pod，说明更新失败了，使用 curl 请求 service 看看：\n$ while true;do curl 10.43.191.168; sleep 0.5s;done This is v2 running in pod kubia-88894bbd4-4wknk This is v2 running in pod kubia-88894bbd4-pxxwt This is v2 running in pod kubia-88894bbd4-rvzvb This is v2 running in pod kubia-88894bbd4-4wknk This is v2 running in pod kubia-88894bbd4-pxxwt This is v2 running in pod kubia-88894bbd4-pxxwt This is v2 running in pod kubia-88894bbd4-pxxwt 发现请求全部打在了 v2 pod 上，没有 v3 版本，这是符合需求的，请求不应该落到一个有问题的 pod 上\n查看 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-88894bbd4-rvzvb 1/1 Running 0 23h kubia-88894bbd4-4wknk 1/1 Running 0 23h kubia-88894bbd4-pxxwt 1/1 Running 0 23h kubia-74c44776ff-rm4xw 0/1 Running 0 98s 发现有一个 pod 没有处于 READY 状态，因为我们设置了 maxSurge 为 1，所以一共有 replicas + 1 个 pod，多出来就是这个没有 READY 的 v3 pod，因为它的探针检测失败了。通过 kubectl describe po kubia-74c44776ff-rm4xw 也可以看到，最后一行的事件显示：\nWarning Unhealthy 2m25s (x22 over 2m45s) kubelet Readiness probe failed: HTTP probe failed with statuscode: 500 ","date":"2022年05月13日","permalink":"/posts/k8s-deployment/","summary":"deplopyment 用于 pod 的更新相关操作。\n如何更新 pod 更新是一个非常常见的场景，比如：当前 pod 运行的是某个 v1 版本的镜像，一个月后该镜像发布了 v2 版本，此时想将所有的 pod 更新到 v2 版本，该怎么做呢？","title":"k8s Deployment"},{"contents":"package main import ( \u0026quot;fmt\u0026quot; \u0026quot;reflect\u0026quot; ) type I interface { Do() } type S struct{} func (s *S) Do() {} func EnforcePtr(obj interface{}) { v := reflect.TypeOf(obj) fmt.Println(v) if v == nil { fmt.Println(v.Kind()) } } func main() { var i I EnforcePtr(i) var ii I = new(S) EnforcePtr(ii) } 这段代码运行后会阻塞，阻塞在 fmt.Println(v.Kind()) 这一行，不太清楚原因，暂时作为记录\nps：阻塞后 shell 窗口无法通过 ctrl+c 强制结束，需要通过下面的命令：\nps -ef | grep \u0026lt;go file name\u0026gt; | grep -v grep | grep run | awk {'print $2'} | xargs kill 只需要将 EnforcePtr 改为如下形式即可正常工作\nfunc EnforcePtr(obj interface{}) { v := reflect.TypeOf(obj) fmt.Println(v) if v == nil { typ := reflect.TypeOf(\u0026amp;v).Elem() fmt.Println(typ.Kind()) } } ","date":"2022年05月13日","permalink":"/posts/block-code/","summary":"package main import ( \u0026quot;fmt\u0026quot; \u0026quot;reflect\u0026quot; ) type I interface { Do() } type S struct{} func (s *S) Do() {} func EnforcePtr(obj interface{}) { v := reflect.","title":"一段莫名阻塞的 reflect 代码"},{"contents":"ssh 到家里的另外一台电脑，发现输入延迟比较严重，体验不是很好，ping 发现延迟普遍在 100 -200 之间，有几次甚至超过了 400，在网上找到了一个解决方法：\n$ sudo vim /etc/ssh/sshd_config 输入 /UseDNS 查找这一行，找到后发现这行 UseDNS no 是被注释的，去掉注释，然后 service sshd restart 即可，修改之后卡顿问题缓解了不少，基本可以正常使用了。\n更新：其实有时候还是卡的一 b，看来该方法用处不是特别大\n","date":"2022年05月11日","permalink":"/posts/ssh-shu-ru-qia-dun/","summary":"ssh 到家里的另外一台电脑，发现输入延迟比较严重，体验不是很好，ping 发现延迟普遍在 100 -200 之间，有几次甚至超过了 400，在网上找到了一个解决方法：","title":"ssh 输入卡顿"},{"contents":"环境：\nMacBook m1\nUbuntu 20.04.4 LTS（使用 cat /etc/issue 查看），基于 multipass 创建\n在阅读**《深入剖析Kubernetes》-张磊——白话容器基础（三）：深入理解容器镜像**，文中提供了一个 namespace 的示例代码：\n#define _GNU_SOURCE #include \u0026lt;sys/mount.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sched.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = { \u0026quot;/bin/bash\u0026quot;, NULL }; int container_main(void* arg) { printf(\u0026quot;Container - inside the container!\\n\u0026quot;); execv(container_args[0], container_args); printf(\u0026quot;Something's wrong!\\n\u0026quot;); return 1; } int main() { printf(\u0026quot;Parent - start a container!\\n\u0026quot;); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL); waitpid(container_pid, NULL, 0); printf(\u0026quot;Parent - container stopped!\\n\u0026quot;); return 0; } 但是在我的机器上无法运行，输出为：\nParent - start a container! Parent - container stopped! 可以看到，传入 clone 的回调函数没有被执行， 因为 Container - inside the container! 没有被输出。\n在main 函数的 waitpid调用后面添加一下错误信息：\nif (WIFEXITED(status) != 1) { // 子进程非正常退出 printf(\u0026quot;error!\u0026quot;); printf(\u0026quot;%d\\n\u0026quot;, WTERMSIG(status)); // 输出信号类型 } WTERMSIG 会输出信号类型，输出为 7，查了一下，意思是 SIGBUS，这是个什么玩意呢？查了一下资料，大概是内存错误、对齐错误什么的，完全看不懂，也不知道如何解决。。。\n注：使用 gdb 进行调试也可以：\n$ gdb ns_err GNU gdb (Ubuntu 9.2-0ubuntu1~20.04.1) 9.2 (gdb) set follow-fork-mode child (gdb) r Starting program: /home/ubuntu/codetest/ns_err Parent - start a container! [Attaching after process 192301 fork to child process 192304] [New inferior 2 (process 192304)] [Detaching after fork from parent process 192301] containerPid: 192304 [Inferior 1 (process 192301) detached] Thread 2.1 \u0026quot;ns_err\u0026quot; received signal SIGBUS, Bus error. [Switching to process 192304] 0x0000000000400738 in container_main () 也可以看到 \u0026quot;ns_err\u0026quot; received signal SIGBUS, Bus error. 这句话\n最后还是从官方文档查到了答案，不得不说还是官方文档最为靠谱\nhttps://man7.org/linux/man-pages/man2/clone.2.html\n最下面有一个 example，把这段代码 copy 到本地，发现居然可以成功运行：\n#define _GNU_SOURCE #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;sys/utsname.h\u0026gt; #include \u0026lt;sched.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdint.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #define errExit(msg) do { perror(msg); exit(EXIT_FAILURE); \\ } while (0) static int /* Start function for cloned child */ childFunc(void *arg) { struct utsname uts; /* Change hostname in UTS namespace of child. */ if (sethostname(arg, strlen(arg)) == -1) errExit(\u0026quot;sethostname\u0026quot;); /* Retrieve and display hostname. */ if (uname(\u0026amp;uts) == -1) errExit(\u0026quot;uname\u0026quot;); printf(\u0026quot;uts.nodename in child: %s\\n\u0026quot;, uts.nodename); /* Keep the namespace open for a while, by sleeping. This allows some experimentation--for example, another process might join the namespace. */ sleep(200); return 0; /* Child terminates now */ } #define STACK_SIZE (1024 * 1024) /* Stack size for cloned child */ int main(int argc, char *argv[]) { char *stack; /* Start of stack buffer */ char *stackTop; /* End of stack buffer */ pid_t pid; struct utsname uts; if (argc \u0026lt; 2) { fprintf(stderr, \u0026quot;Usage: %s \u0026lt;child-hostname\u0026gt;\\n\u0026quot;, argv[0]); exit(EXIT_SUCCESS); } /* Allocate memory to be used for the stack of the child. */ stack = mmap(NULL, STACK_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0); if (stack == MAP_FAILED) errExit(\u0026quot;mmap\u0026quot;); stackTop = stack + STACK_SIZE; /* Assume stack grows downward */ /* Create child that has its own UTS namespace; child commences execution in childFunc(). */ pid = clone(childFunc, stackTop, CLONE_NEWUTS | SIGCHLD, argv[1]); if (pid == -1) errExit(\u0026quot;clone\u0026quot;); printf(\u0026quot;clone() returned %jd\\n\u0026quot;, (intmax_t) pid); /* Parent falls through to here */ sleep(1); /* Give child time to change its hostname */ /* Display hostname in parent's UTS namespace. This will be different from hostname in child's UTS namespace. */ if (uname(\u0026amp;uts) == -1) errExit(\u0026quot;uname\u0026quot;); printf(\u0026quot;uts.nodename in parent: %s\\n\u0026quot;, uts.nodename); if (waitpid(pid, NULL, 0) == -1) /* Wait for child */ errExit(\u0026quot;waitpid\u0026quot;); printf(\u0026quot;child has terminated\\n\u0026quot;); exit(EXIT_SUCCESS); } 不同之处在于这几句话：\nstack = mmap(NULL, STACK_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0); if (stack == MAP_FAILED) errExit(\u0026quot;mmap\u0026quot;); stackTop = stack + STACK_SIZE; /* Assume stack grows downward */ /* Create child that has its own UTS namespace; child commences execution in childFunc(). */ pid = clone(childFunc, stackTop, CLONE_NEWUTS | SIGCHLD, argv[1]); 它是通过 mmap 申请的内存\n现在使用相同的方式，修改一下之前的代码：\n#define _GNU_SOURCE #include \u0026lt;sys/mount.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sched.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #define STACK_SIZE (1024 * 1024) #define errExit(msg) do { perror(msg); exit(EXIT_FAILURE); \\ } while (0) static char container_stack[STACK_SIZE]; char* const container_args[] = { \u0026quot;/bin/bash\u0026quot;, NULL }; int container_main(void* arg) { printf(\u0026quot;Container - inside the container!\\n\u0026quot;); // //mount(\u0026quot;\u0026quot;, \u0026quot;/\u0026quot;, NULL, MS_PRIVATE, \u0026quot;\u0026quot;); //mount(\u0026quot;none\u0026quot;, \u0026quot;/tmp\u0026quot;, \u0026quot;tmpfs\u0026quot;, 0, \u0026quot;\u0026quot;); execv(container_args[0], container_args); printf(\u0026quot;Something's wrong!\\n\u0026quot;); return 1; } int main() { char *stack; /* Start of stack buffer */ char *stackTop; /* End of stack buffer */ printf(\u0026quot;Parent - start a container!\\n\u0026quot;); stack = mmap(NULL, STACK_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0); if (stack == MAP_FAILED) errExit(\u0026quot;mmap\u0026quot;); stackTop = stack + STACK_SIZE; int container_pid = clone(container_main, stackTop, CLONE_NEWNS|SIGCHLD , NULL); printf(\u0026quot;containerPid: %d\\n\u0026quot;, container_pid); int status; waitpid(container_pid, \u0026amp;status, 0); if (WIFEXITED(status) != 1) { printf(\u0026quot;error!\u0026quot;); printf(\u0026quot;%d\\n\u0026quot;, WTERMSIG(status)); } printf(\u0026quot;Parent - container stopped!\\n\u0026quot;); return 0; } 接下来就是见证奇迹的时刻了！\n$ ./a.out Parent - start a container! containerPid: 192918 Container - inside the container! $ exit Parent - container stopped! 没有报错了，成功进入容器，问题终于解决了。\n至于为什么需要使用 mmap，作为一个菜鸡目前还没有头绪，后续再进行研究\n","date":"2022年05月11日","permalink":"/posts/linux-clone-de-keng/","summary":"环境：\nMacBook m1\nUbuntu 20.04.4 LTS（使用 cat /etc/issue 查看），基于 multipass 创建","title":"linux clone() 的坑"},{"contents":"实践 定义一个未知后缀名的文件（这个后缀名起的得随意一点，像 xyz 这种是不行的，会被 chrome 识别为 chemical/x-xyz 类型），但是文件里的内容是 HTML：\ndemo.lubenwei\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;HelloWorld\u0026lt;/h2\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 然后写一个 http file server，将文件内容作为 response 返回：\nmime_sniffing.go\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { http.ServeFile(w, r, \u0026quot;./demo.lubenwei\u0026quot;) }) log.Println(\u0026quot;listen in 8080\u0026quot;) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { panic(err) } } 注意这里没有指定 Content-Type\n接下来运行这个 http server：\n$ go run mime_sniffing.go 然后在浏览器中访问 http://localhost:8080，会发现显示的内容是 h2 标题的 HelloWorld\n这里就比较疑惑了，我既没有指定 Content-Type 响应头，也没有将文件后缀名定义为 html，那浏览器是如何知道其内容是 html，同时解析出来的呢？查了一下，这就是所谓的 MIME 嗅探，大致意思就是如果没有定义 Content-Type，那么浏览器会读取文件内容，推断这是个什么类型的文件，进而采取不同的处理方式，比如这里就会读取我这个后缀名为 lubenwei 的文件，发现里面有 html 标签，然后将其作为 html 解析\n可以通过设置响应头 X-Content-Type-Options: nosniff 来禁止客户端嗅探文件内容，修改后代码：\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026quot;X-Content-Type-Options\u0026quot;, \u0026quot;nosniff\u0026quot;) http.ServeFile(w, r, \u0026quot;./demo.lubenwei\u0026quot;) }) log.Println(\u0026quot;listen in 8080\u0026quot;) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { panic(err) } } 但是我这边访问 http://localhost:8080 依然会显示 h2 格式的 HelloWorld（使用 chrome、edge、safari 分别测试，结果相同）\n貌似是因为 X-Content-Type-Options: nosniff 需要和 Content-Type 搭配使用才行，这里我给指定一个错误的 Content-Type：\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;json\u0026quot;) w.Header().Set(\u0026quot;X-Content-Type-Options\u0026quot;, \u0026quot;nosniff\u0026quot;) http.ServeFile(w, r, \u0026quot;./demo.lubenwei\u0026quot;) }) log.Println(\u0026quot;listen in 8080\u0026quot;) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { panic(err) } } 用浏览器（safari）访问，会发现此时不会显示 HelloWorld 了，而是会下载这个文件，不同的浏览器会有不同的策略，chrome 和 edge 的话是会直接显示出文件的内容，这里需要注意的是浏览器可能会走缓存，导致还是显示 h2 格式的 HelloWorld，可以开一个隐身窗口访问：\n","date":"2022年05月08日","permalink":"/posts/http_mime_sniff/","summary":"实践 定义一个未知后缀名的文件（这个后缀名起的得随意一点，像 xyz 这种是不行的，会被 chrome 识别为 chemical/x-xyz 类型），但是文件里的内容是 HTML：","title":"HTTP MIME 嗅探 [待完善]"},{"contents":"什么是 service pod 往往需要与集群内的其他 pod 进行通信，可能也有外部的客户端需要访问集群中的 pod，如果需要管理员手动将 pod 的地址提供给访问者，那就太麻烦了，而且在 k8s 中 pod 的地址是不确定的、变动的：pod 可能被 rc、rs 缩减，或者发生异常下线，或者节点异常。所以 k8s 需要提供一种统一的对外访问的资源类型，也就是 service。\nservice 为 一组功能相同（逻辑意义上的，例如使用相同的标签）的 pod 提供单一不变的接入点，它的 IP 和端口不会改变，所以客户端可以统一连接到 service，service 再将请求转发给其管理的某一个 pod（会负载均衡的访问），这样就可以解决上面的 pod 地址变动问题。\n创建 service 下面的 yaml 可以创建一个 service：\napiVersion: v1 kind: Service metadata: name: kubia spec: ports: - port: 80 # 该 service 的端口 targetPort: 80 # 转发到 pod 的 80 端口 selector: app: kubia # 具有 app=kubia 标签的 pod 都属于该 service ⚠️ targetPort 必须指向一个有效的、被监听的 port，否则之后访问该 service 会报错 Connection refused\n创建完成后，可以使用下面的命令进行查看，获取 service 的 IP 地址：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 23h kubia ClusterIP 10.43.87.107 \u0026lt;none\u0026gt; 80/TCP 23h 测试 知道 service 的 IP 地址后就可以进行测试了，有以下几种方式\n在 node 执行 curl serviceIP\n$ curl 10.43.87.107 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; # 省略 ... 因为我转发的容器是 nginx，所以会输出 nginx 的 index.html，这里 curl 的地址没有写端口号，是因为不写端口号会默认走 80 端口，然后这个 svc 的 port 刚好也是 80，所以可以省略。\n进入一个容器内部执行 curl\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-2qkls 1/1 Running 0 24h kubia-qgtjw 1/1 Running 0 24h kubia-mbghg 1/1 Running 0 24h nginx 1/1 Running 0 20h $ kubectl exec -it kubia-2qkls -- curl 10.43.87.107 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; # 省略 ... 如果用过 docker，那么这个 exec 应该非常熟悉了\n服务发现 虽然 service 提供了一个统一访问的 pod 的途径，但是还存在一个问题：访问者如何知道 service 的地址？如果需要管理员手动执行命令查看 service 的 IP 并告诉访问者，这显然有些麻烦，不够智能，好在牛逼的 k8s 为这一问题提供了服务发现的解决方案。\n使用环境变量 ⚠️ 这种方式需要 service 先于 pod 创建，这样在 pod 创建过程中，k8s 会将其所属的 service 地址写入到其环境变量中，pod 通过环境变量就可以知道 service 的地址了。\nservice 的环境变量名是 matadata.name_SERVICE_HOST 和 matadata.name_SERVICE_PORT，name 会转换为全大写，- 会转换为下划线\n实践\nservice 依然沿用上面的 yaml：\napiVersion: v1 kind: Service metadata: name: kubia spec: ports: - port: 80 # 该 service 的端口 targetPort: 80 # 转发到 pod 的 80 端口 selector: app: kubia # 具有 app=kubia 标签的 pod 都属于该 service 客户端 pod yaml，这是一个 go 程序，通过 http 来访问 nginx，输出 nginx index.html\napiVersion: v1 kind: Pod metadata: name: http-get spec: containers: - name: http-get image: stdoutt/http-get-env-arm64:v1 args: [\u0026quot;-hostEnv\u0026quot;, \u0026quot;KUBIA_SERVICE_HOST\u0026quot;, \u0026quot;-portEnv\u0026quot;, \u0026quot;KUBIA_SERVICE_PORT\u0026quot;] 注意该镜像只支持 arm64，需要传递两个参数，分别是 service host 的环境变量名和 port 的环境变量名，前面提到过这两个环境变量的命名规则，因为我创建的 service metadata.name 是 kubia，所以两个环境变量名分别是：KUBIA_SERVICE_HOST 和 KUBIA_SERVICE_PORT。\n创建客户端 pod：\n$ kubectl apply -f http-get.yaml pod/http-get created 查看 pod 日志：\n$ kubectl logs http-get # 程序会打印出两个环境变量 KUBIA_SERVICE_HOST:10.43.65.224 KUBIA_SERVICE_PORT:80 url: http://10.43.65.224:80 # nginx index.html 成功输出了 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 通过日志内容，说明 pod 成功通过环境变量获取到了 service 的地址，并通过 service 访问到了 pod。\n附：\ngo 源码：\npackage main import ( \u0026quot;flag\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; ) var hostServiceEnvName = flag.String(\u0026quot;hostEnv\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;k8s service host env name\u0026quot;) var portServiceEnvName = flag.String(\u0026quot;portEnv\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;k8s service port env name\u0026quot;) func main() { flag.Parse() host := os.Getenv(*hostServiceEnvName) port := os.Getenv(*portServiceEnvName) url_ := fmt.Sprintf(\u0026quot;http://%v:%v\u0026quot;, host, port) fmt.Printf(\u0026quot;%v:%v\\n\u0026quot;, *hostServiceEnvName, host) fmt.Printf(\u0026quot;%v:%v\\n\u0026quot;, *portServiceEnvName, port) fmt.Printf(\u0026quot;url: %v\\n\u0026quot;, url_) resp, err := http.Get(url_) if err != nil { panic(err) } defer resp.Body.Close() b, err := io.ReadAll(resp.Body) if err != nil { panic(err) } fmt.Println(string(b)) } Dockerfile\nFROM golang:alpine AS builder # 为我们的镜像设置必要的环境变量 ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=arm64 # 移动到工作目录：/build WORKDIR /build # 将代码复制到容器中 COPY . . # 将我们的代码编译成二进制可执行文件 app RUN go build -o app . ################### # 接下来创建一个小镜像 ################### FROM scratch # 从builder镜像中把/dist/app 拷贝到当前目录 COPY --from=builder /build/app / # 需要运行的命令 ENTRYPOINT [\u0026quot;/app\u0026quot;] 使用 DNS 除了环境变量外，还有一张更简单的方式：通过 DNS 发现服务，之后便可以通过 FQDN 进行访问，格式类似于：backend-database.default.svc.cluster.local，其中 backend-database 是 service 的名字，default 是 service 所在的命名空间，svc.cluster.local 是在所有集群本地服务名称中使用的可配置集群域后缀（这里不懂）。\n⚠️ 客户端仍然需要知道服务的端口号。如果服务使用标准端口号(例如，HTTP 的 80 端口或 Postgres 的 5432 端口)，这样是没问题的。 如果并不是标准端口， 客户端可以从环境变量中获取端口号 。\n（摘抄自书上）\n这里没看懂，都直接通过域名访问了，为什么客户端还需要知道端口号？\n如果客户端 pod 和被访问 pod 在同一个命名空间，那么可以直接用 service.name 进行访问，后面的可以全部省略，比如下面的例子，直接使用 service 的名字 kubia 进行访问：\n# 查看 service，获取 service 的名字 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 33h kubia ClusterIP 10.43.65.224 \u0026lt;none\u0026gt; 80/TCP 8h # 查看所有的 pod $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-msv7g 1/1 Running 0 8h kubia-8d2gm 1/1 Running 0 8h kubia-r4fcs 1/1 Running 0 8h # 选择 1 个容器，在内部执行 curl 域名 $ kubectl exec kubia-msv79 -- curl kubia % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; # curl 完整域名 $ kubectl exec -it kubia-msv79 -- curl kubia.default.svc.cluster.local \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 集群内访问 ClusterIP 通过集群内部 IP 地址暴露服务，但该地址 仅在集群内部 可见、可达，它无法被集群外部的客户端访问，是 service 的默认访问类型。如果不明确指定 clusterIP，则由 K8S 动态指定一个，也支持用户手动明确指定。\n下面这个 yaml 创建了一个没有明确指定 clusterIP 的 service，以及一个创建 nginx 的 rs：\n--- apiVersion: v1 kind: Service metadata: name: nginx-service spec: ports: - port: 80 # 该 service 的端口 selector: app: nginx --- apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-rs spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 --- 运行后查看：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 31h nginx-service ClusterIP 10.43.27.242 \u0026lt;none\u0026gt; 80/TCP 7h9m $ kubectl describe svc nginx-service Name: nginx-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=nginx Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.43.27.242 IPs: 10.43.27.242 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.42.0.30:80,10.42.0.32:80,10.42.0.36:80 Session Affinity: None Events: \u0026lt;none\u0026gt; $ curl 10.43.27.242 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 发现这个 service 是 ClusterIP 类型的，且 k8s 自动指定了一个地址，并且可以成功访问。\n也可以手动指定一个 clusterIP，在上面的 yaml 的 service 部分中修改一下：\napiVersion: v1 kind: Service metadata: name: nginx-service1 # 起个新名字 spec: ports: - port: 80 # 该 service 的端口 clusterIP: 10.43.27.66 # 手动指定 ip selector: app: nginx 运行：\n$ kubectl apply -f svc_clusterip.yaml service/nginx-service1 created $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 32h nginx-service ClusterIP 10.43.27.242 \u0026lt;none\u0026gt; 80/TCP 7h16m nginx-service1 ClusterIP 10.43.27.66 \u0026lt;none\u0026gt; 80/TCP 10s $ curl 10.43.27.66 # 可以成功访问 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 但是不推荐手动指定 clusterIP，可能会造成 IP 冲突。\n使用 kube-proxy 让外部访问 ClusterIP Service 正常来说 ClusterIP 类型的 Service 是无法从外部访问的，但是有种特殊的方法可以——使用 kube-proxy，也就是上图中的 proxy 部分。\n具体的流程：\n首先在集群内节点执行：\n$ kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8081 注意要加上 --address 和 --accept-hosts，否则访问会返回 Forbidden。\n然后在外部机器执行以下格式语句进行访问：\n$ curl http://[nodeIP]:[port]/api/v1/namespaces/[namespace-name]/services/[service-name]/proxy 比如：\n$ curl -X GET -L http://192.168.31.50:8081/api/v1/namespaces/default/services/nginx/proxy PS: \u0026ldquo;curl -L\u0026rdquo; 中的 \u0026ldquo;-L\u0026rdquo; 是 curl 命令中的选项，它的含义是 \u0026ldquo;Follow any redirections\u0026rdquo;. 也就是说，如果服务器返回了一个重定向响应，那么 curl 命令就会自动跟随重定向并请求重定向的地址。\n在我的机器上测试，-X GET 可以不指定，但是 -L 一定要指定，否则返回结果为空\n感觉有点类似 NodePort，也是使用节点 IP + proxy 开启的端口进行访问，不过前提是你的外部机器可以通过 IP 访问到这个节点\n对外访问 参考：\nKubernetes NodePort vs LoadBalancer vs Ingress？ 我们应该什么时候使用？\nKubernetes NodePort vs LoadBalancer vs Ingress？ 我们应该什么时候使用？（原文）\n前面提到的这些都仅限于集群的内部访问，集群外是无法访问的，比如上面的 service IP 是 10.43.65.224，这只在集群内可以 curl ，集群外是不行的，而且这个 IP 即便是在集群内也是无法 ping 通的，因为这是一个虚拟 IP 地址。\n有以下几种方式可以将 service 暴露给集群外部：\nNodePort NodePort 会在 每个 node 上开启一个端口 用来访问 service，这个端口定义在 spec.ports[0].nodePort，用户可以使用 节点IP:nodePort 进行访问，也可以使用 serviceIP:port 访问（这种方式其实就是前面介绍的常规 service 访问方式，只能在集群内部使用），nodePort 也可以不指定，会随机从 30000-32767 中选择一个。\n实践：\n需要准备以下 yaml：\n用来创建 nginx deployment 的 yaml： apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 2 selector: matchLabels: # manage pods with the label app: nginx app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 创建 NodePort 类型的 service，用来访问 nginx pod： apiVersion: v1 kind: Service metadata: name: kubia-node-port spec: type: NodePort ports: - port: 80 # 该 service 的端口 targetPort: 80 # 转发到 pod 的 80 端口 nodePort: 30123 # 30000-32767 之间 selector: app: nginx 之后使用 apply -f 执行上面的两个 yaml（这里就不展示了）\n查看 service：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 13d kubia-node-port NodePort 10.43.21.78 \u0026lt;none\u0026gt; 80:30123/TCP 6m12s 此时就可以使用集群中的任意一个节点的 IP 加 30123 端口进行访问了，比如我的是：\n$ ifconfig | grep 192 inet 192.168.64.4 netmask 255.255.255.0 broadcast 192.168.64.255 $ curl 192.168.64.4:30123 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; LoadBalance LoadBalance（负载均衡），大致流程是：对外提供一个统一的 IP，这个 IP 就是负载均衡器，它会将用户请求负载均衡的分发给 pod，我个人的理解是，它在 NodePort 的基础上又添加了一层类似代理层的东西，让客户端访问更加方便。\n举个例子：假设此时有 2 个工作节点 1.1.1.1 和 2.2.2.2，每个节点上运行 3 个 nginx pod，nodePort 为 30000，那么在 NodePort 下，客户端访问需要指定某个 node 的具体 IP，比如 1.1.1.1:30000 或者 2.2.2.2:30000；如果使用 loadBalance 的话，则会提供一个统一的公网 IP（比如 180.1.1.1）作为负载均衡器，用户只要访问这个 IP 就可以（当然还要添加端口号），负载均衡器会负载均衡的把请求分发到 1.1.1.1:30000 或者 2.2.2.2:30000，比如如下的 loadBalance yaml：\napiVersion: vl kind: Service metadata: name: kubia-loadbalancer spec: type: LoadBalancer ports: - port: 80 # loadBalance 的端口 targetPort: 8080 # 转发到 pod 的 8080 端口 nodePort: 30000 selector: app: kubia LoadBalance 通常由云服务商提供，所以实践起来可能麻烦一些，需要在腾讯云这种平台上实践，好像也有一些组件可以提供本地的使用，这部分我还没有去了解。\nIngress 个人吐槽（请忽视）：\n这个玩意是我目前为止用的最蛋疼的一个功能，不是不好用，而是连用都用不上，我在 minikube 上使用的 ingress-nginx 因为拉取的镜像地址被墙，导致根本无法开启 ingress 服务，又因为我的 minikube 是跑在 multipass 虚拟机上的，不知道如何共享宿主机的 vpn，导致这个问题一直无法解决，不得不吐槽一下，m1 的生态还是有点问题，就虚拟机这块，我找了半天，基本能用的只有这个简陋的 multipass （收费的 parallels 没有尝试），vmware 直接无法运行（提示什么该软件基于 Intel 但却尝试使用 rosetta2 运行），还有一个 virtualBox ，这个我直接懒得下了，据说都不支持 m1，而且这种网络问题搞得我真的很头大，不得不吐槽一下天朝的网络，花费大把时间去解决网络问题，还有看见某个流程一直卡在 pull 上，让人有一种想砸掉电脑的冲动，最蛋疼的是这个问题还没有什么靠谱的解决方式，在 minikube 和 ingress-nginx 的 github 上找到了关于国内拉取的 issue，但是基本也没什么有用的答案，还有一些网上的教程，给的修改版 yaml 直接跑都跑不起来。\n在 mac 上跑 minikube 可以成功开启 ingress 插件，但是又会报 Because you are using a Docker driver on darwin, the terminal needs to be open to run it 错误，网上找了半天也没看见一个能用的解决方法\n折腾了一天都没把 ingress 给跑起来，感觉是在纯纯的浪费时间\nLoadBalance 的方式存在一个缺点：只能为一种类型的 service 提供服务，比如上面的介绍的 loadbalance 只是用来访问 nginx 的，如果现在集群添加了一些 redis pod，那么又要新创建一个 LoadBalance 来提供对外服务（比如 spec.ports[0].port=6379, target=6379, nodePort=30001），客户端通过 LoadBalanceIP:30000 这个地址来完成对 nginx pod 的访问，通过 LoadBalanceIP:30001 来完成对 redis pod 的访问，因为 LoadBalanceIP 是公网 IP，所以这么搞无疑有点浪费。\n为了解决上面的问题，ingress 这个玩意就应运而生了，这个东西其实说白了就是 service 的 service（无限套娃？），类似于 web 路由的功能，通过访问不同的域名来完成对不同 service 的访问，就比如这张图： 而且 ingress 工作在应用层，所以可以提供一些 service 不能实现的功能，比如基于 cookie 的会话亲和性 (session affinity) 等\n基于 minikube nginx ingress 的实践 该实践基于 minikube\nall.yaml\n包含了三个 pod 和三个对应的 service\n--- apiVersion: v1 kind: Pod metadata: name: hello-app labels: app: hello-app spec: containers: - name: hello-app image: stdoutt/hello-app-arm64 # 注意这里使用的镜像仅适用于 arm64 机器 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: hello-app-service spec: type: NodePort ports: - port: 81 # 该 service 的端口 targetPort: 8080 # 转发到 pod 的 8080 端口 nodePort: 30111 selector: app: hello-app --- apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - port: 80 # 该 service 的端口 targetPort: 80 # 转发到 pod 的 80 端口 nodePort: 30222 selector: app: nginx --- apiVersion: v1 kind: Pod metadata: name: redis labels: app: redis spec: containers: - name: redis image: redis ports: - containerPort: 6379 --- apiVersion: v1 kind: Service metadata: name: redis-service spec: type: NodePort ports: - port: 82 # 该 service 的端口 targetPort: 6379 # 转发到 pod 的 8080 端口 nodePort: 30333 selector: app: redis ingress.yaml\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress spec: rules: - host: ingress.example.com http: paths: - path: /redis pathType: Prefix backend: service: name: redis-service port: number: 82 - path: /hello-app pathType: Prefix backend: service: name: hello-app-service port: number: 81 - path: /nginx pathType: Prefix backend: service: name: nginx-service port: number: 80 运行上面的 2 个 yaml\n查看 ingress 的 ip：\n$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE example-ingress nginx ingress.example.com 192.168.49.2 80 9h 将 ingress ip 写入到 /etc/hosts：\n$ vim /etc/hosts # 写入这一条：192.168.49.2 ingress.example.com 问题：\n访问 /hello-app 可以正常显示结果：\n$curl ingress.example.com/hello-app Hello, world! Version: 1.0.0 Hostname: hello-app 但是 /nginx 和 /redis 都显示 404：\n$ curl ingress.example.com/nginx \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;404 Not Found\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;404 Not Found\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.23.1\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 但是直接访问 service 又是通的：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-app-service NodePort 10.103.218.83 \u0026lt;none\u0026gt; 81:30111/TCP 9h kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 14h nginx-service NodePort 10.99.214.102 \u0026lt;none\u0026gt; 80:30222/TCP 9h redis-service NodePort 10.104.106.67 \u0026lt;none\u0026gt; 82:30333/TCP 9h $ minikube ip 192.168.49.2 $ curl 192.168.49.2:30222 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ps: minikube 如果想访问 service，需要将 service 定义为 nodePort 类型，然后通过 minikube ip 命令来获取 minikube 的 ip，然后用 \u0026lt;minikube_IP:nodePort\u0026gt; 的方式进行访问。似乎不能直接在集群内通过 curl ClusterIP 的方式来访问 service。\n解决方法：\n在 ingress.yaml 中添加 nginx.ingress.kubernetes.io/rewrite-target: / 注解\n修改后的 metadata：\nmetadata: name: example-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / 现在试试访问 /nginx：\n$ curl ingress.example.com/nginx \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 可以看到成功使用 ingress 访问了 nginx ，而不是之前的 404。\n但是访问 /redis 会报 502 错误（可能对于 ingress 而言，redis 不是一个好的例子）：\n$ curl ingress.example.com/redis \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;502 Bad Gateway\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;502 Bad Gateway\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 这里暂时不知道什么原因，感觉这里用 redis 来做实践本身也不太合理，毕竟 curl redis 本来就不会正常工作，但是用 redis-cli 通过访问 service 的方式可以正常工作：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-app-service NodePort 10.103.218.83 \u0026lt;none\u0026gt; 81:30111/TCP 12h kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 17h nginx-service NodePort 10.99.214.102 \u0026lt;none\u0026gt; 80:30222/TCP 12h redis-service NodePort 10.104.106.67 \u0026lt;none\u0026gt; 82:30333/TCP 12h $ k8s minikube ip 192.168.49.2 $ redis-cli -h 192.168.49.2 -p 30333 192.168.49.2:30333\u0026gt; keys * 1) \u0026quot;1\u0026quot; 192.168.49.2:30333\u0026gt; 但是用 ingress 的地址访问是不行的：\n$ redis-cli -h ingress.example.com/redis -p 82 Could not connect to Redis at ingress.example.com/redis:82: Name or service not known not connected\u0026gt; $ redis-cli -h ingress.example.com/redis -p 30333 Could not connect to Redis at ingress.example.com/redis:30333: Name or service not known not connected\u0026gt; 这个问题暂时将其搁置，毕竟这里主要还是以学习 ingress 为主。\n特殊的 headless service service 提供了稳定的对外访问服务，同时还提供了负载均衡的能力，每次访问 service，都会将其转发到该 service selector 对应的 pod 中的某一个，具体转发给哪个是由 service 决定的，访问者没有自主选择权，但是如果访问者想自主选择访问哪个 pod，或者想访问该 service 下的所有 pod，此时该如何处理呢？为了解决这类问题，就需要 Headless Service 闪亮登场了。\n疑问：用户自行选择 pod 的场景有哪些？用户需要访问 service 下所有 pod 的场景有哪些？ 疑问：使用 headless service 后，是否代表负载均衡已经失效，需要用户自行实现？ 这个 yaml 会创建一个 nginx rs，以及两个对应的 service，一个是正常的 service，还有一个是 headless 类型的 service：\n（ps：好像如果 pod 指定了 containerPort，那么 service 这边可以不指定 targetPort）\n--- apiVersion: v1 kind: Service metadata: name: nginx-service-headless spec: ports: - port: 80 # 该 service 的端口 #targetPort: 8080 # 转发到 pod 的 8080 端口 clusterIP: None selector: app: nginx --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: ports: - port: 80 # 该 service 的端口 #targetPort: 8080 # 转发到 pod 的 8080 端口 selector: app: nginx --- apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-rs spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 --- 运行：\n$ kubectl apply -f headless_service_test.yaml service/nginx-service-headless unchanged service/nginx-service created replicaset.apps/nginx-rs unchanged $ kubectl get po NAME READY STATUS RESTARTS AGE nginx-rs-f2b4j 1/1 Running 1 (33m ago) 47m nginx-rs-dhxz7 1/1 Running 1 (33m ago) 47m nginx-rs-sn8wk 1/1 Running 1 (33m ago) 47m dnsutils 1/1 Running 1 (33m ago) 36m $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 24h nginx-service-headless ClusterIP None \u0026lt;none\u0026gt; 80/TCP 48s nginx-service ClusterIP 10.43.27.242 \u0026lt;none\u0026gt; 80/TCP 29s 测试一下普通的 service 能否访问：\n$ curl 10.43.27.242 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 因为 headless service 明确指定了 clusterIP 为 None，所以不会给他分配一个集群内 IP，所以无法通过 curl 的方式访问。\n使用 nslookup 查看二者的 dns 解析，看看他们两的区别，service 的 dns 格式是 \u0026lt;service name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local，注意这里需要在容器内部执行\n$ kubectl exec -it nginx-rs-dhxz7 -- nslookup nginx-service.default.svc.cluster.local Server:\t10.43.0.10 Address:\t10.43.0.10:53 Name:\tnginx-service.default.svc.cluster.local Address: 10.43.27.242 $ kubectl exec -it nginx-rs-dhxz7 -- nslookup nginx-service-headless.default.svc.cluster.local Server:\t10.43.0.10 Address:\t10.43.0.10:53 Name:\tnginx-service-headless.default.svc.cluster.local Address: 10.42.0.36 Name:\tnginx-service-headless.default.svc.cluster.local Address: 10.42.0.32 Name:\tnginx-service-headless.default.svc.cluster.local Address: 10.42.0.30 发现区别了吗？普通的 service 只返回了一条地址，这条地址正是 service 自身的 ip，而 headless service 返回了 3 条地址，这 3 条地址正是 service 代理的几个 pod 的地址：\n$ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-rs-f2b4j 1/1 Running 1 (40m ago) 54m 10.42.0.36 ubuntu \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-rs-dhxz7 1/1 Running 1 (40m ago) 54m 10.42.0.30 ubuntu \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-rs-sn8wk 1/1 Running 1 (40m ago) 54m 10.42.0.32 ubuntu \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; dnsutils 1/1 Running 1 (40m ago) 43m 10.42.0.31 ubuntu \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ","date":"2022年05月08日","permalink":"/posts/k8s-service/","summary":"什么是 service pod 往往需要与集群内的其他 pod 进行通信，可能也有外部的客户端需要访问集群中的 pod，如果需要管理员手动将 pod 的地址提供给访问者，那就太麻烦了，而且在 k8s 中 pod 的地址是不确定的、变动的：pod 可能被 rc、rs 缩减，或者发生异常下线，或者节点异常。所以 k8s 需要提供一种统一的对外访问的资源类型，也就是 service。","title":"k8s service"},{"contents":"multipass 是一款比较轻量级的虚拟机方案，使用起来也比较方便，操作方式与容器类似，但是使用过程中发现没有直接提供修改配置的简单方法，比如默认创建的虚拟机硬盘大小只会分配 5G，如果要跑 k8s 之类的显然是不够用的（k3s 都不够），而且默认创建的 CPU 只有 1 核，连 minikube 都跑不起来（最低需要 2 核），所以就需要修改配置。\n在 github 上找到的解决方案（适用于 m1 mac）：\n# 停止 multipass $ sudo launchctl unload /Library/LaunchDaemons/com.canonical.multipassd.plist # 修改配置文件（CPU 对应 num_cores） $ sudo vim /var/root/Library/Application\\ Support/multipassd/qemu/multipassd-vm-instances.json # 启动 multipass $ sudo launchctl load /Library/LaunchDaemons/com.canonical.multipassd.plist 参考：\nhttps://github.com/canonical/multipass/issues/1158\n更新：\nCPU 能更改成功，但是修改内存无法生效，这玩意还是不太好用\n","date":"2022年05月08日","permalink":"/posts/multipass-xiu-gai-pei-zhi/","summary":"multipass 是一款比较轻量级的虚拟机方案，使用起来也比较方便，操作方式与容器类似，但是使用过程中发现没有直接提供修改配置的简单方法，比如默认创建的虚拟机硬盘大小只会分配 5G，如果要跑 k8s 之类的显然是不够用的（k3s 都不够），而且默认创建的 CPU 只有 1 核，连 minikube 都跑不起来（最低需要 2 核），所以就需要修改配置。","title":"multipass 修改配置（内存、硬盘、CPU 等）适用于 m1 mac"},{"contents":"问题重现 有如下 yaml：\napiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 8080 hostPort: 9527 此时 curl nodeIP:9527 会发现报 Connection refused 错误，使用 kubectl exec -it nginx -- curl localhost:8080 也是如此\n修改 apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 hostPort: 9527 将 containerPort 改为 80，这是 nginx 的默认监听端口。\n此时从浏览器访问 NodeIP:9527 发现可以显示 nginx 的主页，使用 kubectl exec -it nginx -- curl localhost:80 也可以正常工作，curl localhost:9527 也同样可以正常工作。但是 kubectl exec -it nginx -- curl localhost:9527 会报 Connection refused 错误。\n🤔 从上面的例子中可以得出：hostPort 是主机对外暴露的 IP 地址，其指向容器内部的 containerPort，在 修改 的例子中，意义为：对外暴露 9527 端口，该端口指向 nginx 容器内部的 80 端口，所以访问主机的 9527 相当于访问 nginx 容器的 80 端口，从 kubectl exec -it nginx -- curl localhost:9527 报 Connection refused 可以得出，容器内部是没有监听 9527 这个端口的。\n在 stackoverflow 上找到了两个同样的问题：\nhttps://stackoverflow.com/questions/69282237/nginx-pod-not-taking-specified-port\n回答：\nNGINX Docker image uses port 80 to listen for HTTP connections by default. containerPort is the port which you expose your application to external traffic.\nNGINX Docker 镜像默认使用 80 端口来监听 HTTP 连接。容器端口是您将应用程序公开给外部流量的端口。\nhttps://stackoverflow.com/questions/66526811/unable-to-curl-pod-ip-using-containerport\n回答：\nBy default, nginx server listen to the port 80. You can see it in their docker image ref.\nWith kubectl run nginx --image=nginx --port=8888 what you have done here is you have expose another port along with 80. But the server is still listening on the 80 port.\nSo, try with target port 80. For this reason when you tried with other than port 80 it\u0026rsquo;s not working. Try with set --target-port=8888 to --target-port=80.\nOr, If you want to change the server port you need to use configmap along with pod to pass custom config to the server.\n默认情况下，nginx 服务器监听 80 端口。你可以在他们的 docker 镜像中看到它 [ref](https://hub.docker.com/layers/nginx/library/nginx/1.18.0 - alpine/images /sha256 - d7038eae37cfa36cd8e286f6d6daf0df7a445a2da327517b3cde4ba1833adc0c?context=explore)。 使用 kubectl run nginx - image=nginx - port=8888 你在这里所做的是你已经暴露了另一个端口以及 80。但是服务器仍在监听 80 端口。 因此，请尝试使用目标端口“80”。因此，当您尝试使用端口“80”以外的其他端口时，它不起作用。尝试将-target-port=8888设置为-target-port=80。 或者，如果要更改服务器端口，则需要使用 configmap 和 pod 将自定义配置传递给服务器。\n那么 containerPort 这个参数到底是什么作用呢？\n查阅资料，有这几种说法：\ncontainerPort 是 pod 中的容器需要暴露的端口\ncontainerPort 是容器内部的 port\nports : containerPort List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses,but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default \u0026ldquo;0.0.0.0\u0026rdquo; address inside a container will be accessible from the network. Cannot be updated.\ncontainerPort 是要从容器公开的端口列表。在此处公开端口可为系统提供有关容器使用的网络连接的附加信息，但主要是信息性的。此处不指定端口不会阻止该端口被暴露。任何侦听容器内默认“0.0.0.0”地址的端口都可以从网络访问。无法更新。（机翻，来自 https://faun.pub/should-i-configure-the-ports-in-kubernetes-deployment-c6b3817e495）\n从上面这些说法大致了解了，nginx 容器会默认监听 80 端口，而我指定的 containerPort 实际上不会改变 nginx 监听的端口，所以如果将其指定为非 80 端口，比如 8080，那么 hostPort 会访问容器内的 8080 端口，但 8080 没有被任何程序监听，所以会产生 Connection refused 错误。就和上面所说的，containerPort 主要是信息性的，它只是注明了容器对外暴露的端口，不会实际对容器造成影响。\n","date":"2022年05月08日","permalink":"/posts/k8s-nginx-pod-de-containerport-wen-ti/","summary":"问题重现 有如下 yaml：\napiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 8080 hostPort: 9527 此时 curl nodeIP:9527 会发现报 Connection refused 错误，使用 kubectl exec -it nginx -- curl localhost:8080 也是如此","title":"k8s nginx pod 的 containerPort 问题"},{"contents":"顾名思义，daemon 代表守护程序，所以 DaemonSet （简称 ds）会在每个节点上运行一个专门的 pod，这个 pod 是特殊的，比如资源监控器或者日志收集器，或者 k8s 自己的 kube-proxy。与 rc 或者 rs 不同，这两个会随机地分布在整个集群中，比如副本数量为 5，一共有 4 个节点，可能会在节点 1 创建两个副本，节点 2 创建 1 个副本，节点 4 创建 2 个副本，但 DaemonSet 会保证这 4 个节点各自都有一个副本。\napiVersion: apps/v1 kind: DaemonSet metadata: name: ssd-monitor spec: selector: matchLabels: # DaemonSet 会对标签为 app=ssd-monitor 的 pod 进行管理 app: ssd-monitor template: metadata: labels: # 这个标签不能省略，且必须与上面的 matchLabels 相同，否则 create 会报错 app: ssd-monitor spec: containers: - name: main image: nginx:alpine ps：因为书上提供的镜像都是 x86 的，在我的机器上无法运行，所以把镜像替换为了 nginx，反正只是学习，也无所谓了\n书上的 template.spec 下其实还有一个 nodeSelector，值为 disk=ssd，代表该 ds 只会在标签为 disk=ssd 的节点上创建、管理 pod，这里偷懒了没写，不然还要给节点添加标签。\n如果将一个已经运行 ds 创建的 pod 的节点的标签改掉，比如上面的例子中，将节点的标签 disk=ssd 改为 dsik=hdd，那么先前已经创建好的 ds pod 会被终止。\n","date":"2022年05月07日","permalink":"/posts/k8s-daemonset/","summary":"顾名思义，daemon 代表守护程序，所以 DaemonSet （简称 ds）会在每个节点上运行一个专门的 pod，这个 pod 是特殊的，比如资源监控器或者日志收集器，或者 k8s 自己的 kube-proxy。与 rc 或者 rs 不同，这两个会随机地分布在整个集群中，比如副本数量为 5，一共有 4 个节点，可能会在节点 1 创建两个副本，节点 2 创建 1 个副本，节点 4 创建 2 个副本，但 DaemonSet 会保证这 4 个节点各自都有一个副本。","title":"k8s DaemonSet"},{"contents":" 本篇笔记摘自 《Kubernetes in Aciton 》\n介绍 ReplicaSet（简称 rs） 是 ReplicationController （简称 rc）的升级版，它的标签选择器功能更加强大。\n标签选择器 使用 selector.matchExpressions 进行标签选择，比如：\nselector: matchExpressions: - key: app\t# 此选择器要求该 pod 包含名为 “app” 的标签 operator: In values: - kubia\t# 标签的值必须是 \u0026quot;kubia\u0026quot; 其中，key 和 operator 是必须的，values 可能为空，也可能为多个值，运算符有以下几个：\nIn : Label 的值必须与其中一个指定的 values 匹配。 Notln : Label 的值与任何指定的 values 不匹配。 Exists : pod 必须包含一个指定名称的标签(值不重要)。使用此运算符时，不应该指定 values 字段。 DoesNotExist : pod 不得包含有指定名称的标签。values 属性不得指定 。不应指定 values字段。 DoesNotExist : pod不得包含有指定名称的标签。values属性不得指定 。 如果你指定了多个表达式，则所有这些表达式都必须为 true 才能使选择器与 pod 匹配。如果同时指定matchLabels 和 matchExpressions，则所有标签都必须匹配，并且所有表达式必须计算为 true 以使该 pod 与选择器匹配。\n例子：\nselector: matchLabels: component: redis matchExpressions: - {key: tier, operator: In, values: [cache]} - {key: environment, operator: NotIn, values: [dev]} 测试 rs 是否会抢占 rc 的 pod 现在有 3 个 rc 创建的 pod，标签都为 app=kubia，容器为 nginx、数量为 3 ，如果此时使用 rs 创建条件相同的 pod 会怎样？会直接将 rc 的 3 个容器抢过来吗？\nrs 的 yaml 如下：\nkubia-replicaset.yaml\napiVersion: apps/v1 kind: ReplicaSet metadata: name: kubia spec: replicas: 3 selector: matchLabels: app: kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: nginx:alpine ports: - containerPort: 8080 创建 rs：\n$ kubectl apply -f kubia-replicaset.yaml 查看 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-qb26r 1/1 Running 0 13h kubia-d9xc6 1/1 Running 0 13h kubia-dg2j8 1/1 Running 0 17m kubia-vdrf6 1/1 Running 0 10m kubia-cq4wt 1/1 Running 0 10m kubia-qx467 1/1 Running 0 10m 发现 k8s 又新创建了 3 个 pod，通过 kubectl describe 可以看到每个 pod 的管理者，kubia-qb26r 显示的是：Controlled By: ReplicationController/kubia，kubia-qx467 显示的是：Controlled By: ReplicaSet/kubia，说明 rs 不会抢占 rc 的 pod。\n脱离 rc 管理的 pod 会被 rs 接管吗 还是和上面的条件一样，有 3 个 rc 创建的 pod，并且标签都为 app=kubia，先删除之前创建的 rs，这会一并删除其创建的 3 个 pod：\n$ kubectl delete rs kubia replicaset.apps \u0026quot;kubia\u0026quot; deleted $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-qb26r 1/1 Running 0 13h kubia-d9xc6 1/1 Running 0 13h kubia-dg2j8 1/1 Running 0 20m 删除之前的 rc，注意添加 \u0026ndash;cascade= false 来避免删除 pod（有个警告，意思 \u0026ndash;cascade=false 被废弃了，应该替换为 cascade=orphan，查了下 orphan 是孤儿的意思，挺贴切的）\n$ kubectl delete rc kubia --cascade=false warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan. replicationcontroller \u0026quot;kubia\u0026quot; deleted $ kubectl get po NAME READY STATUS RESTARTS AGE kubia-d9xc6 1/1 Running 0 13h kubia-dg2j8 1/1 Running 0 28m kubia-qb26r 1/1 Running 0 13h 再创建之前的 rs\n$ kubectl apply -f kubia-replicaset.yaml replicaset.apps/kubia created $ kubectl get po NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 20h kubia-dg2j8 1/1 Running 0 30m kubia-d9xc6 1/1 Running 0 13h kubia-qb26r 1/1 Running 0 13h $ kubectl describe pod kubia-dg2j8 Name: kubia-dg2j8 Labels: app=kubia # 省略 Controlled By: ReplicaSet/kubia # 省略 可以看到 rs 会直接接管这 3 个脱离 rc 管理的 pod。\n","date":"2022年05月07日","permalink":"/posts/k8s-replicaset/","summary":"本篇笔记摘自 《Kubernetes in Aciton 》","title":"k8s ReplicaSet"},{"contents":" 本篇笔记摘自 《Kubernetes in Aciton 》\nReplicationController 通过标签选择器（selector）与 pod 相关联。可以通过修改一个 pod 的标签将其移出 ReplicationController 的管理，或者移入到另一个 ReplicationController。\n查看所有的 ReplicationController:\n$ kubectl get rc NAME DESIRED CURRENT READY AGE kubia 2 2 2 39m 实验 按照书上的 yaml：\napiVersion: v1 kind: ReplicationController metadata: name: kubia spec: replicas: 3 selector: app: kubia template: metadata: labels: app: kubia\t# 这里需要与 selectro 中的标签保持一致 spec: containers: - name: kubia image: luksa/kubia ports: - containerPort: 8080 发现无法运行 pod：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kubia-25mrj 0/1 CrashLoopBackOff 22 (3m22s ago) 92m kubia-x4dd8 0/1 CrashLoopBackOff 22 (3m11s ago) 92m kubia-8gdfz 0/1 CrashLoopBackOff 22 (2m45s ago) 92m 打印一下日志看看：\n$ kubectl logs kubia-25mrj standard_init_linux.go:228: exec user process caused: exec format error 查了一下，貌似是因为 luksa/kubia 这个镜像是 x86 的，而我的电脑是 arm 的，所以导致容器无法运行\n可以将其换成 nginx:alpine\nkubectl get po NAME READY STATUS RESTARTS AGE kubia-fcf4d 1/1 Running 0 16s kubia-d9xc6 1/1 Running 0 16s kubia-qb26r 1/1 Running 0 16s 尝试删除一个 pod：\n$ kubectl delete pod kubia-fcf4d pod \u0026quot;kubia-fcf4d\u0026quot; deleted 再次查看 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 7h12m kubia-d9xc6 1/1 Running 0 3m6s kubia-qb26r 1/1 Running 0 3m6s kubia-6lp8k 1/1 Running 0 23s 发现 ReplicationController 又新创建了一个 pod，在最下方的 kubia-6lp8k\n修改标签实验 先看一下当前的 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-d9xc6 1/1 Running 0 20m kubia-qb26r 1/1 Running 0 20m kubia-6lp8k 1/1 Running 0 17m 修改一个 pod 的 label：\n$ kubectl label po kubia-d9xc6 app=kubia1 --overwrite pod/kubia-d9xc6 labeled 再次查看 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-qb26r 1/1 Running 0 21m kubia-6lp8k 1/1 Running 0 18m kubia-d9xc6 1/1 Running 0 21m kubia-mnfft 1/1 Running 0 4s 发现此时又新创建了一个 pod\n使用 kubectl describe 查看 kubia-d9xc6，发现 Controlled By: ReplicationController/kubia 这句话消失了，说明该 pod 当前已经不由任何 ReplicationController 管理了。\n问题来了，如果把 kubia-d9xc6 的标签再改回去会怎么样？\n$ kubectl label po kubia-d9xc6 app=kubia --overwrite pod/kubia-d9xc6 unlabeled 这个 unlabeled 没太搞懂，但是 describe 查看发现还是更改成功了。\n此时再查看 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-qb26r 1/1 Running 0 26m kubia-6lp8k 1/1 Running 0 24m kubia-d9xc6 1/1 Running 0 26m 发现先前新创建的 pod 又被删除了。\n看来 ReplicationController 会始终保证 pod 符合指定数量，多删少补。\n水平缩放 可以改变 spec.replicas 来变更 pod 的数量：\n$ kubectl edit rc kubia 此时会打开一个 vim，将 spec.replicas 修改为 2，esc + :wq 退出，退出后 k8s 会自动更新，输出：replicationcontroller/kubia edited，此时再查看 pod：\n$ kubectl get po NAME READY STATUS RESTARTS AGE kubia-qb26r 1/1 Running 0 37m kubia-d9xc6 1/1 Running 0 37m 可以看到 pod 的数量已经缩减到 2 个了\n同样也可以用以下命令：\n$ kubectl scale rc kubia --replicas=2 删除 rc 使用下面的命令进行删除：\n$ kubectl delete rc [rcName] 删除 rc 会一并删除其所管理的 pod，不过不想这么做，则需要添加 --cascade=false 选项。\n","date":"2022年05月06日","permalink":"/posts/k8s-replicationcontroller/","summary":"本篇笔记摘自 《Kubernetes in Aciton 》","title":"k8s ReplicationController"},{"contents":" 本篇笔记摘自 《Kubernetes in Aciton 》4.1 保持 pod 健康\n基本概念 k8s 会 检测 pod 中的容器 是否正常工作，如果容器的主线程奔溃，那么 k8s 会自动重启该容器，对于开发者而言，无需对应用程序任何额外操作便获得了自动修复的能力，但有时容器没有奔溃也不一定就代表正常，比如遇到了死锁问题，此时虽然容器不会奔溃，但是整个程序已经完全卡死无法继续工作了，针对这种情况，k8s 提供了一个 存活探针 功能。\n存活探针 会检查容器是否正常工作，如果探测失败， k8s 将定期执行探针并重新启动容器。\n存活探针 有以下几种机制：\nHTTP，对容器进行 HTTP 访问（端口和路由需要自己指定），根据返回码来判断是否正常工作（2xx，3xx代表正常，其他代表错误），如果长时间未响应也同样代表无法正常工作。\nTCP socket，对容器的指定端口建立 TCP 连接，如果建立成功则代表存活，否则容器重新启动。\nExec，在容器内部执行任意命令，并检查命令的退出状态码。如果状态码是 0, 则探测成功。所有其他状态码都被认为失败。\n实践 使用 http 方式 实践会创建一个基于 HTTP 的存活探针。\nyaml 文件如下：\napiVersion: v1 kind: Pod metadata: name: kubia-liveness spec: containers: - image: luksa/kubia-unhealthy # 这个镜像包含了不知道怎么坏掉的应用 name: kubia livenessProbe: httpGet: # 一个 HTTP GET 存活探针 path: / # HTTP 请求的路径 port: 8080 # 探针连接的网络端口 创建后，查看 pod 状态：\n$kubectl get pods NAME READY STATUS RESTARTS AGE kubia-liveness 0/1 CrashLoopBackOff 16 (21s ago) 60m RESTARTS 代表重启次数，上面重启了 16 次，说明 k8s 一直在尝试自动修复，只是这个镜像会在重启后不久坏掉，导致一直在重启（为了演示刻意而为之，这个镜像会正确处理前 5 个 HTTP 请求，之后每个请求都会返回 500，导致探针检测失败而重启）\n注意：当容器被强行终止时，会创建一个全新的容器——而不是重启原来的容器。\n疑问： 官方文档里说，http 存活指针的判断依据是：返回大于或等于 200 并且小于 400 的任何代码都标示成功，其它返回代码都标示失败。那如果整个容器都挂掉了，不会返回任何状态码，此时会如何处理呢？是超时未收到回复后就自动判定为未存活吗？这个需要自己实践一下（ps：官方文档包括一些书籍里面提供了现成的镜像，但是这些镜像基本都是 amd64 的，在我的 m1 上无法成功运行，所以有点麻烦，需要自己写代码，然后 docker build，再 push 到 hub，）\n如果 pod 中有多个容器，如何检测 可以使用存活探针，为每个容器设置不同的探测方案，比如可以使用 tcp 探针，让 pod 中的每个容器暴露一个端口，然后存活探针分别尝试与这些端口建立连接，如果其中有一个端口建立连接失败，则说明容器出现了问题，然后 k8s 就会删除这个有问题的 pod，并新建一个相同的 pod 替代。\n待实践\n查看 pod 探针 $ kubectl describe pod kubia-liveness Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3 delay=0s 表示在容器启动后立即开始探测，timeout=1s 表示容器必须在 1s 内对探针做出响应，period=10s 代表每 10s 进行一次探测，failure=3 表示累计失败 3 次则视为容器无法正常工作。success=1 书中没有说明，不过应该可以猜到是代表探针成功一次便视为容器可以正常工作。\n对于一些比较重的应用，启动可能需要较长的时间，此时将 delay 设置为 0 就不太合适了，可能程序都还没跑起来，探针却已经探测过好几次了，导致错误的重启当前容器。通过 livenessProbe 下的 initialDelaySeconds 进行设置。\n","date":"2022年05月06日","permalink":"/posts/k8s-cun-huo-tan-zhen/","summary":"本篇笔记摘自 《Kubernetes in Aciton 》4.","title":"k8s 存活探针"},{"contents":" 本篇笔记摘自 《Kubernetes in Aciton 》第三章：pod：运行于 Kubernetes 中的容器\n什么是 pod pod 是 k8s 的最基本构建模块，他是多个容器的集合（当然也可以只有1个容器），一个 pod 里的所有容器都会运行在同一个节点上，不存在一个 pod 中的容器 A 在节点 1 运行，容器 B 在节点 2 运行这种情况。\n为何不将所有进程放到一个容器中？\n容器被设计为每个容器只运行一个进程(除非进程本身产生子进程)。 如果在单个容器中运行多个不相关的进程，那么保持所有进程运行、 管理它们的日志等将会是我们的责任。 例如，我们需要包含一种在进程崩溃时能够自动重启的机制。同时这些进程都将记录到相同的标准输出中， 而此时我们将很难确定每个进程分别记录了什么。\n综上所述， 我们需要让每个进程运行在自己的容器中， 而这就是 Docker 和 Kubernetes 期望使用的方式。\n隔离性：pod 中的容器互相之间并不是完全隔离的，它们共享相同的主机名和网络接口，也能使用 ipc 进行通信。进程和文件系统也可以共享，但这两个需要做一些额外的操作。因为共享网络接口，所以也共享相同的 IP 地址和端口，这代表一个 pod 中不同容器的进程不能绑定到同一个端口号，此外容器可以通过 localhost 与同一 pod 中的其他容器进行通信。\npod 间通信：k8s 集群中的所有 pod 都在同一个共享网络地址空间中，所以 pod 间可以通过 IP 进行相互访问\n总结：pod 类似于一台物理机或者虚拟机，只是其每个进程都封装在一个容器中，同时比较轻量，开销小\n通过 pod 合理管理容器 每个 pod 只包含紧密相关的组件或进程 pod 有一条总的原则：应该将应用程序组织到多个 pod 中， 而 每个 pod 只包含紧密相关的组件或进程，而不是将所有程序都放到一个 pod。\n原因有几下几点：\n最大程度利用节点，比如现在有一个前端应用和一个后端应用，以及一个双节点的 k8s 集群，此时应该将前端 pod 放到一个节点，后端 pod 放到另一个节点，而不是将前后端放到一个 pod 中。 方便扩缩容，pod 是 k8s 的扩缩容基本单位，它只能扩缩整个 pod，不能扩缩某个容器，如果一个 pod 中同时存放了前后端程序，那么如果发生扩容，会导致前后端同时扩容，但可能我只想让后端进行扩容（这是很正常的，毕竟前后端的需求不同）。 何时在 pod 中使用多个容器 举个例子：有两个容器，一个负责对外提供文件访问服务，一个负责从外部下载资源到文件（sidecar容器），这两个容器间是紧密关联的，此时便可以放到一个 pod 中。\n如果一个 pod 中存在多个容器，而其中有一个容器挂掉了，此时会如何处理？ 可以使用存活探针，为每个容器设置不同的探测方案，比如可以使用 tcp 探针，让 pod 中的每个容器暴露一个端口，然后存活探针分别尝试与这些端口建立连接，如果其中有一个端口建立连接失败，则说明容器出现了问题，然后 k8s 就会删除这个有问题的 pod，并新建一个相同的 pod 替代。\n相关命令 查看一个 pod 的 yaml 定义\n$ kubectl get po [pod name] -o [yaml|json] -o 可以输入 yaml 或者 json\n查看 pod yaml 的字段含义\n$ kubectl explain pods 创建 pod\n$ kubectl [create|apply] -f [srcFile] 通过 srcFile 来创建 pod，该文件可以是 yaml 或者 json\n**查看当前的 pod **\n$ kubectl get pods 获取 pod 日志\n$ kubectl logs [podName] 获取指定容器的日志\n$ kubectl logs [podName] -c [containerName] 更新 pod\n修改 yaml 后重新执行\n$ kubectl apply -f [srcFile] 这只适用于更改 Pod 中容器（包括工作容器与初始化容器）的镜像，以及 activeDeadlineSeconds （对 Job 类型的 Pod 定义失败重试的最大时间）， tolerations （Pod 对污点的容忍）\n如果是其他情况，则需要删除后重新创建容器\nKubernetes 对 Pod 的更新做了限制，除了更改 Pod 中容器（包括工作容器与初始化容器）的镜像，以及 activeDeadlineSeconds （对 Job 类型的 Pod 定义失败重试的最大时间）， tolerations （Pod 对污点的容忍），修改其它部分将不会产生作用，如我们可以尝试在前面 Pod 定义文档 pod-test.yaml 中将宿主机端口 8081 改为 8082，重新执行 kubectl apply， 将提示如下错误：\nThe Pod \u0026quot;nginx\u0026quot; is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds`, `spec.tolerations` (only additions to existing tolerations) or `spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative) 删除 pod\n$ kubectl delete [podName] pod 提供对外访问 对外访问 pod 的最普遍方式是使用 service，但 service 是一个相当重要的内容，所以会另起一篇笔记，在本篇笔记中只会记录除 service 外的方式，这些方式并不常用，仅作为记录。\nhostNetwork 在 yaml 中添加 hostNetwork: true 字段：\napiVersion: v1 kind: Pod metadata: name: nginx spec: hostNetwork: true containers: - name: nginx image: nginx 执行 yaml：\n$ kubectl apply -f hostnetwork.yaml 查看 pod：\n$ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 18m 192.168.49.2 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 此时通过 pod IP 就可以进行访问了：\n$ curl 192.168.49.2 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; HostPort 参考 pod 实战 部分\nport-forward 使用 kubectl 自带的端口转发命令：\n$ kubectl port-forward [podName] [对外暴露的端口]:[pod内部端口] 比如在我的电脑上实践（访问这个 pod 会输出一句谚语，参考书上的 p168）：\n$ kubectl port-forward fortune 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 此时上面的 shell 会阻塞，新开一个 shell，执行 curl：\n$ curl http://localhost:8080 Q:\tHow many psychiatrists does it take to change a light bulb? A:\tOnly one, but it takes a long time, and the light bulb has to really want to change. 🤔️ 疑问：\n1）curl 必须要添加 http 协议才行，如果只是 curl localhost 8080 会报错：curl: (7) Failed to connect to localhost port 80: Connection refused，这是为什么？\n😅 格式错误，应该是 curl localhost:8080，curl http://localhost:8080 和 curl localhost:8080 实际是一样的\n2）此外，我在宿主机用 curl 虚拟机IP:8080 也无法成功访问，只有在虚拟机内部才行\npod 实战 通过实战创建一个 nginx pod\nnginx yaml 如下：\napiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 hostPort: 8080 创建 pod\n$ kubectl apply -f nginx_pod.yaml 等待一会，直到创建完成\n$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx 0/1 ContainerCreating 0 2s NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 2m17s 在 yaml 中设置了 hostPort 为 8080，代表对外暴露 8080 端口，映射到容器内部的 80 端口，此时便可以通过 IP:8080 进行访问了，如果不设置 hostPort，那么需要使用 kubectl exec -it nginx sh 进入容器内部，执行 curl localhost，可以看到 nginx 欢迎页的 HTML 代码。\n标签 标签用来为 pod 进行分类。\n有几种方式可以为 pod 创建标签：\n在 yaml 中的 metadata labels 中指定，标签是 key:value 的方式，key 需要全局唯一，比如在上面 nginx yaml 中，添加了一个 app: nginx 标签。 使用 kubectl label po nginx key=value 添加标签 pod 的 key 必须唯一，否则报错：\n$ kubectl label po nginx key=value1 error: 'key' already has a value (value), and --overwrite is false 查看 pod 的所有标签：\n$ kubectl get po --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx 1/1 Running 0 118m app=nginx,key=value 查看 pod 的某个标签：\n$ kubectl get po -L app,key NAME READY STATUS RESTARTS AGE APP KEY nginx 1/1 Running 0 120m nginx value 修改 pod 的标签：\n$ kubectl label po nginx key=value1 --overwrite pod/nginx labeled $ kubectl get po --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx 1/1 Running 0 122m app=nginx,key=value1 指定 --overwrite 进行修改\n将 pod 调度到特定节点 可以为某个 node 设置一个标签，然后在 pod yaml 中指定 spec.nodeSelector ，保证 pod 被调度到符合其需求的节点。\n通过以下命令为 node 设置标签：\n$ kubectl label node [nodeName] [key=value] pod yaml 指定被调度节点：\napiVersion: vl kind: Pod metadata: name: kubia-gpu spec: nodeSelector: gpu: \u0026quot;true\u0026quot; containers: - image: luksa/kubia name: kubia 以上会保证该 pod 被调度到 gpu=true 的节点上，通过 nodeSelector 指定。\npod 如何调度 之前实验使用的都是单节点方式，所以没在意 pod 是如何调度的，后来用 kind 搭建了一个 3 节点的 k8s 集群后才意识到这个问题，所以做了一个实验：\n当前集群的 node：\n$ kubectl get node -o wide | awk '{printf \u0026quot;%-30s%-15s\\n\u0026quot;, $1, $6}' NAME INTERNAL-IP kind-control-plane 172.18.0.2 kind-worker 172.18.0.4 kind-worker2 172.18.0.3 创建 3 个 nginx pod（使用 deployment）\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: name: nginx labels: app: nginx spec: containers: - image: nginx name: nginx 查看 po：\n$ kubectl get po -o wide | awk '{printf \u0026quot;%-30s%-15s\\n\u0026quot;, $1,$9}' NAME NODE nginx-66b9cc684d-8l7hg kind-worker2 nginx-66b9cc684d-flj48 kind-worker nginx-66b9cc684d-hxzc9 kind-worker 发现这三个 pod 都被调度到 worker 节点，没有被调度到 kind-control-plane 节点，网上查阅资料，得知 pod 的调度是由 kube-scheduler 来执行的，有一系列规则来完成这件事，比如官方文档里所描述的：\nkube-scheduler 给一个 pod 做调度选择包含两个步骤：\n过滤 打分 过滤阶段会将所有满足 Pod 调度需求的 Node 选出来。 例如，PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求。 在过滤之后，得出一个 Node 列表，里面包含了所有可调度节点；通常情况下， 这个 Node 列表包含不止一个 Node。如果这个列表是空的，代表这个 Pod 不可调度。\n在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。 根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。\n最后，kube-scheduler 会将 Pod 调度到得分最高的 Node 上。 如果存在多个得分最高的 Node，kube-scheduler 会从中随机选取一个。\n鉴于目前实力有限，无法摸清具体的流程，待我日后研究再详细展开（）\n","date":"2022年05月04日","permalink":"/posts/k8s-pod-bi-ji/","summary":"本篇笔记摘自 《Kubernetes in Aciton 》第三章：pod：运行于 Kubernetes 中的容器","title":"k8s pod"},{"contents":"这是一个困扰我已久的问题，当选中一段代码并继续向下滑动时，滑动速度会非常快，导致无法准确选中我想要的部分，比如我的屏幕能显示200行代码，我想选中1-250行的代码，这就需要我向下滑动来选中剩余的50行，但是因为速度太快，导致我轻轻一滑就会瞬间跑到第800行代码的位置，于是我又要向上滑动，结果又瞬间到了第150行，非常令人蛋疼。\n在网上用中文搜索不到相关的问题，没办法只能用我蹩脚的英文搜索 vscode scroll too fast selecting，没想到还找出来不少，看来有时候还是得用英文搜索才能解决问题。https://github.com/microsoft/vscode/issues/40890 这里描述了同样的问题，但是里面的解决方案都对我无效，像 \u0026quot;editor.smoothScrolling\u0026quot;: true，editor.mouseWheelScrollSensitivity:0.1 这些都不行，似乎 vscode 本身也没有提供一个专门针对选中后滑动速度的选项。\n最后在 https://stackoverflow.com/questions/44374762/visual-studio-code-scroll-speed-too-fast-for-selecting-with-mouse 这里找到了一个将将就的解决方案:\nWhile not a solution to the scroll speed issue, there is a rather trivial workaround. Select part of the region you want selected, scroll down to the point the region to select ends and Shift-Click. Voila, whole region selected.\n意思就是先选中一小部分文字，然后松开鼠标/触控板，然后向下滑动到要选择内容的末尾，再 shift + 左键点击此处，即可完成区域选择。\n","date":"2022年04月10日","permalink":"/posts/vscode-xuan-zhong-wen-ben-shi-hua-dong-su-du-guo-kuai/","summary":"这是一个困扰我已久的问题，当选中一段代码并继续向下滑动时，滑动速度会非常快，导致无法准确选中我想要的部分，比如我的屏幕能显示200行代码，我想选中1-250行的代码，这就需要我向下滑动来选中剩余的50行，但是因为速度太快，导致我轻轻一滑就会瞬间跑到第800行代码的位置，于是我又要向上滑动，结果又瞬间到了第150行，非常令人蛋疼。\n在网上用中文搜索不到相关的问题，没办法只能用我蹩脚的英文搜索 vscode scroll too fast selecting，没想到还找出来不少，看来有时候还是得用英文搜索才能解决问题。https://github.","title":"vscode 选中文本时滑动速度过快"},{"contents":"安装： curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --write-kubeconfig-mode 666 \u0026ndash;write-kubeconfig-mode设置读写权限，否则每次 kubectl 都要 sudo，同时使用国内镜像进行下载\n设置镜像 参考 https://blog.csdn.net/xs20691718/article/details/106515605\n# 1 cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl # 拷贝一份配置文件 # 2 sudo vim /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl # 在 config.toml.tmpl 文件中添加 [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\u0026quot;docker.io\u0026quot;] endpoint = [\u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;] # 3 重启服务 systemctl restart k3s # 4 查看修改是否成功 sudo crictl info | grep mirror 创建 pod 一直处于 pending 状态 一开始以为是污点问题，因为 pod 描述中有 warning 警告：0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn\u0026rsquo;t tolerate.，使用 kubectl describe node [nodename] 查看，发现节点的污点为 :NoSchedule，而 pod 的 Tolerations 是 :NoExecute，先尝试移除节点的污点，使用 kubectl taint nodes node1 key1:NoSchedule- 进行移除，但是移除后执行 kubectl get no -o yaml | grep taint -A 5 发现依然显示有污点（正常应该输出为空），难道是移除污点失效了吗？网上也查不到任何相关资料，于是又尝试更改 pod 的容忍度，修改 yaml 文件，添加如下内容：\ntolerations: - key: \u0026quot;node.kubernetes.io/disk-pressure\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; 之后再重新执行 kubectl apply -f 重新创建 pod，发现还是无效。最后重启了一下 k3s ，然后莫名其妙的发现 pod 创建成功了，此时再查看 node 描述信息，发现 taint 为 none 了。\n一开始的 pod 描述信息中还有一条错误信息： kubernetes The node was low on resource: ephemeral-storage.” Evicted ，网上找了半天资料，也没有看到很好的解决方法（其实是我看不懂😅）\n​\t====================== 更新 ======================\n破案了，注意 node 的污点 key 是 node.kubernetes.io/disk-pressure，查了一下这表示当前节点的磁盘容量不足，该污点会阻止 pod 被调度到该节点，且无法去除，这就是 pod 一直 pending 的原因，因为我使用的是 multipass 创建的虚拟机，默认只分配的 5G 的磁盘，所以很容易就满了，导致 k3s 出现这一污点，只要重新创建一个虚拟机并分配大一些的磁盘空间即可（也可以调整现有虚拟机的磁盘空间，但是有些麻烦，尝试无果后放弃了）\n老实说第一次用这个 k3s，感觉比 minikube 要难用很多，发生了不少莫名其妙的问题，不过内存占用确实要低得多，暂且先摸索一下吧，刚好还学到了污点这个新知识。\n","date":"2022年04月08日","permalink":"/posts/k3s/","summary":"安装： curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --write-kubeconfig-mode 666 \u0026ndash;write-kubeconfig-mode设置读写权限，否则每次 kubectl 都要 sudo，同时使用国内镜像进行下载","title":"k3s 踩坑记录"},{"contents":"聚簇索引和非聚簇索引 索引类型分为主键索引和非主键索引。\n主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。\n非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。\n比如在上图中，ID 就是聚簇索引，K 是非聚簇索引\n根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？\n如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n回表查询 在上图中，如果执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？\n在 k 索引树上找到 k=3 的记录，取得 ID = 300；\n再到 ID 索引树查到 ID=300 对应的 R3；（因为查询的是 *，所以需要去主键索引处拿到整行的值）\n在 k 索引树取下一个值 k=5，取得 ID=500；\n再回到 ID 索引树查到 ID=500 对应的 R4；\n在 k 索引树取下一个值 k=6，不满足条件，循环结束。\n在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。\n覆盖索引 覆盖索引（covering index）指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。 如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫做覆盖索引。\n比如上面的例子中，如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。\n","date":"2022年03月26日","permalink":"/posts/mysql-suo-yin/","summary":"聚簇索引和非聚簇索引 索引类型分为主键索引和非主键索引。\n主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。\n非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。","title":"MySQL 索引"},{"contents":"这是我在看 Mysql Buffer Pool 时的一个疑问，先说说 Buffer Pool，为了提高性能，Mysql 为读写操作添加了一层缓存，这样就可以直接从内存中操作，速度相比操作硬盘要快很多，只要之后再将内存中的数据定期写入到磁盘中即可保证数据一致。但是这样就产生了一个问题：如果在内存写入到磁盘之前，Mysql 进程挂掉了，这样岂不是就丢失数据了？\n通过网上查阅资料，了解了 Mysql 对于这种情况的解决方法：redo log，对内存中的数据进行的操作，会同步把对应的日志写入到 redo log 中，如果发生了上面的情况，可以通过查看 redo log 进行恢复，但是此时我又产生了几个疑问：\n这样不是还会发生上面的问题吗？如果内存中操作了，然后数据库进程挂掉了，没来得及写入到 redo log，那不是一样也发生丢失了吗？ 引入 buffer pool 就是为了防止直接操作硬盘，现在又要同步写入到 redo log 中，等于还是要操作硬盘，那还有什么意义呢？ 查找的一些说法（不确定正不正确）：\n通过 WAL(Write Ahead Log，预写日志) 机制来保证不会发生上面的情况，即：写入日志一定发生在更新内存之前（关于 WAL 暂时也没有完全理解，所以这里标记为存疑） 写入 redo log 的方式使用了追加操作， 所以磁盘操作是 顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是 随机写（具体可以参见硬盘的顺序读写和随机读写），磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。 ","date":"2022年03月16日","permalink":"/posts/mysql-buffer-pool-de-yi-xie-yi-wen-ji-lu/","summary":"这是我在看 Mysql Buffer Pool 时的一个疑问，先说说 Buffer Pool，为了提高性能，Mysql 为读写操作添加了一层缓存，这样就可以直接从内存中操作，速度相比操作硬盘要快很多，只要之后再将内存中的数据定期写入到磁盘中即可保证数据一致。但是这样就产生了一个问题：如果在内存写入到磁盘之前，Mysql 进程挂掉了，这样岂不是就丢失数据了？","title":"Mysql Buffer Pool 的一些疑问记录"},{"contents":"引子 使用过 Go 的小伙伴应该都遇到过这样一个坑：当使用 slice 作为参数传递时，如果调用方函数内部发生了 append 操作，那么对应的改变不会在实参处体现出来，比如：\npackage main import \u0026quot;fmt\u0026quot; func main() { slice := make([]int, 0, 0) slice = fn(slice) fmt.Println(\u0026quot;[main] slice: \u0026quot;, slice) } func fn(slice []int) (s []int) { s = append(slice, 1) fmt.Println(\u0026quot;[func] slice: \u0026quot;, s) } 输出：\n[func] slice: [1] [main] slice: [] 看过一些博客，里面阐述的原因是：slice 的底层数据结构是一个名为 SliceHeader 的结构体：\ntype SliceHeader struct { Data uintptr Len int Cap int } 其中的 Data 指向底层数组，将 slice 作为函数参数传递，实际上传递的就是这样一个结构体，而 append 可能会导致扩容，也就是重新分配一个更大的数组，将之前的数据拷贝过去，并且重新将 Data 指向这个新数组，因为传入的参数不是指针类型而是值类型，所以 Data 的改变不能体现在实参中。\n听起来有些道理，那么就动手实践一下，看看实际情况如何：\npackage main import ( \u0026quot;reflect\u0026quot; \u0026quot;unsafe\u0026quot; \u0026quot;fmt\u0026quot; ) // 通过反射来获取 slice 对应的底层 SliceHeader 结构体 func getSliceHeader(slice []int) { struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;slice)) fmt.Printf(\u0026quot;%+v\\n\u0026quot;, struc) dataPtr := unsafe.Pointer(struc.Data) fmt.Println(dataPtr) } func sliceAppend(slice []int) { slice = append(slice, 1, 2, 3, 4) fmt.Println(\u0026quot;[func] sliceHeader info: \u0026quot;) getSliceHeader(slice) fmt.Println(\u0026quot;========================\u0026quot;) struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;slice)) dataPtr := unsafe.Pointer(struc.Data) data := *(*[4]int)(dataPtr) fmt.Println(\u0026quot;[func] after append, sliceHeader.data: \u0026quot;, data) } func main() { n := make([]int, 0, 0) fmt.Println(\u0026quot;[main] sliceHeader info: \u0026quot;) getSliceHeader(n) fmt.Println(\u0026quot;========================\u0026quot;) sliceAppend(n) fmt.Println(\u0026quot;[main] after call, slice data: \u0026quot;, n) struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;n)) dataPtr := unsafe.Pointer(struc.Data) data := *(*[4]int)(dataPtr) fmt.Println(\u0026quot;[main] sliceHeader.data\u0026quot;, data) } 上面的代码看起来有些长，实际只是通过反射去拿到 slice 对应的 SliceHeader，并通过 unsafe 来获取 data 对应的底层数组，无需过多关注。\n输出结果：\n[main] sliceHeader info: \u0026amp;{Data:4336299344 Len:0 Cap:0} 0x10276ad50 ======================== [func] sliceHeader info: \u0026amp;{Data:1374390272064 Len:4 Cap:4} 0x140000b4040 ======================== [func] after append, sliceHeader.data: [1 2 3 4] [main] after call, slice data: [] [main] sliceHeader.data [0 0 0 0] main 里的 slice 是一个 len 和 cap 都为 0 的 slice，而函数内部需要 append 4 个元素，这势必会发生扩容，从上面的输出信息中也可以看出，data 指向的地址确实发生了改变，main 中 data 指向 0x10276ad50，而 func 中 data 指向了 0x140000b4040。\n新的疑问 在了解原因后，我又突发奇想，既然是因为扩容才导致的丢失，那我不让他扩容不就好了吗？在 make 时直接指定 cap 为 20，而函数内只 append 4 个元素，这样就不会扩容了，此时结果会如何？\npackage main import ( \u0026quot;reflect\u0026quot; \u0026quot;unsafe\u0026quot; \u0026quot;fmt\u0026quot; ) func getSliceHeader(slice []int) { struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;slice)) fmt.Printf(\u0026quot;%+v\\n\u0026quot;, struc) dataPtr := unsafe.Pointer(struc.Data) fmt.Println(dataPtr) } func sliceAppend(slice []int) { slice = append(slice, 1, 2, 3, 4) fmt.Println(\u0026quot;[func] sliceHeader info: \u0026quot;) getSliceHeader(slice) fmt.Println(\u0026quot;========================\u0026quot;) struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;slice)) dataPtr := unsafe.Pointer(struc.Data) data := *(*[4]int)(dataPtr) fmt.Println(\u0026quot;[func] after append, sliceHeader.data: \u0026quot;, data) } func main() { n := make([]int, 0, 20) fmt.Println(\u0026quot;[main] sliceHeader info: \u0026quot;) getSliceHeader(n) fmt.Println(\u0026quot;========================\u0026quot;) sliceAppend(n) fmt.Println(\u0026quot;[main] after call, slice data: \u0026quot;, n) struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;n)) dataPtr := unsafe.Pointer(struc.Data) data := *(*[4]int)(dataPtr) fmt.Println(\u0026quot;[main] sliceHeader.data\u0026quot;, data) } 输出：\n[main] sliceHeader info: \u0026amp;{Data:1374390001664 Len:0 Cap:20} 0x14000072000 ======================== [func] sliceHeader info: \u0026amp;{Data:1374390001664 Len:4 Cap:20} # 注意这里的 Len 0x14000072000 ======================== [func] after append, sliceHeader.data: [1 2 3 4] [main] after call, slice data: [] [main] sliceHeader.data [1 2 3 4] 可以看到，SliceHeader.Data 的指向确实没有发生改变，都是 0x14000072000，并且通过最后一句输出 [main] sliceHeader.data [1 2 3 4] 可以得知，底层的数组确实已经发生了改变，但是最终 main 的 slice 输出还是为空，这是为什么呢？\n一个猜测：\n我们在 main 函数调用 sliceAppend 之后在加入一段话：\nsliceAppend(n) fmt.Println(\u0026quot;[main] after call, slice data: \u0026quot;, n) struc := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;n)) dataPtr := unsafe.Pointer(struc.Data) data := *(*[4]int)(dataPtr) fmt.Println(\u0026quot;after call, sliceHeader info: \u0026quot;) // 新增 getSliceHeader(n)\t// 新增 输出：\nafter call, sliceHeader info: \u0026amp;{Data:1374390730752 Len:0 Cap:20} data address: 0x14000124000 发现在 append 之后，Len 依然为 0，难道这就是问题所在？fmt.Println 可能是检测到 slice 的 Len 为 0，所以即便底层数组发生了改变，但还是打印出了空切片？（具体还是得看 Println 的源码才能真正知道原因）\n因为传的不是指针类型，所以 Len 的变化不能反映到实参，在之前的输出可以看到，形参的 Len 已经发生了改变：\n[func] sliceHeader info: \u0026amp;{Data:1374390001664 Len:4 Cap:20} 综上所述，以后如果要把 slice 作为参数传递，且调用方要执行 append 操作，直接统一传递指针就好了。\n","date":"2022年03月10日","permalink":"/posts/go-slice-append-de-keng-cun-yi/","summary":"引子 使用过 Go 的小伙伴应该都遇到过这样一个坑：当使用 slice 作为参数传递时，如果调用方函数内部发生了 append 操作，那么对应的改变不会在实参处体现出来，比如：","title":"Go slice append 的坑 [存疑]"},{"contents":"页的概念 线程怎么调度的 进程间通信有哪几种方式，哪种方式最快？ 1.无名管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。\n2.高级管道(popen)：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程，这种方式我们成为高级管道方式。\n3.有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。\n4.消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。\n5.信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。\n6.信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。\n7.共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。\n8.套接字( socket ) ： 套解字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。\n","date":"2022年02月15日","permalink":"/posts/cao-zuo-xi-tong-mian-shi-ti/","summary":"页的概念 线程怎么调度的 进程间通信有哪几种方式，哪种方式最快？ 1.无名管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。","title":"操作系统面试题"},{"contents":" 申明：以下内容来自网上的博客、课程，通过自己的理解进行一下转述，可能存在一些错误和纰漏\n介绍 MySQL 的 可重复读 和 读已提交 是基于 MVCC 实现的，它的最大优点是读不加锁，因此读写不冲突，并发性能好，其实现主要基于以下技术及数据结构：\n首先，InnoDB 里面每个事务有一个唯一的事务ID，叫作 transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。\n同一行记录可能被不同的事务进行操作，这样同一行记录就可能会有多个版本，每个记录都会有以下内容：\n1）隐藏列：InnoDB 中每行数据都有隐藏列，隐藏列中包含了本行数据的事务 id（就是上面提到的，row trx_id）、指向 undo log 的指针等。\n2）基于 undo log 的版本链：前面说到每行数据的隐藏列中包含了指向 undo log 的指针，而每条 undo log 也会指向更早版本的 undo log，从而形成一条版本链。\n比如在上图中，id 为 15 的事务将 k 更新为 10，此时该版本的 row_trx_id 为当前事务 15，undo log 指针指向之前的版本，以此类推，id 为 17 的事务将 k 更新为 11，并重复上面的操作。这样，同一个记录就有了多个版本，并且通过指针形成了一条版本链。\n此外还有一个 Read View 的概念，可以将其理解为一个数据快照，就像相机拍照那样，定格某一时刻的风景。在 读已提交 和 可重复读 这两个隔离级别中就用到了 Read View 这个东西，读提交 隔离级别是在 每次读操作执行前 都会重新生成一个 Read View，而 可重复读 隔离级别是 启动事务时 生成一个 Read View，然后整个事务期间都在用这个 Read View。\nRead View 有四个重要的字段：\nm_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。 min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值，可以称为 低水位。 max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1，可以称为 高水位； creator_trx_id ：指的是创建该 Read View 的事务的事务 id。 当创建出一个 Read View 时，它会根据 select 条件，找到某一行（或者多行）记录，首先找到的是这行的最新版本（如果这里理解不了，请看后面的示例），看一下这个版本的 row trx_id，有以下几种可能：\n如果落在 绿色部分（小于低水位），表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是 可见 的；\n如果落在 红色部分（大于等于高水位），表示这个版本是由将来启动的事务生成的，是肯定 不可见 的；\n如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还 没提交 的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是 **已经提交 **了的事务生成的，可见。\n如果该版本不可见，那么会通过 undolog 指针，继续查看上个版本，重复上面的流程，直到找到一个可见的版本。\nmvcc 示例 举个例子： 假设这三个事务开始前，系统里面还有一个活跃事务， ID 是 99，事务 A、B、C 的事务 ID 分别是100、101、102（事务 ID 按照事务开始时间递增，A 最先执行，所以事务 ID 在三者中最小），且当前系统里只有这四个事务，三个事务开始前，k (1,1）这一行数据的 row trx_id 是 90。\n这样，事务 A 的视图数组就是[99,100], 事务 B 的视图数组是[99,100,101], 事务 C 的视图数组是[99,100,101,102]。\n事务 C 最先将 k 更新为 (1, 2)，此时便产生了一个新的版本，该版本的 row_trx_id 为事务 C 的 id，也就是 102，之后 C 便提交了（事务 C 没有显式地使用 begin/commit，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交）。\n之后，B 进行了更新，此时又产生了一个新的版本，row_trx_id 为 101，但此时 B 还没有 commit，\n之后，A 尝试获取 k 的值，它将从当前版本开始读（也就是这一行的最新值，即 b 更新后的版本），此时 A 的视图数组为 [99, 100]，低水位是数组中的最小值 99，高水位是给下一个事务分配的 ID，也就是 101，查找流程如下：\n找到 (1,3) 的时候，判断出 row trx_id=101，等于高水位大（疑问：如果高水位是数组最大+1，也就是 101，那此时 row trx_id 是等于高水位而不是大于啊？），处于红色区域，不可见； 接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见； 再往前找，终于找到了（1,1)，它的row trx_id=90，比低水位小，处于绿色区域，可见。 这样执行下来，虽然期间这一行数据被修改过，但是事务 A 不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。\n再次理解视图数组的作用 假设有事务 A，id 为 90，事务 B，id 为 91，在 A，B 之前还有一个活跃事务，id 为 80，k 这一行的当前版本事务 id 为 30\n事务 A [80, 90] 事务 B [80, 90, 91] begin set k = k+1 begin get k 前面说到，视图数组保存的是该事务一启动时，当前活跃的事务，事务 A 的 set 操作会产生一个新版本，该版本的 row_trx_id 为 90，当 B 要查询 k 时，先查询当前版本，也就是 A set 后的版本，发现 row_trx_id 为 90，此时事务 B 的低水位是 80（视图数组里的最小值），高水位是 92（91+1，此处存疑），而 row_trx_id 在低水位和高水位之间，那么就进一步查看 B 的视图数组中是否有 90，如果有则说明该事务还未 commit，不能被查看，此时 B 的数组里有 90，所以该版本不能查看，继续通过 undo log指针找到之前的版本，也就是 row_trx_id 为 30 的版本，此时 30 低于低水位，可以查看。这样一套流程下来，就避免了 读未提交 的发生。\n可重复读 和 读已提交 在 MVCC 下实现的区别 可重复读 和 读已提交 都是使用 MVCC 实现的，那么他们有什么区别？ RC（读已提交）与 RR（可重复读） 一样，都使用了MVCC，其主要区别在于：\nRR 是在事务开始后第一次执行 select 前创建 ReadView，直到事务提交都不会再创建。根据前面的介绍，RR 可以避免脏读、不可重复读和幻读。（这里有点疑惑，RR 到底能不能解决幻读问题？看不少博客有说能的，也有说不能的，貌似是在 sql 标准下的 RR 是不能解决幻读的，但是 Innodb 的 RR 使用了 next-key，可以防止幻读）\nRC 每次执行 select 前都会重新建立一个新的 ReadView，因此如果事务 A 第一次 select 之后，事务 B 对数据进行了修改并提交，那么事务 A 第二次 select 时会重新建立新的 ReadView，因此此时事务 B 的修改对事务 A 是可见的。因此 RC 隔离级别可以避免脏读，但是无法避免不可重复读和幻读。\n更新逻辑 对于更新语句，使用的是 当前读，而不是上面的 快照读，select 语句如果加锁，也是当前读，顾名思义，当前读就是获取该值的最新值，比如在下图中，事务 B 读出来的 k 为 3（k 的初值为 1），就是因为当前读的缘故： 这么做的原因是，如果不使用当前读，那会就会发生更新覆盖的情况。\n对于这种情况，事务 C 没有马上提交，那么它会持有锁，直到提交后才会释放锁，在它提交之前，B 尝试更新，但是因为拿不到锁，所以 B 就会阻塞，直到 C commit\n参考：\n极客时间——MySQL 实战45讲 https://www.cnblogs.com/kismetv/p/10331633.html ","date":"2022年02月14日","permalink":"/posts/mvcc-yu-shi-wu-ge-chi/","summary":"申明：以下内容来自网上的博客、课程，通过自己的理解进行一下转述，可能存在一些错误和纰漏\n介绍 MySQL 的 可重复读 和 读已提交 是基于 MVCC 实现的，它的最大优点是读不加锁，因此读写不冲突，并发性能好，其实现主要基于以下技术及数据结构：","title":"MVCC 与事务隔离"},{"contents":"SO_REUSEADDR 该选项可以绑定处于 TIME_WAIT 状态的地址，实践： server 端：\nfunc TestServer(t *testing.T) { fd, err := Socket(AF_INET, SOCK_STREAM, 0) if err != nil { t.Fatal(err) } if err := SetsockoptInt(fd, SOL_SOCKET, SO_REUSEADDR, 1); err != nil { t.Fatal(err) } if err := Bind(fd, \u0026amp;SockaddrInet4{Port: 9999, Addr: [4]byte{127, 0, 0, 1}}); err != nil { t.Fatal(err) } if err := Listen(fd, 1024); err != nil { t.Fatal(err) } for { connfd, _, err := Accept(fd) if err != nil { t.Log(err) continue } buf := make([]byte, 1024) _, err = Read(connfd, buf) if err != nil { t.Log(err) break } if _, err := Write(connfd, buf); err != nil { t.Log(err) break } Close(connfd) } } client 端：\nfunc TestClient(t *testing.T) { fd, err := Socket(AF_INET, SOCK_STREAM, 0) if err != nil { t.Fatal(err) } if err := Connect(fd, \u0026amp;SockaddrInet4{Port: 9999, Addr: [4]byte{127, 0, 0, 1}}); err != nil { t.Fatal(err) } if _, err := Write(fd, []byte(\u0026quot;123\u0026quot;)); err != nil { Close(fd) t.Fatal(err) } buf := make([]byte, 1024) if _, err := Read(fd, buf); err != nil { Close(fd) t.Fatal(err) } t.Log(string(buf)) time.Sleep(time.Second * 5) // client 端不会先退出 Close(fd) } 先运行 server 再运行 client，之后迅速关闭 server，此时因为 server 是主动关闭方，所以状态为 TIME_WAIT，此时再次运行 server，发现可以运行成功，如果去掉 SetsockoptInt(fd, SOL_SOCKET, SO_REUSEADDR, 1) ，那么再次运行会报错：address already in use\n","date":"2022年02月10日","permalink":"/posts/so_reuseaddr-he-so_reuseport-shi-jian/","summary":"SO_REUSEADDR 该选项可以绑定处于 TIME_WAIT 状态的地址，实践： server 端：","title":"SO_REUSEADDR 和 SO_REUSEPORT 实践"},{"contents":"问题记录 最近写了一个普通的 tcp demo，发现 server 的状态处于 CLOSE_WAIT，而 client 的状态处于 FIN_WAIT_2，为了排查这个问题，特此写了这篇文章作为记录，这也\n没有 close 的服务端：\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;net\u0026quot; ) func handler(conn net.Conn) error { for { b := make([]byte, 1024) _, err := conn.Read(b) if err != nil { return err } _, err = conn.Write(b) if err != nil { return err } } } func main() { addr := \u0026quot;:8080\u0026quot; l, err := net.Listen(\u0026quot;tcp\u0026quot;, addr) log.Printf(\u0026quot;listen in %v \\n\u0026quot;, addr) if err != nil { panic(err) } for { conn, err := l.Accept() if err != nil { log.Println(err) break } go func() { if err := handler(conn); err != nil { log.Println(\u0026quot;handler error: \u0026quot;, err) } }() } } 没有 close 的客户端：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net\u0026quot; ) func main() { c, err := net.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;:8080\u0026quot;) if err != nil { panic(err) } for { var input string fmt.Scan(\u0026amp;input) n, err := c.Write([]byte(input)) if err != nil { log.Println(\u0026quot;write error: \u0026quot;, err) continue } log.Println(\u0026quot;write byte size: \u0026quot;, n) output := make([]byte, 4096) n, err = c.Read(output) if err != nil { log.Println(\u0026quot;read error: \u0026quot;, err) continue } fmt.Printf(\u0026quot;read byte size %v, content: %v \\n\u0026quot;, n, string(output)) } } 实验步骤： 打开两个终端分别运行 server 和 client，先运行 server，再运行 client，client 连接后立马 ctrl + c 退出，输入 netstat -an | awk '/^tcp/' | grep 8080 查看连接状态：\ntcp4 0 0 127.0.0.1.8080 127.0.0.1.57785 CLOSE_WAIT tcp4 0 0 127.0.0.1.57785 127.0.0.1.8080 FIN_WAIT_2 tcp46 0 0 *.8080 *.* LISTEN 发现此时 server 的状态是 CLOSE_WAIT，而 client 的状态是 FIN_WAIT_2，这是为什么呢？分析代码发现，server 端没有对连接进行关闭处理，那么根据四次挥手的状态变化可知：\n第一次挥手，主动关闭方（client）发送 FIN，并进入 FIN_WAIT_1 状态 第二次挥手，被动关闭方（server）收到 FIN 后，回复一个 ACK，同时进入 CLOSE_WAIT 状态，主动（client）关闭方收到 ACK 后，进入 FIN_WAIT_2 第三次挥手，被动关闭方（server）发送一个 FIN，并进入 LAST_ACK，问题来了，该 FIN 是调用 close 函数后才会发送的，而上面的代码没有调用 close，所以不会发送 FIN 一切都明了了，此时因为 server 这边没有没有调用 close 导致无法发出第三次挥手，所以整个挥手流程被卡住，server 的状态停留在 CLOSE_WAIT，而 client 的状态停留在 FIN_WAIT_2\n一小段时间后（大约几分钟），CLOSE_WAIT 和 FIN_WAIT_2 状态都会自动消失\n解决 修改 server 的代码，添加 close：\n// 省略 go func() { if err := handler(conn); err != nil { log.Println(\u0026quot;handler error: \u0026quot;, err) // 新添加的关闭连接函数 conn.Close() } }() // 省略 重复之前的步骤，再次输入 netstat -an | awk '/^tcp/' | grep 8080 查看连接状态：\ntcp46 0 0 *.8080 *.* LISTEN tcp4 0 0 127.0.0.1.57896 127.0.0.1.8080 TIME_WAIT 可以看到此时直接进入到了 TIME_WAIT 状态了。\n总结： 通过此次实验，明白了 CLOSE_WAIT 出现的原因，平时写代码比较疏忽，对这些关闭连接的小细节不够重视，如果在上线环境，高并发下可能会导致出现大量的 CLOSE_WAIT，就因为这么一个小小的细节可能导致严重的后果，同时也深刻认识到了基础的重要性。\n附 CLOSE_WAIT 的危害 处于 CLOSE_WAIT 状态的 TCP 连接会一直占用端口号、socket fd 等资源，造成资源浪费，如果主机存在大量的 CLOSE_WAIT 状态的连接，甚至可能会导致无法建立新连接，从而整个服务不可用。\nTCP 四次挥手 第一次挥手，主动关闭方（client）发送 FIN，并进入 FIN_WAIT_1 状态 第二次挥手，被动关闭方（server）收到 FIN 后，回复一个 ACK，同时进入 CLOSE_WAIT 状态，主动（client）关闭方收到 ACK 后，进入 FIN_WAIT_2 第三次挥手，被动关闭方（server）发送一个 FIN，并进入 LAST_ACK，主动关闭方（client）收到 FIN 后，进入 TIME_WAIT 状态 第四次挥手，主动关闭方（client）发送 ACK，被动关闭方（server）收到 ACK 后，进入 CLOSED 状态 ","date":"2022年02月02日","permalink":"/posts/close_wait-he-fin_wait_2/","summary":"问题记录 最近写了一个普通的 tcp demo，发现 server 的状态处于 CLOSE_WAIT，而 client 的状态处于 FIN_WAIT_2，为了排查这个问题，特此写了这篇文章作为记录，这也","title":"TCP CLOSE_WAIT 和 FIN_WAIT_2"},{"contents":"TCP 相关 1. 说说 TIME_WAIT 及其作用 当一端 主动 断开连接并发送最后一次挥手后，该端状态变为 TIME_WAIT ，此时会等待 2 MSL （MSL 是 Maximum Segment Lifetime，译为“报文最大生存时间”，可为 30s，1min 或 2min。） 作用：\n避免新旧连接混淆 如果一个 tcp 连接被关闭了，紧接着马上又有一个相同四元组的 tcp 连接建立了，且此时刚好旧连接中有一个延迟的报文到达了，那么这个旧报文就会错误的被新连接接收（seq 对新连接恰好有效），进而可能会产生一些诡异或者错误的现象。TIME_WAIT 可以在一定程度确保这些延迟的旧报文在网络中消失。\n确保双方的连接都可以正常关闭 假设 A 主动关闭，那么根据四次挥手可知，最后一次挥手，A 需要发送一个 ACK 给 B，如果发送之后没有 TIME_WAIT，而是直接 CLOSED，那么如果该 ACK 丢失，B 会因为超时而重发 FIN，但是因为 A 已经关闭了，所以会回复一个 RST，B 就会认为发生了错误，实际上并没有。如果 A 在发送最后一个 ACK 后进入 TIME_WAIT 等待一段时间，那么 B 的重传 FIN 就可以被正常接收了。\n也有一些博客说，客户端四次挥手的最后一个 ACK 报文如果在网络中被丢失了，此时如果客户端 TIME_WAIT 过短或没有，则就直接进入了 CLOSED 状态了，那么服务端则会一直处在 LASE_ACK 状态。 当客户端发起建立连接的 SYN 请求报文后，服务端会发送 RST 报文给客户端，连接建立的过程就会被终止。\n2. TCP 如何保证可靠性 3. 三次握手对应到 socket 函数 4. 四次挥手对应到 socket 函数 主动关闭方调用 close 发送第一次挥手 被动关闭方调用 close 发送第三次挥手 5. 为什么断开连接需要四次挥手 6. 三次握手为什么需要 3 次？2 次或者 4 次不行吗 2 次：无法阻止历史连接，造成资源浪费 图片来源：https://blog.csdn.net/qq_34827674/article/details/122040501\n在上面的图片中，服务端一收到 SYN 就建立连接，无法辨认该 SYN 是否是旧连接，只有在收到客户端的 RST 后才知道，导致白白创建了一条连接\n图片来源：https://blog.csdn.net/qq_34827674/article/details/122040501\n可以看到，在三次握手的情况下， 可以在服务端建立连接之前，阻止掉历史连接，从而保证建立的连接不是历史连接。\n4 次：既然 3 次已经可以保证连接正常建立了，就没必要再多加一次了\n7. TCP 的拥塞控制 HTTP 相关 1. HTTP 1.0，1.1，2.0，3.0 HTTP 1.0 采用短连接，也就是每次请求都需要与服务器建立一次连接，服务器处理完成后便断开连接。无状态：即每次请求完成后就会断开连接，每个连接断开后都无法获取上次连接的状态和信息。 存在的问题：\n无法复用连接，每次发送请求，都需要进行一次TCP连接，而TCP的连接释放过程又是比较费事的。这种无连接的特性会使得网络的利用率变低。\n队头阻塞(head of line blocking)，由于HTTP1.0规定下一个请求必须在前一个请求响应到达之前才能发送，假设前一个请求响应一直不到达，那么下一个请求就不发送，后面的请求就阻塞了。\n不支持断点续传，也就是说，每次都会传送全部的页面和数据。\nHTTP 1.1 长连接：HTTP 1.1 默认支持 keep-alive 的长连接模式，解决了 1.0 中无法复用 TCP 连接的问题，当本次请求结束后不会断开连接并保持本次连接的状态与信息，客户端与服务端都有权取消该模式。\n管道化：多个请求不用排队发送，但是服务端必须按照发送的顺序进行相应，所以并没有完全解决 HTTP 1.0 的队头阻塞问题。举例：假设文件A中有三个静态文件D,E和F，虽然管道化技术使E不需要等待D请求完成并接收到响应后再开始请求，F也同理，但是最后服务端还是需要按照发送请求时的顺序来依次给出响应，这时候依旧会发生阻塞。 原因是 HTTP1.1 的请求和响应并没有序号标识，所以无法将乱序的响应与请求对应起来。\n基于长连接的模式就可以实现断点续传的功能了，这样对大文件的传输和下载会更加友好，同时HTTP1.1在请求头中引入了range头域，它允许只请求资源的某部分，此时的返回码是206\nHTTP 2 （参考 https://blog.csdn.net/qq_34827674/article/details/115188949）\n头部压缩：在 HTTP 1/1.1 中，可以通过指定 Content-Encoding 来设置 body 的压缩方式，比如 gzip，但是 header 部分不能进行压缩，且 header 部分存在几个问题：1. 使用 ASCII 码，效率低 2. 可能很多请求的 header 的字段值都是相同的，比较冗余 3. 存在一些固定字段，比如 Cookie 解决方式： 静态编码表 静态编码表为一些常见的 Header 进行了映射，用一个 index 来标识，比如在上图中，:method GET 使用 2 来代替，这样传输就会节省很多字节，服务器在接收到报文后，通过静态编码表就可以获取到相应的信息。 但是也会发现一个问题，一些 index 是没有 header value 的，这是因为这些 Value 并不是固定的而是变化的，这些 Value 都会经过 Huffman 编码后，才会发送出去。具体的编码方式参考上面给出来的原文链接。\n动态编码表 静态表只包含了 61 种高频出现在头部的字符串，不在静态表范围内的头部字符串就要自行构建动态表，它的 Index 从 62 起步，会在编码解码的时候随时更新。\n比如发送的报文包含这样一个头部：XXX: YYY，那么客户端和服务端都会在各自的动态编码表中添加这一 header，并且用一个 index 标识，如果客户端之后要再次发送该头部，那么只需要发送对应得 index 即可。\n使得动态表生效有一个前提：必须同一个连接上，重复传输完全相同的 HTTP 头部。 如果消息字段在 1 个连接上只发送了 1 次，或者重复传输时，字段总是略有变化，动态表就无法被充分利用了。\n因此，随着在同一 HTTP/2 连接上发送的报文越来越多，客户端和服务器双方的「字典」积累的越来越多，理论上最终每个头部字段都会变成 1 个字节的 Index，这样便避免了大量的冗余数据的传输，大大节约了带宽。\n动态表的弊端是：随着表字段的增长，占用的内存也会越来越大，为了解决该问题，Web 服务器都会提供类似 http2_max_requests 的配置，用于限制一个连接上能够传输的请求数量，避免动态表无限增大，请求数量到达上限后，就会关闭 HTTP/2 连接来释放内存。\n二进制帧\n多路复用 为了解决 HTTP 1/1.1 中的对头阻塞问题，HTTP 2 使用了**流（stream）**的概念，每一次请求对应一个流，有一个唯一的 ID，用来区分不同的请求，多个 stream 复用一条 TCP 连接，达到并发的效果。在每个 stream 中还有多个 message，message 对应 HTTP1 中的请求或响应，有 header 和 body 组成。message 中有一个或多个 frame（帧），frame 是 HTTP2 的最小单位，以二进制压缩格式存放 HTTP1 中的内容。一个请求的数据会被分成多个帧，方便进行数据分割传输，每个帧都唯一属于某一个流ID，将帧按照流ID进行分组，即可分离出不同的请求。这样同一个TCP连接中就可以同时并发多个请求，不同请求的帧数据可穿插在一起，根据流ID分组即可。\nHTTP2 依然存在的问题：TCP 的队头阻塞 如果HTTP/2连接双方的网络中有一个数据包丢失，或者任何一方的网络出现中断，整个TCP连接就会暂停，丢失的数据包需要被重新传输。 因为TCP是一个按序传输的链条，因此如果其中一个点丢失了，链路上之后的内容就都需要等待。 这种单个数据包造成的阻塞，就是TCP上的队头阻塞（head of line blocking）。\nHTTP 3 TODO\nHTTP 状态码，1xx-5xx 1xx：表示已接收请求，需要继续发送请求 2xx：表示成功处理了请求 3xx：表示需要进行重定向 4xx：表示请求不能被理解、处理 5xx：表示服务器在处理请求时发生了内部错误，这些错误可能是服务器本身的错误，而不是请求出错\n输入 url 到浏览器的过程 首先需要确保本机拥有一个 IP 地址，如果没有，则需要通过 DHCP 获取一个 IP，获取流程如下：\n1.1 本机生成一个 DHCP 请求报文，放入到一个目的端口为 67，源端口为 68 的 UDP 报文中\n1.2 再将这个 UDP 报文放到一个目的地址为 255.255.255.255 （广播地址）和源地址为 0.0.0.0 （无 IP 地址）的 IP 报文中\n1.3 再将这个 IP 报文放到一个以太网帧中，该以太网帧的目的 MAC 地址为 FF:FF:FF:FF:FF:FF（表示广播给局域网内的所有主机）， 源 MAC 地址为 00:16:D3:23:68:8A\n1.4\n","date":"2022年01月28日","permalink":"/posts/ji-suan-ji-wang-luo-mian-shi-ti/","summary":"TCP 相关 1. 说说 TIME_WAIT 及其作用 当一端 主动 断开连接并发送最后一次挥手后，该端状态变为 TIME_WAIT ，此时会等待 2 MSL （MSL 是 Maximum Segment Lifetime，译为“报文最大生存时间”，可为 30s，1min 或 2min。） 作用：","title":"计算机网络面试题"},{"contents":"Goroutine 阻塞的话，是不是对应的 M 也会阻塞 如果 G 被阻塞在某个系统调用上，那么不仅仅 G 会阻塞，执行 G 的 M 也会解绑 P，与 G 一起进入挂起状态。如果此时有空闲的 M,则 P 和与其绑定并继续执行其他的 G;如果没有空闲的 M,但还是有其他 G 需要去执行，那么会创建一个新 M。当系统调用返回后，阻塞在该系统调用上的 G 会尝试获取一个可用的 P,如果没有可用的 P,那么这个 G 会被标记为 runnable 并把它放入全局的 runqueue 中等待调度，之前的那个挂起的 M 将再次进入挂起状态。\nGMP 当一个 G 阻塞时，G、M、P 会发生什么 用户态阻塞\n当 goroutine 因为 channel 操作或者 network I/O 而阻塞时（实际上 golang 已经用 netpoller 实现了goroutine网络 I/O 阻塞不会导致 M 被阻塞，仅阻塞 G ），对应的 G 会被放置到某个 wait 队列(如 channel 的 waitq )，该 G 的状态由 _Gruning 变为 _Gwaitting，而 M 会跳过该 G 尝试获取并执行下一个 G，如果此时没有 runnable 的G 供 M 运行，那么 M 将解绑 P，并进入 sleep 状态；当阻塞的 G 被另一端的 G2 唤醒时（比如 channel 的可读/写通知），G 被标记为 runnable，尝试加入 G2 所在 P 的 runnext，然后再是 P 的 Local 队列和 Global 队列。\n系统调用阻塞\n当 G 被阻塞在某个系统调用上时，此时 G 会阻塞在 _Gsyscall 状态，M 也处于 block on syscall 状态，此时的 M可被抢占调度：执行该 G 的 M 会与 P 解绑，而 P 则尝试与其它 idle 的 M 绑定，继续执行其它 G。如果没有其它idle 的 M，但 P 的 Local 队列中仍然有 G 需要执行，则创建一个新的 M；当系统调用完成后，G 会重新尝试获取一个 idle 的 P 进入它的 Local 队列恢复执行，如果没有 idle 的 P，G 会被标记为 runnable 加入到 Global 队列。\nP 和 M 数量可以无限扩增的吗？ 不是无线扩增的。\nP 的数量：由启动时环境变量 $GOMAXPROCS 或者是由 runtime 的方法 GOMAXPROCS() 决定。\nM的数量：goroutine 程序启动时，会设置 M 的最大数量，默认10000。但是内核很难创建出如此多的线程，因此默认情况下 M 的最大数量取决于内核。也可以调用 runtime/debug 中的 SetMaxThreads 函数，手动设置 M 的最大数量。\nP 的调度逻辑 先从本地 runq 获取待执行的 G，如果没有，再从全局 runq 获取待执行的 G，还没有的话，就从别的 P 中拿（偷）走一半的 G\n如果一个 G 运行时间过长，导致队列中后续 G 都无法运行呢？ 内容来自：https://www.bilibili.com/video/BV1zT4y1F7XF?spm_id_from=333.999.0.0，里面有非常详细的介绍\n这涉及到 GMP 的抢占，在 main goroutine 启动时，会创建一个 sysmon goroutine，这是一个特殊的协程，其不依赖 P，也不由 GMP 调度，他会本着公平调度的原则，对运行时间过长的 P，实行抢占操作，就是告诉那些运行时间超过阈值的 P 该让出了，那么怎么知道运行时间过长了呢？P 里面有一个 schedtick 字段，每当调度执行一个新的 G，并且不继承上个 G 的时间片时（这里不懂），就会把它自增 1，还有一个 sysmontick.schedwhen 记录的是上次调度的时间，监测协程如果检测到 sysmontick.schedtick 不等于 p.schedtick，说明这个 P 又发生了新的调度，就会同步 schedwhen 和 schedtick，但如果相等，则说明子 schedwhen 这个时间点之后，这个 P 并未发生了新的调度，或者沿用了之前 G 的时间片，所以可以通过当前时间与 schedwhen 的差值，(sysmontick.schedwhen + forcePreemptNS \u0026lt; now)，来判断当前 P 上的 G 是否运行时间过长了，如果运行时间过长，那么就要通知该 P 让出了。\n如何通知 P 呢？使用的是 栈增长 的方式。除了对协程栈没什么消耗的函数调用，Go 语言编译器都会在函数头部插入栈增长检测相关代码，会根据栈帧大小来插入不同的代码，SP 表示当前的栈使用到了哪个位置，stackguard0 表示协程栈的空间下界，当栈的消耗达到或超过 stackguard0 时，就需要进行栈增长，会根据超出大小的多少来使用不同的增长代码（这个具体看视频里的说明，一共 3 种策略），如果调度器希望当前 P 让出，那么就会将 stackguard0 设置为 stackPreempt，这是一个非常大的值，真正的栈指针不可能指向这个位置，所以可以安全的用作特殊标识，此外因为该值足够大，那么 3 种策略都会满足条件，从而 goto 到 morestack 处，morestack 会调用 runtime.newstack 函数，负责栈增长工作，但是在增长前，会先判断 stackguard0 是否等于 stackPreempt，如果等于就不进行栈增长了，而是执行一次协程调度，从而达到抢占的目的。\n不过这种抢占方式的缺陷就是过于依赖栈增长代码，如果来个 for{}，因为不涉及到函数调用，所以与栈增长无关，也就无法通过上面的方式来实现抢占，这一问题在 go1.14 中得到了解决，因为它实现了异步抢占，是通过信号（signal）实现的，当要抢占时，会向协程关联的 M 发送一个 sigPreempt 信号（好像底层是 SIGURG），目标线程（M）收到信号后会被中断，转去执行 sigHandler，该函数检测到信号为 sigPreempt 后，会调用 runtime.doSigPreempt 函数，它会向当前被打断的协程上下文中，注入一个异步抢占函数调用，之后返回，被打断的协程恢复，立刻执行被注入的异步抢占函数，该函数最终会调用 runtime 中的调度逻辑，从而实现让出。\nG 陷入系统调用会发生什么 一个协程要执行系统调用，就要切换到 g0 栈（为什么要切换到 g0 栈），在系统调用过程中，G 和 M 会一直绑定在一起，不能被分开，也就用不到 P 了，所以在陷入系统调用前，当前 M 会让出 P，解除 m.P 与当前 P 的强关联，并且记录到 m.oldp 中。但是这个 P 如果放着不管就有点浪费了，还是需要将其关联到其他 M，继续执行工作，当之前的 M 结束系统调用后，会先检查之前的 P（m.oldp）是否被占用，没有的话就继续使用，否则就重新申请一个，没申请到的话（什么情况下会申请不到？），就把当前 G 放到全局 runq 中，然后 M 就进入睡眠。\n如果当前 M 没有绑定 P，那么如何获取 G GMP 的几个队列 貌似 GMP 里的全局队列有两种，一种就是单纯保存用作记录的，还有一种是调度器的全局队列，里面保存的是空闲状态的\n调度器的可运行 G 队列\n存放等待执行的 g，新创建的 g 会先尝试保存到当前 g （这个 g 就是新创建 g 的父 goroutine）对应的 p 的本地队列，如果本地队列已满（256个），就会把 g 放到全局队列中。\n该队列有容量限制吗？\n该队列里的 g 什么时候被消费？\n如果某个 p 的本地 g 队列为空，则会从全局队列里获取\n全局 M 队列\n作用： 获取所有 M 的信息 防止 M 被当作垃圾回收掉 调度器的空闲 M 队列\n运行时系统在停止 M 时，会把它放入调度器的空闲 M 队列\n什么时候被消费 当 G 陷入系统调用时，与其关联的 M 也会被阻塞，而与之关联的 P 会被分离，使得这个 P 中剩余的 G 可以被执行，此时 P 就会去全局 M 队列里查找，如果有的话\n全局 P 队列\n保存了当前 runtime 创建的所有 P，runtime 会把这些 p 中的可运行 g 全部取出，并放入调度器的可运行 g 队列中，被转移的这些 g，会在以后经由调度再次放入某个 p 的可运行 g 队列\n调度器的空闲 P 队列\n当一个 p 不与任何 m 关联时，runtime 就会把它放入该列表，当 runtime 需要一个空闲的 p 来关联某个 m 时，就会从该队列获取。此外，p 进入空闲队列的一个重要条件是，本地队列里没有可运行的 g\nP 的 G 队列\n保存的是可运行的 g，新创建的 g 会先尝试放到这里，最大容量为 256。\nP 的自由 G 队列，调度器的自由 G 列表\n自由 g 表示的是已经运行完成的 g，主要是为了提高复用率，避免频繁创建 g，因为 g 本质也是一个对象。\n运行完成的 g 会先放到对应的 p 的自由 g 队列里，如果太多了，就会转移一部分到调度器的自由 g 列表\n当创建一个 goroutine 时，会先尝试从 p 的自由 g 队列中获取一个现成的 g，如果没有或者太少，会从调度器的自由 g 队列中拿一部分，如果调度器中也没有，才会新创建一个 g\ng0，m0 是什么 程序题 以下程序的运行结果是什么？\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;runtime\u0026quot; ) func main() { runtime.GOMAXPROCS(1) var wg sync.WaitGroup wg.Add(3) // goroutine1 go func(n int) { fmt.Println(n) wg.Done() }(1) // goroutine2 go func(n int) { fmt.Println(n) wg.Done() }(2) // goroutine3 go func(n int) { fmt.Println(n) wg.Done() }(3) wg.Wait() } 结果：3 1 2\nP 有一个 runnext 字段，保存的是下一个要运行的 g，当创建一个新的 g 时，会调用 runqput 将其添加到 P 的 runq 中，如果当前 P 的 runq 中已经有 g 了，则会将其挤走，被挤走的那个进入 P 的本地 runq。runnext 的优先度比 runq 高，会先执行 runnext 的 g，再按顺序执行 runq 中的 g。\n在上面的程序中，因为设置了 runtime.GOMAXPROCS(1)，所以整个 runtime 中只有 1 个 P，也就是说上面的 3 个goroutine 都共用这一个 P，首先，goroutine1（简称 g1） 先放到 runnext，之后 g2 进来把 g1 挤走，g1 放到 runq，再之后，g3 把 g2 挤走，g2 放到 runq，此时 runnext 保存的是 g3，runq 中是 g1，g2，所以输出结果是 3，1，2\ngoroutine 的调度时机有哪些 ","date":"2022年01月26日","permalink":"/posts/gmp-mian-shi-ti/","summary":"Goroutine 阻塞的话，是不是对应的 M 也会阻塞 如果 G 被阻塞在某个系统调用上，那么不仅仅 G 会阻塞，执行 G 的 M 也会解绑 P，与 G 一起进入挂起状态。如果此时有空闲的 M,则 P 和与其绑定并继续执行其他的 G;如果没有空闲的 M,但还是有其他 G 需要去执行，那么会创建一个新 M。当系统调用返回后，阻塞在该系统调用上的 G 会尝试获取一个可用的 P,如果没有可用的 P,那么这个 G 会被标记为 runnable 并把它放入全局的 runqueue 中等待调度，之前的那个挂起的 M 将再次进入挂起状态。","title":"GMP 面试题"},{"contents":" 转载自：https://blog.csdn.net/u010476994/article/details/104001848 （Centos 7 部分） https://blog.csdn.net/u014630144/article/details/108129079 （Ubuntu 20 部分）\n本帖子能够实现的效果： 1、虚拟机能访问外网、虚拟机能访问Mac本机； 2、Mac本机可以连接虚拟机。\nMac 前提步骤 1、配置 VMware Fusion 虚拟网络配置 VMware Fusion 安装完成后，会在Mac OS中新建两个网卡： vmnet1以及vmnet8（在 /Library/Preferences/VMware Fusion 下可以看到），其中 vmnet1 是Host-only模式， vmnet8是NAT模式。此处仅对网卡vmnet8 进行修改（ 修改过程中需关闭VMWare Fusion）。\n1.1 修改 /Library/Preferences/VMware\\ Fusion/networking 。 sudo vi /Library/Preferences/VMware\\ Fusion/networking 将 DHCP 设置为 no， 即使用静态IP。 将 SUBNET 修改为自己想用的网段，此处我填的是 192.168.111.0 网段。\n保存退出。 注意：只修改 vmnet8 的配置， 不要修改 vmnet1 的配置。\n1.2 修改 /Library/Preferences/VMware\\ Fusion/vmnet8/nat.conf 。 设置网关为 192.168.111.2 ， 网关的IP要和上一步中的IP 保持网关一致。 至此，VMware Fusion的配置完毕。\n1.3 将虚拟机网络切换到NAT模式。 CentOS 7 步骤如下：\n1、打开虚拟机，配置虚拟机网络配置信息 sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33 可按如下格式配置网络信息（部分信息需按照自己的情况配置）\nTYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=405afcb9-08fe-4507-8775-232604d2e819 DEVICE=ens33 ONBOOT=yes IPADDR=192.168.110.14 # 自定义的虚拟机IP， 需与VMware Fusion配置的IP在同一个网段上 GATEWAY=192.168.110.2 # 网关。1.2中配置的网关地址 NETMASK=255.255.255.0 # 掩码。1.2中配置的掩码 DNS1=192.168.1.1 # Mac本机的DNS地址。 系统偏好设置-\u0026gt; 网络 -\u0026gt; 在左侧选择当前使用的网络，点击右下角的“高级”按钮 -\u0026gt; 切换Tab页，可找到DNS地址。 DNS2=192.168.1.1 # 同上 保存退出，重启网络服务\nsystemctl restart network 至此。所有网络配置完成。\n注意：修改完 DNS 后需要重启 VMware 才能生效 （貌似只需要重启一次，后续再更改 DNS 不需要重启了）\nUbuntu 20 1. 编辑网络配置文件： sudo vi /etc/netplan/00-installer-config.yaml 编辑文件内容如下：\n# This is the network config written by 'subiquity' network: ethernets: ens33: dhcp4: no addresses: [192.168.110.10/24] # 确保该值在前提步骤 1.1 中设置的 ip 段内，且子网掩码相同 gateway4: 192.168.110.2 # 与前提步骤 1.2 中网管 ip 相同 nameservers: addresses: [114.114.114.114] version: 2 使配置的ip地址生效：\nsudo netplan apply 重启 VMware Fusion 生效。\n","date":"2022年01月17日","permalink":"/posts/mac-huan-jing-xia-vmware-fusion-xia-de-xu-ni-ji-centos-7de-nat-wang-luo-pei-zhi/","summary":"转载自：https://blog.csdn.net/u010476994/article/details/104001848 （Centos 7 部分） https://blog.","title":"Mac环境下， VMware Fusion下的虚拟机（ CentOS 7/Ubuntu20）的 NAT网络配置"},{"contents":"测试数据如下：\nCREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; +----+------+------+ | id | c | d | +----+------+------+ | 0 | 0 | 0 | | 5 | 5 | 5 | | 10 | 10 | 10 | | 15 | 15 | 15 | | 20 | 20 | 20 | | 25 | 25 | 25 | +----+------+------+ 测试情景 1 不存在的主键，select 是否会加间隙锁 session A session B 1 begin 2 begin 3 select * from t where id = 9 for update; 4 insert into t values(11,5,5); 5 insert into t values(6,5,5); 6 insert into t values(10,5,5); 7 insert into t values(4,5,5); 8 select * from t where id = 9 for update; 结果：\nB1：成功执行，输出 Empty set\nB4：成功执行\nB5：被阻塞\nB6：Duplicate entry \u0026lsquo;10\u0026rsquo; for key \u0026rsquo;t.PRIMARY\u0026rsquo; 主键不能重复\nB7：成功执行\nB8：成功执行，输出 Empty set\n说明：\n因为 id = 9 这一行不存在，所以会加间隙锁，区间为 (5, 10]，左开右闭。所以所有尝试在 [5, 10) 区间 insert 数据的 sql 都被阻塞了，但是区间外不会被阻塞，比如 id=11 和 id = 4 就成功执行了。\n关于 B8 为什么没有被阻塞，可能是因为因为 id=9 不存在，所以 A3 处的 for update 并没有生效加锁。\n测试情景 2 session A session B 1 begin 2 begin 3 select * from t where id = 9 for update; 4 insert into t values(9,9,9); 5 select * from t where id = 9 for update; 6 select * from t where id = 7 for update; 7 8 A3：成功执行，输出 Empty set\nA4：成功执行\nB5：被阻塞\nB6：成功执行，输出 Empty set\n为什么 B5 会被阻塞呢？似乎除了 id=9 以外，其他的查询都不会被阻塞\n破案了，insert 会加排它锁\n测试情景 3 已经存在的主键，select 是否会加间隙锁 session A session B 1 begin 2 begin 3 select * from t where id = 10 for update; 4 insert into t values(11,5,5); 5 insert into t values(9,5,5); 6 select * from t where id = 10 for update; 7 select * from t where id = 9 for update; 8 select * from t where id = 11 for update; 9 select * from t where id = 8 for update; A3：成功执行\nB4：成功执行\nB5：成功执行\nB6：被阻塞\nA7：被阻塞\nA8：被阻塞\nA9：成功执行\n因为 id=10 存在且 id 为主键，所以只加了排他锁，没有加间隙锁，所以 B4，B5 的插入都成功了，B6 被阻塞了，因为 id=10 这一行已经上锁了。\n由于主键是唯一索引，而且是只使用一个索引查询，并且只锁定一条记录，所以以上的例子，只会对 id = 10 的数据加上记录锁，而不会产生间隙锁。\n比较怪异的是 A7、A8 这两个地方，居然被阻塞了，观察发现，这两个 sql 查询的 id 是 9 和 10，刚好是 B 中插入的两个 id，而 A9 处的 id=8 没有被阻塞，难道在 insert 之后会给新插入的行加锁吗？\n破案了，insert 会加排它锁\n测试情景 4 已经存在的非索引列（可能查出多条数据），select 是否会加间隙锁 session A session B 1 begin 2 begin 3 select * from t where d=5 for update; 4 insert into t values(26,26,5); 5 insert into t values(26,26,4); 6 insert into t values(100,100,100); A3：成功执行\nB4：被阻塞\nB5：被阻塞\nB6：被阻塞\n因为 A3 查询条件是 d=5，而 d 既不是主键也没有索引，所以会加间隙锁，防止幻读出现，间隙锁的范围是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +suprenum]，所有的 insert 全部被阻塞了，\n测试情景 5 不存在的非索引列（可能查出多条数据），select 是否会加间隙锁 session A session B 1 begin 2 begin 3 select * from t where d=100 for update; 4 insert into t values(26,26,5); 5 insert into t values(26,26,4); 6 insert into t values(100,100,100); 结论：session B 的 insert 全部被阻塞，由此可见，即便 d=100 这一行不存在，依然会加间隙锁\n测试场景 6 不存在的普通索引列，select 是否会加间隙锁 session A session B 1 begin 2 begin 3 select * from t where c=9 for update; 4 insert into t values(11,11,11); 5 insert into t values(100,8,100); 6 insert into t values(100,10,100); 7 insert into t values(101,5,100); 8 insert into t values(102,4,100); B4：成功执行\nB5：被阻塞\nB6：成功执行\nB7：被阻塞\nB8：成功执行\n对于列 c 而言，有这些间隙： (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +suprenum]\n因为查询的是 c=9 ，所以间隙锁范围是 (5,10]，但是我没有太明白这个范围的意思，按照数学来说，应该是不包含 5，但是包含 10 的意思，也就是 c=5 不加锁，c=10 加锁，但是从上面的测试来看，insert c=5 被阻塞，而 insert c=10 没有被阻塞，完全相反，这是为什么？\n测试情景 6 session A session B 1 begin 2 begin 3 insert into t values(9,9,9); 4 select * from t where id=9 for update; A3：执行成功\nB4：阻塞\n可以参考 https://zhuanlan.zhihu.com/p/48269420\n","date":"2021年10月19日","permalink":"/posts/mysql-jian-xi-suo-shi-jian/","summary":"测试数据如下：\nCREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; +----+------+------+ | id | c | d | +----+------+------+ | 0 | 0 | 0 | | 5 | 5 | 5 | | 10 | 10 | 10 | | 15 | 15 | 15 | | 20 | 20 | 20 | | 25 | 25 | 25 | +----+------+------+ 测试情景 1 不存在的主键，select 是否会加间隙锁 session A session B 1 begin 2 begin 3 select * from t where id = 9 for update; 4 insert into t values(11,5,5); 5 insert into t values(6,5,5); 6 insert into t values(10,5,5); 7 insert into t values(4,5,5); 8 select * from t where id = 9 for update; 结果：","title":"Mysql 间隙锁实践"},{"contents":"全文基于此表：\nCREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 1. 什么是幻读 如下是文章中的图，需要注意的是，这里的 result 是基于 如果只在id=5这一行加锁，而其他行的不加锁 （也就是行锁）这一基础上的，当时没注意这句话，导致自己用 Mysql 做实验时出现了不一样的结果：session A 的 T1 执行完成后，session B 的 T2 被阻塞，直到 A 事务结束，因为 B 被阻塞了，所以 A 的 T3 查询结果依然是 (5,5,5) 而不是图中的 (0,0,5)(5,5,5) 幻读 指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。比如在上图中，Q3 读到了 (1,1,5) ，便是幻读。\n幻读 在 当前读 下才会出现，图中的 select 都加了 for update，代表当前读。（当前读：读取的是记录数据的最新版本，并且当前读返回的记录都会加上锁，保证其他事务不会再并发的修改这条记录），在快照读下不会出现（可重复读隔离级别下），因为快照读看不到别的事务插入的数据。\n幻读 仅专指 新插入的行，所以 session B T2 的 update 操作，导致的 A T3 结果不算是幻读。比如在上图中，Q2 读到了 (0,0,5) ，但不是幻读。\n2. 幻读有什么问题 这是原文中的标题，但是我不是很理解之后举的例子和幻读有什么关系，倒是换成 只加行锁有什么问题 更贴切（个人拙见）。\n2.1 语义被破坏 session A 里 Q1 语句，表示要锁住所有 d=5 的行，阻塞其他事务的读写操作。但是在 session B T2 中找个了漏洞，先将 id=0 这一行的 d 改成 5，再对这一行进行更改，这样就破坏了 A 中锁住所有 d=5 这一语句进行了破坏，session C 也是一样的道理。\n2.2 数据一致性的问题 锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。\n看看 binlog 里面的内容：\nT2 时刻，session B 事务提交，写入了两条语句；\nT4 时刻，session C 事务提交，写入了两条语句；\nT6 时刻，session A 事务提交，写入了update t set d=100 where d=5 这条语句。\n// session B update t set d=5 where id=0; /*(0,0,5)*/ update t set c=5 where id=0; /*(0,5,5)*/ // session C insert into t values(1,1,5); /*(1,1,5)*/ update t set c=5 where id=1; /*(1,5,5)*/ // session A update t set d=100 where d=5;/*所有d=5的行，d改成100*/ 因为 binlog 是按照 commit 的先后顺序记录的，而不是按 update 的执行顺序记录的（上图中 A 最先执行 update，然后是 B，最后是 C），所以会导致错误的结果，如果以后用 binlog 来克隆一个库，这三行的结果变成了 (0,5,100)、(1,5,100) 和 (5,5,100)，与实际的 (0,5,5)、(1,5,5) 和 (5,5,100) 不符，因为 A 被记录到了最后。\n如何让 binlog 按 update 的执行顺序记录呢？按照文中的说法是，把扫描过程中碰到的行，也都加上写锁，（这里没太明白，意思是把 session A 中的 select 语句扫描到的行全部加锁吗？如果是这样的话，因为 d 没有索引，所以会扫描全表，也就是把整张表都加锁吗？暂时先这么理解吧）A select for update 时直接将所有行锁起来，这样后续的 B C 都会被阻塞，直到 A commit，这样 binlog 中，最先记录的就是 A，此时的 binlog：\ninsert into t values(1,1,5); /*(1,1,5)*/ update t set c=5 where id=1; /*(1,5,5)*/ update t set d=100 where d=5;/*所有d=5的行，d改成100*/ update t set d=5 where id=0; /*(0,0,5)*/ update t set c=5 where id=0; /*(0,5,5)*/ 可以看到上面的记录依然存在问题，最先记录的不是预想中的 A，而是 C，这是为什么呢？明明都已经把整张表都锁起来了，还是阻止不了id=1这一行的插入和更新呢？\n原因很简单。在T3时刻，我们给所有行加锁的时候，id=1这一行还不存在，不存在也就加不上锁。\n也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，这也是为什么“幻读”会被单独拿出来解决的原因。\n3. 如何解决幻读 3.1 间隙锁 产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的间隙。\n何为间隙？比如 (0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25) 这六条记录中，便存在以下 7 个间隙： 这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的6个记录加上了行锁，还同时加了7个间隙锁。这样就确保了无法再插入新的记录。\n跟 间隙锁 存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。\n对于指定查询某一条记录的加锁语句，如果该记录不存在，会产生记录锁和间隙锁，如果记录存在，则只会产生记录锁。（注意这里的查询一条记录，如果是范围查询，或者查询出多条结果，那么不管存不存在，都会加间隙锁）\n对于查找某一范围内的查询语句，会产生间隙锁，如：WHERE id BETWEEN 5 AND 7 FOR UPDATE。\n间隙锁是在可重复读隔离级别下才会生效的。\n间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。\n3.2 间隙锁导致的死锁案例 session A 执行select \u0026hellip; for update语句，由于id=9这一行并不存在，因此会加上间隙锁(5,10);\nsession B 执行select \u0026hellip; for update语句，同样会加上间隙锁(5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；\nsession B 试图插入一行(9,9,9)，被session A的间隙锁挡住了，只好进入等待；\nsession A试图插入一行(9,9,9)，被session B的间隙锁挡住了。\n至此，两个 session 进入互相等待状态，形成死锁。当然，InnoDB 的死锁检测马上就发现了这对死锁关系，让 session A 的 insert 语句报错返回了。\n","date":"2021年10月19日","permalink":"/posts/mysql-shi-zhan-bi-ji-20-jiang-huan-du-shi-shi-me/","summary":"全文基于此表：\nCREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 1.","title":"MySQL实战笔记 20讲幻读是什么"},{"contents":"https://www.cnblogs.com/Joezzz/p/9803344.html\n","date":"2021年10月04日","permalink":"/posts/zhuan-zai-jin-cheng-de-xu-ni-di-zhi-kong-jian-fen-bu/","summary":"https://www.cnblogs.com/Joezzz/p/9803344.html","title":"[转载] 进程的虚拟地址空间分布"},{"contents":"https://blog.csdn.net/qq_41855420/article/details/103746234\n","date":"2021年10月04日","permalink":"/posts/mac-tong-guo-dosbox-da-jian-hui-bian-huan-jing-fu-bian-yi-lian-jie-zhi-xing-diao-shi-xiang-xi-guo-cheng/","summary":"https://blog.csdn.net/qq_41855420/article/details/103746234","title":"[转载] Mac 通过DOSBox搭建汇编环境（附编译、链接、执行、调试详细过程）"},{"contents":" 给定一个无重复元素的正整数数组 candidates 和一个正整数 target ，找出 candidates 中所有可以使数字和为目标数 target 的唯一组合。\ncandidates 中的数字可以无限制重复被选取。如果至少一个所选数字数量不同，则两种组合是唯一的。 对于给定的输入，保证和为 target 的唯一组合数少于 150 个。\n示例 1： 输入: candidates = [2,3,6,7], target = 7 输出: [[7],[2,2,3]]\n示例 2： 输入: candidates = [2,3,5], target = 8 输出: [[2,2,2,2],[2,3,3],[3,5]]\n示例 3： 输入: candidates = [2], target = 1 输出: []\n示例 4： 输入: candidates = [1], target = 1 输出: [[1]]\n示例 5： 输入: candidates = [1], target = 2 输出: [[1,1]]\n提示： 1 \u0026lt;= candidates.length \u0026lt;= 30 1 \u0026lt;= candidates[i] \u0026lt;= 200 candidate 中的每个元素都是独一无二的。 1 \u0026lt;= target \u0026lt;= 500\n方法 1 回溯 递归树如下：（以 candidates = [2,3,6,7], target = 7 为例） 代码：\nfunc combinationSum(candidates []int, target int) [][]int { var res [][]int var tmp []int backtrack(candidates, target, 0, 0, tmp, \u0026amp;res) return res } func backtrack(c []int, t, sum, start int, tmp []int, res *[][]int) { if sum == t { tmpp := make([]int, len(tmp)) copy(tmpp, tmp) *res = append(*res, tmpp) return } if sum \u0026gt; t { return } for i := start; i \u0026lt; len(c); i++ { tmp = append(tmp, c[i]) sum += c[i] fmt.Println(tmp) backtrack(c, t, sum, i, tmp, res) tmp = tmp[:len(tmp)-1] sum -= c[i] } } ","date":"2021年09月25日","permalink":"/posts/39-zu-he-zong-he/","summary":"给定一个无重复元素的正整数数组 candidates 和一个正整数 target ，找出 candidates 中所有可以使数字和为目标数 target 的唯一组合。","title":"39. 组合总和"},{"contents":"命令： ps -ef | grep name | grep -v grep | awk '{print $2}' | xargs kill 解释： ps -ef：查看全格式的全部进程\ngrep name：将 ps -ef 的输出作为输入，过滤出其中包含 ”name“ 的结果\ngrep -v grep：-v 代表反向选择，这条是比较迷惑的，为什么要加这条语句呢？因为之前的 grep name 这句话本身也会创建一个进程，并且该进程中也会包含 \u0026ldquo;name\u0026rdquo; 这个字段，所以 grep name 输出的列表中也会包含其自身的进程，但是该进程在 grep 执行之后就会退出，如果此时再将其 kill，会产生 kill: 32169: No such process 错误。所以这里需要使用 -v grep，反向选择，选择出不包含 grep 的条目\nawk：ps 输出的命令包含了 root、pid 等多个字段，但是 kill 只需要 pid 字段，通过该命令，可以筛选出需要的 pid，$2 表示每行第二个变量，在这个例子中就是进程号\nxargs：表示用前面命令的输出结果（也就是一系列的进程号）作为 kill 命令的参数\n实践 使用 go 进行实践 首先创建出任意一个二进制文件，这里使用 go 编写一个简单程序，并编译出来，文件名为 sleep.go：\npackage main import \u0026quot;time\u0026quot; func main() { time.Sleep(time.Hour) } 编译为二进制：\ngo build sleep.go 编译出来的二进制文件名为 sleep\n再创建一个 go 程序，用来创建多个进程：\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;os/exec\u0026quot; \u0026quot;sync\u0026quot; ) func main() { var wg sync.WaitGroup wg.Add(100) for i := 0; i \u0026lt; 100; i++ { go func() { defer wg.Done() cmd := exec.Command(\u0026quot;./sleep\u0026quot;) _, err := cmd.CombinedOutput() if err != nil { log.Fatalln(err) } }() } wg.Wait() } 此时执行 ps -ef | grep sleep，终端输出：\n501 32764 1880 0 10:16下午 ttys000 0:00.01 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox sleep 501 32660 32659 0 10:16下午 ttys009 0:00.01 ./sleep 501 32661 32659 0 10:16下午 ttys009 0:00.01 ./sleep 501 32662 32659 0 10:16下午 ttys009 0:00.01 ./sleep 501 32663 32659 0 10:16下午 ttys009 0:00.01 ./sleep .... 省略 注意，这里的第一条就是 grep 自身的进程\n我们尝试一下，不加 grep -v grep 会怎么样：\nps -ef | grep sleep | awk '{print $2}' | xargs kill 结果：\nkill: 32772: No such process 运行出错了（这里比较疑惑的是，为什么不是 32764？）\n此时再执行 ps -ef | grep sleep\n501 32784 1880 0 10:17下午 ttys000 0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox sleep 发现还有一条没被 kill 掉\n而如果使用 ps -ef | grep name | grep -v grep | awk '{print $2}' | xargs kill，则不会产生错误，并且所有带有 ”sleep“ 的进程都被 kill 掉了\n","date":"2021年09月21日","permalink":"/posts/linux-pi-liang-kill-jin-cheng/","summary":"命令： ps -ef | grep name | grep -v grep | awk '{print $2}' | xargs kill 解释： ps -ef：查看全格式的全部进程","title":"Linux 批量 kill 进程"},{"contents":"2021.9.17 梦想工场 go 实习生\n现场面，只有纸和笔（吐槽：公司的大厦正好在装修，找了半天才找到，还好出门比较早，不然多半得迟到）\n一上来就是一道算法题：字符串的比较，根据字典序，如果字符串特别大，问我用什么数据结构合适（完全不会，于是换了一道，期间为了遮掩自己的一无所知，随口瞎说了个 kmp 。。。面完才想起来，比较字符串不是可以使用双指针吗？）\n第二道是括号匹配，面试官还在描述题意，我就打断说了句：“括号匹配吧”，然后面试官说：“做过？那就换一道”，顿时想抽自己一巴掌\n第三道题直接把我整懵逼了，假如有很多辆送货车，每辆车都有一个坐标（x, y），他们需要把货物送到一个驿站，找出一个合适的坐标作为驿站，使得每辆车到驿站的距离加起来最短。（完全不会，面试官引导我说，那就先从一维开始，只有 x 坐标，比如数组下标的 [1, 3, 5] 处有三辆车，此时在哪里建驿站最合适，然后我想了一会，说建在中间最合适，偶数个就取中间两个，然后引申到二维，因为我完全不会，所以只能瞎扯，说找出所有 x 坐标的中间坐标，和所有 y 坐标的中间坐标，此时的坐标就是最近的，面试官问我为什么，我沉默了一会说不知道。。。没想到面试官说，其实你这个做法是对的，也不知道是不是在安慰我。。。后来又加了新的条件：每辆车不仅有坐标，还有运货次数，还是一维的，此时又如何建站？此时我的内心是痛苦的，能不能别折磨我了，这面试官不会是个搞算法的吧，我又胡乱扯了下，说根据运货次数进行排序，因为这个问题实在是不会，所以也没有继续往下说了）\n前面刚好提到了排序，面试官让我在纸上手写快排，幸好昨天刚练过，开始写的很熟练流程，结果有一句话卡住了，就是 n[l] = p 这句话。。。连快排这种基础算法都能卡，估计是凉透了\n问完以后又问了我 go 的优点，为什么选 go ，我随便瞎扯了一下，之后说换一个面试官问我项目，可能是因为他不太懂 go ？\n第二个面试官穿着一件印有 pincap 的短袖，一看就是个大佬，问了我项目中的一些问题：\njwt 如何防止数据篡改？（不知道，硬是瞎扯了半天，简直就是造孽，只是照着网上教程简单使用过） websocket 的发布订阅？（完全没听说过）如何辨别每一条连接？（不知道，瞎说了每条连接都有一个随机 id） 用过哪些 linux 命令，如何杀死一批进程？（说了 ps -ef | grep，但是不知道如何筛选 pid，也不知道如何处理一批） kill 和 kill - 9 的区别，我说 -9 是强制的，又继续问原理，我说 kill 的信号能被接收，kill -9 的不能被接收。。。又错了 go pprof 怎么使用的，怎么找项目里的性能问题？我把我写项目遇到的经历说了下，其间提到了 cpu 占用 500%，面试官问 500% 代表什么，我说占了 5 个核 简历上的项目哪个写的时间最长？ 介绍一下简历上的 【TCP 服务器】项目，期间我提到了心跳、长连接和 CLOSE_WAIT，面试官只是点点头，没有过多地提问 没有反问\n总结： 人生中第一次面试，给我整傻了，全程没有八股，算法题也是我没刷过的（虽然一共也没刷多少题），充分认识到了自己菜的程度，不过面试体验不错，第一个面试官全程笑脸。\n其次，只要是写到简历上的，一定要比较熟悉，起码要了解一些原理，不管你写的是熟练、了解还是简单使用过，面试官不会管你这么多，也不会问你“这是干嘛的？怎么使用？”这种特别浅的问题，像我今天的 jwt 和 websocket 就吃亏了。\n2021.9.22\n汇量科技 go 实习生 一面\n说一下你的项目 说说项目里的限流是如何实现的 new 和 make 分别是分配在堆上还是栈上？二者的区别 如何使用 pprof，pprof 除了可以分析 cpu 以外，还能分析什么 说说 go 的逃逸分析 go 如何检测逃逸（我回答说使用 go tool，面试官说可以设置一个 flag，后来网上查了下，正确答案是go run -gcflags \u0026ldquo;-m -l\u0026rdquo; (-m打印逃逸分析信息，-l禁止内联编译)） 说说 go 的垃圾回收 进程、线程、协程区别，介绍协程时我说了 gmp gmp 中 g 有哪几种状态 如何给一个程序分配内存，流程是怎样的？画图说明（不会） 说说虚拟内存 如果给进程分配虚拟内存时缺页，怎么办 说说淘汰算法（说虚拟内存时提到了会淘汰一部分页） 介绍项目的时候，我说 go 不能强制结束掉一个 goroutine，只能等待其自己退出，后来面试官说其实可以，说 runtime 有一个函数可以强制结束，但是我没听清 goroutine 初始的栈空间大小是多少？ linux 用过 awk 吗？一个文本文件，有三列，每列以空格分隔，筛选出所有第三列大于 100 的行 如果不是以空格分隔，而是以 \\t 分隔呢？，我说好像可以用 -F，（瞎说的，因为之前刚好有看过这个参数），没想到说对了，但是我不知道判断大于 100 的命令 如果是统计次数呢？（我瞎说了 wc -l，不知道对不对） redis 的持久化，哪种比较好，有没有不需要持久化的场景？ redis 是用来做什么的 说说数据库的事务，我说了说 ACID，顺便提了提隔离级别 幻读和不可重复读的区别 可重复读是如何实现的 链表和数组的区别 链表和数组哪个内存占用更大\n算法题： 括号匹配 用切片实现一个栈 最长不重复子串（没写出来，有些bug，面试官说思路差不多）\n2021.9.24 二面\n说说项目里的心跳检测和封包协议 虽然协议记录了数据包的长度，但是还是会被截断（？没听清），这时该怎么处理？ tcp 三次握手对应到 socket redis 主要用来干什么 redis 有哪几种数据结构 redis zset 是如何实现的 redis 的 io 模型\n算法题： 连续递增子序列的最大和 反转链表2\n吐槽：相比一面而言，二面的体验简直太差了，面试官在最后一分钟才进入房间，并且没开摄像头，声音很小并且断断续续，导致有些问题都没听清；全程心不在焉，不断传来点击鼠标的声音，可能是边工作边面试；回答完问题后没有任何反馈，沉默几十秒后再问下一个，场面一度十分尴尬；在随便问了几个问题后，就开始做题了，第一道做完以后让我说了下思路，第二道没完全ac，只通过了 15/20，面试官说差不多了，应该是还有一些特殊情况没处理好，然后就结束了面试。\n","date":"2021年09月17日","permalink":"/posts/wo-de-mian-jing/","summary":"2021.9.17 梦想工场 go 实习生\n现场面，只有纸和笔（吐槽：公司的大厦正好在装修，找了半天才找到，还好出门比较早，不然多半得迟到）","title":"我的面经"},{"contents":"B 树和 B+ 树的区别 来源：https://draveness.me/whys-the-design-mysql-b-plus-tree/\nB 树与 B+ 树的最大区别就是，B 树可以在非叶结点中存储数据，但是 B+ 树的所有数据其实都存储在叶子节点中，当一个表底层的数据结构是 B 树时，假设我们需要访问所有『大于 4，并且小于 9 的数据』： 如果不考虑任何优化，在上面的简单 B 树中我们需要进行 4 次磁盘的随机 I/O 才能找到所有满足条件的数据行：\n加载根节点所在的页，发现根节点的第一个元素是 6，大于 4； 通过根节点的指针加载左子节点所在的页，遍历页面中的数据，找到 5； 重新加载根节点所在的页，发现根节点不包含第二个元素； 通过根节点的指针加载右子节点所在的页，遍历页面中的数据，找到 7 和 8； 当然我们可以通过各种方式来对上述的过程进行优化，不过 B 树能做的优化 B+ 树基本都可以，所以我们不需要考虑优化 B 树而带来的收益，直接来看看什么样的优化 B+ 树可以做，而 B 树不行。\n由于所有的节点都可能包含目标数据，我们总是要从根节点向下遍历子树查找满足条件的数据行，这个特点带来了大量的随机 I/O，也是 B 树最大的性能问题。\nB+ 树中就不存在这个问题了，因为所有的数据行都存储在叶节点中，而这些叶节点可以通过『指针』依次按顺序连接，当我们在如下所示的 B+ 树遍历数据时可以直接在多个子节点之间进行跳转，这样能够节省大量的磁盘 I/O 时间，也不需要在不同层级的节点之间对数据进行拼接和排序；通过一个 B+ 树最左侧的叶子节点，我们可以像链表一样遍历整个树中的全部数据，我们也可以引入双向链表保证倒序遍历时的性能。\n我的理解：因为 B+ 树把所有数据存储在了叶节点，并且叶节点全部连接起来形成了一个双向链表，所以在范围查找时特别高效，只需要通过指针前后移动即可，这也是其相对于 B 树而言最大的优势，不再需要像 B 树那样进行回溯操作了（比如上面的例子中，要查找一个范围内的数据，需要在叶节点和根节点之间移动）。\n** 补充一点：** B+ 树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确\n这个很好理解，由于 B- 树节点内部每个 key 都带着 data 域，而 B+ 树节点只存储 key 的副本，真实的 key 和 data 域都在叶子节点存储。前面说过磁盘是分 block 的，一次磁盘 IO 会读取若干个 block，具体和操作系统有关，那么由于磁盘 IO 数据大小是固定的，在一次 IO 中，单个元素越小，量就越大。这就意味着 B+ 树单次磁盘 IO 的信息量大于 B- 树，从这点来看 B+ 树相对 B- 树磁盘 IO 次数少。\n总结 来源：https://leetcode-cn.com/circle/discuss/F7bKlM/\n1. B+ 树和 B 树的区别？ B 树非叶子结点和叶子结点都存储数据,因此查询数据时，时间复杂度最好为 O(1)，最坏为 O(log n)。 B+ 树只在叶子结点存储数据，非叶子结点存储关键字，且不同非叶子结点的关键字可能重复，因此查询数据时，时间复杂度固定为 O(log n)。\nB+ 树叶子结点之间用链表相互连接，因而只需扫描叶子结点的链表就可以完成一次遍历操作，B树只能通过中序遍历。\n2. 为什么 B+ 树比 B 树更适合应用于数据库索引？ B+ 树更加适应磁盘的特性，相比 B 树减少了 I/O 读写的次数。由于索引文件很大因此索引文件存储在磁盘上，B+ 树的非叶子结点只存关键字不存数据，因而单个页可以存储更多的关键字，即一次性读入内存的需要查找的关键字也就越多，磁盘的随机 I/O 读取次数相对就减少了。\nB+ 树的查询效率相比 B树 更加稳定，由于数据只存在在叶子结点上，所以查找效率固定为 O(log n)。\nB+ 树叶子结点之间用链表有序连接，所以扫描全部数据只需扫描一遍叶子结点，利于扫库和范围查询；B 树由于非叶子结点也存数据，所以只能通过中序遍历按序来扫。也就是说，对于范围查询和有序遍历而言，B+ 树的效率更高。\n引申 为什么不使用二叉搜索树？ 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。\n你可以想象一下一棵100万节点的平衡二叉树，树高20（2^20 = 1048576）。一次查询可能需要访问20个数据块。在机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。\n为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。\n","date":"2021年09月11日","permalink":"/posts/mysql-wei-shi-me-shi-yong-b-shu/","summary":"B 树和 B+ 树的区别 来源：https://draveness.me/whys-the-design-mysql-b-plus-tree/","title":"MySQL 为什么使用 B+ 树"},{"contents":"来源于官方文档（https://grpc.io/docs/languages/cpp/quickstart/），但是其中有一些坑，特此记录，系统为 macOS 11.0.1 (20B29)：\n1. 设置目录（很重要，必须执行该步骤） 选择一个目录来保存本地安装的软件包。此页面假定环境变量MY_INSTALL_DIR包含此目录路径。例如：\nexport MY_INSTALL_DIR=$HOME/.local mkdir -p $MY_INSTALL_DIR export PATH=\u0026quot;$MY_INSTALL_DIR/bin:$PATH\u0026quot; 这一步没什么好说的，照着文档做就好。\n2. 安装 cmake brew install cmake 3. 安装构建 gRPC 所需的基本工具 brew install autoconf automake libtool pkg-config 4. 从 GitHub 上 clone gRPC 源码 git clone --recurse-submodules -b v1.38.0 https://github.com/grpc/grpc 5. 构建和安装 gRPC、Protocol Buffers 和 Abseil 以下来自官方，但是需要修改一些内容：\n$ cd grpc $ mkdir -p cmake/build $ pushd cmake/build $ cmake -DgRPC_INSTALL=ON \\ -DgRPC_BUILD_TESTS=OFF \\ -DCMAKE_INSTALL_PREFIX=$MY_INSTALL_DIR \\ ../.. $ make -j $ make install $ popd $ mkdir -p third_party/abseil-cpp/cmake/build $ pushd third_party/abseil-cpp/cmake/build $ cmake -DCMAKE_INSTALL_PREFIX=$MY_INSTALL_DIR \\ -DCMAKE_POSITION_INDEPENDENT_CODE=TRUE \\ ../.. $ make -j $ make install $ popd 这一步遇到的问题比较多。 问题 1： 执行第一个 make install 后产生如下错误：\n-- Install configuration: \u0026quot;\u0026quot; CMake Error at cmake_install.cmake:41 (file): file cannot create directory: /lib. Maybe need administrative privileges. 在网上查了下，是因为 macOS 从 big sur 开始就不允许对根目录进行读写操作了，一开始还以为要解决根目录的读写问题，后来在 GitHub 上找到了对应的 issues（https://github.com/grpc/grpc/issues/24660），解决方法是执行上述文章一开始的第一步，也就是设置目录。（还好没折腾系统）\n问题 2: make -j 电脑直接卡死 还没找到原因，不过找到了解决方法：make -j 4 ，即在 j 的后面指定一个数字即可，这个数字貌似代表的意思是开启多个线程，并发的进行编译。（难道不加数字的话默认开启最大线程？）\n问题 3： 执行第二个 make install 后产生如下错误：\n3 warnings and 9 errors generated. make[2]: *** [absl/base/CMakeFiles/log_severity.dir/log_severity.cc.o] Error 1 make[1]: *** [absl/base/CMakeFiles/log_severity.dir/all] Error 2 15 warnings and 20 errors generated. 15 warnings and 20 errors generated. make[2]: *** [absl/time/CMakeFiles/civil_time.dir/internal/cctz/src/civil_time_detail.cc.o] Error 1 make[2]: *** [absl/time/CMakeFiles/time_zone.dir/internal/cctz/src/time_zone_format.cc.o] Error 1 make[1]: *** [absl/time/CMakeFiles/civil_time.dir/all] Error 2 15 warnings and 20 errors generated. 15 warnings and 20 errors generated. 15 warnings and 20 errors generated. make[2]: *** [absl/time/CMakeFiles/time_zone.dir/internal/cctz/src/time_zone_if.cc.o] Error 1 make[2]: *** [absl/time/CMakeFiles/time_zone.dir/internal/cctz/src/time_zone_lookup.cc.o] Error 1 make[2]: *** [absl/time/CMakeFiles/time_zone.dir/internal/cctz/src/time_zone_info.cc.o] Error 1 15 warnings and 20 errors generated. make[2]: *** [absl/time/CMakeFiles/time_zone.dir/internal/cctz/src/time_zone_impl.cc.o] Error 1 make[1]: *** [absl/time/CMakeFiles/time_zone.dir/all] Error 2 make: *** [all] Error 2 总而言之就是编译失败，解决方法是： 将第二个 cmake 替换为如下语句，就是多加了一行 -DCMAKE_CXX_STANDARD=11 ，指定使用 c++ 11 进行编译。\ncmake -DCMAKE_INSTALL_PREFIX=$MY_INSTALL_DIR \\ -DCMAKE_POSITION_INDEPENDENT_CODE=TRUE \\ -DCMAKE_CXX_STANDARD=11 \\ ../.. 记得如果之前已经执行过 cmake，则需要先清除（rm -rf third_party/abseil-cpp/cmake/build，再执行 mkdir -p third_party/abseil-cpp/cmake/build），再重新执行上面的 cmake 语句。之后就可以正确的 make install 了。\n之后就可以愉快的开始构建示例了：\ncd examples/cpp/helloworld mkdir -p cmake/build pushd cmake/build cmake -DCMAKE_PREFIX_PATH=$MY_INSTALL_DIR ../.. make -j 构建完成后，运行服务端：\n# 运行服务器： ./greeter_server 新创建一个终端：\n./greeter_client 输出：\nGreeter received: Hello world ","date":"2021年08月31日","permalink":"/posts/mac-xia-gou-jian-ji-yu-c-de-grpc/","summary":"来源于官方文档（https://grpc.io/docs/languages/cpp/quickstart/），但是其中有一些坑，特此记录，系统为 macOS 11.0.1 (20B29)：\n1. 设置目录（很重要，必须执行该步骤） 选择一个目录来保存本地安装的软件包。此页面假定环境变量MY_INSTALL_DIR包含此目录路径。例如：","title":"Mac 下构建基于 C++ 的 gRPC"},{"contents":"表锁 语法：\n加锁：\n加读锁 LOCK TABLES [tablename] READ;\n加写锁 LOCK TABLES [tablename] WRITE;\n释放锁： UNLOCK TABLES;\n加锁后会阻塞其他线程的部分操作，同时也会对加锁线程进行一些限制，直到加锁的线程释放锁。\n读锁和写锁的区别： 当加 读锁 后，其他线程的 SELECT 语句不会被阻塞，但是 INSERT、UPDATE、DELETE 这些写语句都会被阻塞。此外，加锁线程只能执行读语句，不能执行写语句， 否则会产生错误 Table 'sort_demo' was locked with a READ lock and can't be updated。\n当加 写锁 后，其他线程的读写语句都会被阻塞，但是加锁线程的读写语句都可以正常执行。\n可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。\n在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。\nMDL 锁（元数据锁） MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。\n读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。\n读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。\n虽然 MDL 锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，我经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了。 我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是MDL 读锁，因此可以正常执行。\n之后 session C 会被blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要MDL 写锁，因此只能被阻塞。\n如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。\n如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。\n你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。\n划重点：一旦出现写锁等待，不但当前操作会被阻塞，同时还会阻塞后续该表的所有操作。事务一旦申请 到 MDL 锁后，直到事务执行完才会将锁释放。\n基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段？\n首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。\n但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？\n这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL写锁 最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。\nMariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。\nALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... 2. 全局锁 顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等） 和更新类事务的提交语句。\n全局锁的典型使用场景是，做全库逻辑备份，也就是把整库每个表都 select 出来存成文本。\n悲观锁 乐观锁 用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将version 字段的值一同读出，数据每更新一次，对此version值加 1。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。\n举例\n1、数据库表设计\n三个字段，分别是id, value, version\nselect id,value,version from TABLE where id=#{id} 2、每次更新表中的value字段时，为了防止发生冲突，需要这样操作\nupdate TABLE set value=2,version=version+1 where id=#{id} and version=#{version}; 举个例子，比如有下面两个事务：\nA B select id,value,version from TABLE where id=1，结果：version = 1 select id,value,version from TABLE where id=1，结果：version=1 拿上面查询出来的 version 进行更新，update set version=version+1 where version=1，结果：version = 2 update set version=version+1 where version=1，此时version已经被 A 更新成了 2，所以这条 sql 语句不会执行成功，从而避免旧数据覆盖新数据 死锁 这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。\nfor update 加行锁还是表锁？ 实验： 有如下表：\ncreate for_update_is_lock_row_or_lock_table( id int primary key auto_increment, d int, c int, key(c)c ); insert into for_update_is_lock_row_or_lock_table(d, c) values(1, 1) insert into for_update_is_lock_row_or_lock_table(d, c) values(2, 2) insert into for_update_is_lock_row_or_lock_table(d, c) values(3, 3) 实验 1 使用索引列作为条件查询 事务1：\nupdate for_update_is_lock_row_or_lock_table set d=11 where c= 1; 事务2： 更新同一列数据\nupdate for_update_is_lock_row_or_lock_table set d=1 where c= 1; 上面的语句被被阻塞\nupdate for_update_is_lock_row_or_lock_table set d=22 where c= 2; 但是这条语句不会被阻塞，说明此时加的是行锁\n实验2 使用非索引列作为条件查询 事务1：\nupdate for_update_is_lock_row_or_lock_table set c=11 where d= 1; 事务2:\nupdate for_update_is_lock_row_or_lock_table set c=22 where d=2; 被阻塞，此时不是同一行但却被阻塞，说明此时加的是表锁\n结论 如果查询条件用了索引/主键，那么select \u0026hellip;.. for update就会进行行锁。\n如果是普通字段(没有索引/主键)，那么select \u0026hellip;.. for update就会进行锁表。\n","date":"2021年08月29日","permalink":"/posts/mysql-suo/","summary":"表锁 语法：\n加锁：\n加读锁 LOCK TABLES [tablename] READ;","title":"MySQL 锁"},{"contents":"1. 未提交读（脏读） 事务的修改，即使没有提交，对其他事务也是可见的。事务可以读取未提交的数据，称之为 脏读。它存在 4 个常见问题（脏读、不可重复读、幻读、丢失更新）。 2. 提交读（不可重复读） 就是一个事务要等另一个事务提交后才能读取数据。 它解决了脏读问题，存在 3 个常见问题（不可重复读、幻读、丢失更新）。 新的问题：更新丢失（提交覆盖） 新的问题：不可重复读 新的问题：回滚丢失（回滚覆盖） 和 更新丢失 一样，撤消一个事务时，在该事务内的写操作要回滚，把其它已提交的事务写入的数据覆盖了。\n3. 可重复读 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 就是在开始读取数据（事务开启）时，不再允许修改操作 。它解决了脏读和不可重复读，还存在2个常见问题（幻读、丢失更新）。\n实现原理： 数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。\n适用场景： 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。\n这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。\nTips： 幻读和不可重复读的区别 摘自网上： (1) 不可重复读 主要是说多次读取一条记录, 发现该记录中某些列值被修改过 幻读 主要是说多次读取一个范围内的记录(包括直接查询所有记录结果或者做聚合统计), 发现结果 不一致(标准档案一般指记录增多, 记录的减少应该也算是幻读)。 (2) 幻读问题对应的是插入 INSERT 操作 “幻读”是指读的过程中，某些元组被增加或删除，这样进行一些集合操作，比如算总数，平均值等等，就 会每次算出不一样的数。所以“不可重复读”和“幻读”都是读的过程中数据前后不一致，只是前者侧重于修 改，后者侧重于增删。个人认为，严格来讲“幻读”可以被称为“不可重复读”的一种特殊情况，没错的。但 是从数据库管理的角度来看二者是有区别的。解决“不可重复读”只要加行级锁就可以了。而解决“幻读”则 需要加表级锁，或者采用其他更复杂的技术，总之代价要大许多。这是搞数据库的那帮家伙非要把这两者 区分开的动机吧。\n4. 可串行化 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。\n串行化是最高的隔离级别，会强制事务串行执行，它将隔离问题全部解决，但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 ","date":"2021年08月27日","permalink":"/posts/mysql-de-shi-wu-ge-chi-ji-bie/","summary":"1. 未提交读（脏读） 事务的修改，即使没有提交，对其他事务也是可见的。事务可以读取未提交的数据，称之为 脏读。它存在 4 个常见问题（脏读、不可重复读、幻读、丢失更新）。 2.","title":"MySQL 的事务隔离级别"},{"contents":"有一对兔子，从出生后的第五个月起每月生出一对小兔子（即满4月就开始生小兔），小兔子也会出生从第五个月起每月生一对小免子。假如兔子不会死，第n个月时，兔群有多少对兔子。\n输入描述: 第n月（n为自然数，n\u0026lt;101）\n输出描述: 第 n 个月时，兔子的对数（免群的兔子有多少对兔子）。 示例 1 输入 5 输出 2 说明 第五个月时，有两对兔子\n方法1 动态规划 ACM 模式\npackage main import ( \u0026quot;fmt\u0026quot; ) func rabbit(n int) int { dp := make([]int, n) dp[0] = 1 dp[1] = 1 dp[2] = 1 dp[3] = 1 for i := 4; i \u0026lt; n; i++ { dp[i] = dp[i-1] + dp[i-4] } return dp[n-1] } func main() { var input int fmt.Scan(\u0026amp;input) v := rabbit(input) fmt.Println(v) } ","date":"2021年08月21日","permalink":"/posts/niu-ke-bi-shi-ti-tu-zi-fan-yan/","summary":"有一对兔子，从出生后的第五个月起每月生出一对小兔子（即满4月就开始生小兔），小兔子也会出生从第五个月起每月生一对小免子。假如兔子不会死，第n个月时，兔群有多少对兔子。\n输入描述: 第n月（n为自然数，n\u0026lt;101）\n输出描述: 第 n 个月时，兔子的对数（免群的兔子有多少对兔子）。 示例 1 输入 5 输出 2 说明 第五个月时，有两对兔子","title":"[牛客][笔试题] 兔子繁衍"},{"contents":"types zskiplistNode typedef struct zskiplistNode { sds ele; // 成员 double score; // 分值 struct zskiplistNode *backward; // 后退指针 // 层 struct zskiplistLevel { struct zskiplistNode *forward; // 前进指针 unsigned long span; // 跨度 } level[]; } zskiplistNode; zskiplist typedef struct zskiplist { struct zskiplistNode *header, *tail; unsigned long length; int level; // 当前跳跃表内，层数最大的那个节点的层数 } zskiplist; zset typedef struct zset { dict *dict; zskiplist *zsl; } zset; functions zslCreate /* Create a new skiplist. */ zskiplist *zslCreate(void) { int j; zskiplist *zsl; zsl = zmalloc(sizeof(*zsl)); zsl-\u0026gt;level = 1; // 初始最高层数为 1 zsl-\u0026gt;length = 0; // 不包含任何节点 // header 指向一个空节点，该节点的有 32 层（即 level[] 的长度为 32） zsl-\u0026gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); // 为头结点每层的属性进行初始化 for (j = 0; j \u0026lt; ZSKIPLIST_MAXLEVEL; j++) { zsl-\u0026gt;header-\u0026gt;level[j].forward = NULL; zsl-\u0026gt;header-\u0026gt;level[j].span = 0; } zsl-\u0026gt;header-\u0026gt;backward = NULL; zsl-\u0026gt;tail = NULL; return zsl; } zslInsert 向 skiplist 中插入一个节点。\n/* Insert a new node in the skiplist. Assumes the element does not already * exist (up to the caller to enforce that). The skiplist takes ownership * of the passed SDS string 'ele'. */ // score: 分数 // ele: 成员，是一个 sds 字符串类型 zskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) { // update 用来保存每层的某个节点，该节点是待插入节点的前一个节点，例如： // // head // level[2] ---- 1---------- 6 ----- 10 // level[1] ---- 1 ----- 4 - 6 ----- 10 插入一个 7 // level[0] ---- 1 - 2 - 4 - 6 - 8 - 10 // // 这里先不说明插入的详细过程，只通过上面的跳表就可以知道，7 会被插入到 6 的后面， // 并且假设 7 的高度为 3，此时的 update 数组内容如下所示： // // update[2] = 6，代表第三层的 6 是待插入节点 7 的前一个节点。 // update[1] = 6, update[0] = 6，同理。之后会遍历 update 数组，并将数组中每个元素 // 的 forward 指向新节点。 // // 最终结果为： // head // level[2] ---- 1 --------- 6 - 7 ----- 10 // level[1] ---- 1 ----- 4 - 6 - 7 ----- 10 插入一个 7 // level[0] ---- 1 - 2 - 4 - 6 - 7 - 8 - 10 zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; // unsigned int rank[ZSKIPLIST_MAXLEVEL]; int i, level; serverAssert(!isnan(score)); x = zsl-\u0026gt;header; // 从 zsl 的最高层开始查找，并不断下降，直到最后到达第 1 层 // Tips: 什么是最高层？ // 比如 zsl 当前有 3 个节点（不算头结点），每个节点的层数 // 分别为 4，2，5，那么 zsl-\u0026gt;level 就是节点中层数最高的 5 for (i = zsl-\u0026gt;level-1; i \u0026gt;= 0; i--) { /* store rank that is crossed to reach the insert position */ rank[i] = i == (zsl-\u0026gt;level-1) ? 0 : rank[i+1]; // 循环需要满足的条件如下： // 1. level[i] 前进指针指向一个非 null 节点，且前进指针指向的节点的分值 // 小于当前插入值的分值，那么就可以向前移动，继续查找，例如有如下跳表： // // head // level[2] ---- 1---------- 4 ----- 6 // level[1] ---- 1 ----- 3 - 4 ----- 6 插入一个 8 // level[0] ---- 1 - 2 - 3 - 4 - 5 - 6 // // 此时跳表的 level 为 3（此时外层的 for 循环中的 i = 3-1 = 2）， // 从 head 的 level[3-1] 开始找起，level[2] 的前进指针指向了 1，不为空， // 且要插入的节点分值为 8，大于 1，所以可以将 x 移动到当前节点的 forward 节点， // 即移动到 4。 // // 4 还是小于 8，继续向前移动，移动到 6，此时 6 的前进指针为 null，所以不会进入 // while 循环了，继续向下执行，update[2] = x = 当前节点 6。至此，此轮 for 循环 // 结束。 // // 继续进行第二轮 for 循环，此时 i = 1，这代表着现在降到了跳表的第二层。第二层的 // 6 依然指向 null，所以不进入 while，update[1] = 当前节点 6。 // // 第三轮 for 循环，此时 i = 0，这代表已经来到了跳表的第一层，此时的 6 依然指向 // null，update[0] = 6。 // // // 2. level[i] 前进指针指向一个非 null 节点，且前进指针指向的节点的分值 // 等于当前插入值的分值，也就是遇到了分值一样的情况了，此时会继续判断二者的 // 成员值，因为成员值是一个字符串，所以会根据字典顺序进行比较，如果 forward // 的成员值小于要插入的成员值，则继续向前移动。 // // 简单的说，如果分值相同，会继续根据成员值进行判断。 // 比如： // // 节点 1 节点 2 （省略头结点，头节点指向节点 1） // +--------+ +--------+ // + l3 + ----\u0026gt; + l3 + // + l2 + ----\u0026gt; + l2 + // + l1 + ----\u0026gt; + l1 + 添加一个新节点，成员值为 bbbbb， // + 30.0 + ----\u0026gt; + 50.0 + 分值为 30.0 // + aaaaa + ----\u0026gt; + ccccc + // +--------+ +--------+ // // 开始会从头结点开始查找，找到节点 1，发现节点 1 的 score 为 30.0，继续判断成员值， // 因为 bbbbb \u0026gt; aaaaa，所以移动到节点 1，看后一个节点 节点 2 的 score 值，因为 // 50 \u0026gt; 30，所以停止移动，新节点将插入到节点 1 的后面 // // 节点 1 新节点 // +--------+ +--------+ // + l3 + ----\u0026gt; + l3 + // + l2 + ----\u0026gt; + l2 + // + l1 + ----\u0026gt; + l1 + // + 30.0 + ----\u0026gt; + 30.0 + // + aaaaa + ----\u0026gt; + bbbbb + // +--------+ +--------+ while (x-\u0026gt;level[i].forward \u0026amp;\u0026amp; // level[i] 的前进指针指向非 null 节点 // 如果插入节点的分值大于 level[i] 指向节点的分值 (x-\u0026gt;level[i].forward-\u0026gt;score \u0026lt; score || // 插入节点分值等于 level[i] 指向节点的分值 (x-\u0026gt;level[i].forward-\u0026gt;score == score \u0026amp;\u0026amp; // level[i] 指向节点的 ele 小于插入节点的 ele // Tips: sdscmp(x, y), 当 x \u0026lt; y 时，返回一个负数 sdscmp(x-\u0026gt;level[i].forward-\u0026gt;ele,ele) \u0026lt; 0))) { // rank[i] += x-\u0026gt;level[i].span; // 移动到下一个节点 x = x-\u0026gt;level[i].forward; } // 记录当前层的待插入节点的前一个节点 update[i] = x; } /* we assume the element is not already inside, since we allow duplicated * scores, reinserting the same element should never happen since the * caller of zslInsert() should test in the hash table if the element is * already inside or not. */ level = zslRandomLevel(); // 如果随机生成的高度大于当前 zsl 的最大高度，那么会多出来独立的几层，需要连接起来， // 例如下面这个例子，新插入一个 8，且 8 的随机层数为 5，那么就会多出来 2 层，这两 // 层是独立的，需要将其与 head 连接起来。 // // head 8 （独立） // 8 （独立） // level[2] ---- 1---------- 4 ----- 6 --- 8 // level[1] ---- 1 ----- 3 - 4 ----- 6 --- 8 // level[0] ---- 1 - 2 - 3 - 4 - 5 - 6 --- 8 // // 连接后： // // head // level[4] ------------------------------ 8 // level[3] ------------------------------ 8 // level[2] ---- 1---------- 4 ----- 6 --- 8 // level[1] ---- 1 ----- 3 - 4 ----- 6 --- 8 // level[0] ---- 1 - 2 - 3 - 4 - 5 - 6 --- 8 if (level \u0026gt; zsl-\u0026gt;level) { // 为多出的这几层进行连接 for (i = zsl-\u0026gt;level; i \u0026lt; level; i++) { rank[i] = 0; // 独立的节点，那么需要用头结点作为 update update[i] = zsl-\u0026gt;header; update[i]-\u0026gt;level[i].span = zsl-\u0026gt;length; } // 更新最大高度 zsl-\u0026gt;level = level; } // 创建一个节点 x = zslCreateNode(level,score,ele); // 将新节点的每一层与跳表中对应的那一层关联起来 // 比如：update[0] 指向 x[0]，update[1] 指向 x[1] // 文字不好理解，看下面的图示： // // head // level[2] ---- 1---------- 4 ----- 8 // level[1] ---- 1 ----- 3 - 4 ----- 8 插入一个 7, 假设层数为 2 // level[0] ---- 1 - 2 - 3 - 4 - 5 - 8 // // 因为层数为 2，所以 for 2 次，每次都会将 7 的层数关联起来 // // head // level[2] ---- 1---------- 4 --------- 8 // level[1] ---- 1 ----- 3 - 4 ----- 7 - 8 第二次：4 指向 7，7 指向 8 // level[0] ---- 1 - 2 - 3 - 4 - 5 - 7 - 8 第一次循环，将 7 的第一层关联起来 // 5 指向 7，7 指向 8 for (i = 0; i \u0026lt; level; i++) { // 就和链表的添加操作一样，要将 x 添加到 update 的后面， // 需要先将 x 指向 update 的后一个节点，再将 update 指向 x // // update -\u0026gt; update.next, 要将 x 添加到 update 后面 // 1. x -\u0026gt; update.next // 2. update -\u0026gt; x // 结果：update -\u0026gt; x -\u0026gt; update.next x-\u0026gt;level[i].forward = update[i]-\u0026gt;level[i].forward; update[i]-\u0026gt;level[i].forward = x; /* update span covered by update[i] as x is inserted here */ x-\u0026gt;level[i].span = update[i]-\u0026gt;level[i].span - (rank[0] - rank[i]); update[i]-\u0026gt;level[i].span = (rank[0] - rank[i]) + 1; } /* increment span for untouched levels */ for (i = level; i \u0026lt; zsl-\u0026gt;level; i++) { update[i]-\u0026gt;level[i].span++; } // 更新新添加节点的回退指针，如果 update[0] 是 header，那么就将回退指针设置为 null， // 不是 header，则设置为 update[0] // 这里为什么只有 update[0] 呢？因为每个节点只有一个后退指针， // 所以这里只需为第 1 层设置后退节点 x-\u0026gt;backward = (update[0] == zsl-\u0026gt;header) ? NULL : update[0]; // 将新节点的下一节点的 backward 指向新节点（如果有下一个节点的话） // 就和双链表的添加操作一样 if (x-\u0026gt;level[0].forward) x-\u0026gt;level[0].forward-\u0026gt;backward = x; else // 否则说明新节点是最后一个节点，此时更新 tail 为新节点 zsl-\u0026gt;tail = x; zsl-\u0026gt;length++; // 节点数量 + 1 return x; } zslDeleteNode /* Internal function used by zslDelete, zslDeleteRangeByScore and * zslDeleteRangeByRank. */ void zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) { int i; // 删除每层中的 x for (i = 0; i \u0026lt; zsl-\u0026gt;level; i++) { // 如果 update 的下一个节点是 x if (update[i]-\u0026gt;level[i].forward == x) { update[i]-\u0026gt;level[i].span += x-\u0026gt;level[i].span - 1; // 和链表的删除操作一样，将被删除的前一个节点指向要删除的后一个节点 update[i]-\u0026gt;level[i].forward = x-\u0026gt;level[i].forward; } else { update[i]-\u0026gt;level[i].span -= 1; } } // 如果被删除节点有下一个节点，将下一个节点的后退指针指向被删除节点的前一个节点 if (x-\u0026gt;level[0].forward) { x-\u0026gt;level[0].forward-\u0026gt;backward = x-\u0026gt;backward; } else { // 否则说明被删除节点是跳表中的最后一个节点，更新 tail 为被删除 // 节点的前一个 zsl-\u0026gt;tail = x-\u0026gt;backward; } // 更新跳表的最大高度字段 // 从最高层开始，如果头结点在该层没有下一个节点，那么说明该层为空，level--， // 循环删除所有空层 while(zsl-\u0026gt;level \u0026gt; 1 \u0026amp;\u0026amp; zsl-\u0026gt;header-\u0026gt;level[zsl-\u0026gt;level-1].forward == NULL) zsl-\u0026gt;level--; // 更新跳表的节点数量 zsl-\u0026gt;length--; } zslDelete /* Delete an element with matching score/element from the skiplist. * The function returns 1 if the node was found and deleted, otherwise * 0 is returned. * * If 'node' is NULL the deleted node is freed by zslFreeNode(), otherwise * it is not freed (but just unlinked) and *node is set to the node pointer, * so that it is possible for the caller to reuse the node (including the * referenced SDS string at node-\u0026gt;ele). */ int zslDelete(zskiplist *zsl, double score, sds ele, zskiplistNode **node) { zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; int i; // 整体流程： // // head // level[4] ------------------------------ 8 // level[3] ------------------------------ 8 // level[2] ---- 1---------- 4 ----- 6 --- 8 删除 6 // level[1] ---- 1 ----- 3 - 4 ----- 6 --- 8 // level[0] ---- 1 - 2 - 3 - 4 - 5 - 6 --- 8 // // 1. 先从最高处 level[4] 开始，下一个节点为 8，比 6 大，不满足 while 条件， // x = head.level[4], update[4] = x，降到 level[3] // // 2. level[3] 和 [4] 情况一样， // x = head.level[3], update[3] = x // // 3. level[2] 会从头结点前进到节点 4， // x = 4, update[2] = x（其实当 score 相同时，还会继续比较 ele，这里省略） // // 4. level[1] =\u0026gt; x = 4, update[1] = x // 5. level[0] =\u0026gt; x = 5, update[0] = x // 至此，update 记录了被删除在每层的前一个节点，x 是第一层的被删除节点的前一个 // 节点 // 和插入操作一样，先查找跳表，找到被删除节点在每层的前一个节点 x = zsl-\u0026gt;header; for (i = zsl-\u0026gt;level-1; i \u0026gt;= 0; i--) { while (x-\u0026gt;level[i].forward \u0026amp;\u0026amp; (x-\u0026gt;level[i].forward-\u0026gt;score \u0026lt; score || (x-\u0026gt;level[i].forward-\u0026gt;score == score \u0026amp;\u0026amp; sdscmp(x-\u0026gt;level[i].forward-\u0026gt;ele,ele) \u0026lt; 0))) { x = x-\u0026gt;level[i].forward; } update[i] = x; } /* We may have multiple elements with the same score, what we need * is to find the element with both the right score and object. */ // 此时的 x 是被删除节点在第一层的前一个节点，获取 x 的下一个节点 x = x-\u0026gt;level[0].forward; // 如果 x 存在，且 score 和 ele 都相同，调用删除节点函数进行删除 if (x \u0026amp;\u0026amp; score == x-\u0026gt;score \u0026amp;\u0026amp; sdscmp(x-\u0026gt;ele,ele) == 0) { zslDeleteNode(zsl, x, update); if (!node) zslFreeNode(x); else *node = x; return 1; } return 0; /* not found */ } ","date":"2021年08月17日","permalink":"/posts/redis-zset-yuan-ma-yue-du/","summary":"types zskiplistNode typedef struct zskiplistNode { sds ele; // 成员 double score; // 分值 struct zskiplistNode *backward; // 后退指针 // 层 struct zskiplistLevel { struct zskiplistNode *forward; // 前进指针 unsigned long span; // 跨度 } level[]; } zskiplistNode; zskiplist typedef struct zskiplist { struct zskiplistNode *header, *tail; unsigned long length; int level; // 当前跳跃表内，层数最大的那个节点的层数 } zskiplist; zset typedef struct zset { dict *dict; zskiplist *zsl; } zset; functions zslCreate /* Create a new skiplist.","title":"Redis zset 源码阅读"},{"contents":" 给你一个二进制数组 nums ，你需要从中删掉一个元素。\n请你在删掉元素的结果数组中，返回最长的且只包含 1 的非空子数组的长度。\n如果不存在这样的子数组，请返回 0 。\n提示 1：\n输入：nums = [1,1,0,1] 输出：3 解释：删掉位置 2 的数后，[1,1,1] 包含 3 个 1 。 示例 2：\n输入：nums = [0,1,1,1,0,1,1,0,1] 输出：5 解释：删掉位置 4 的数字后，[0,1,1,1,1,1,0,1] 的最长全 1 子数组为 [1,1,1,1,1] 。 示例 3：\n输入：nums = [1,1,1] 输出：2 解释：你必须要删除一个元素。 示例 4：\n输入：nums = [1,1,0,0,1,1,1,0,1] 输出：4 示例 5：\n输入：nums = [0,0,0] 输出：0\n提示：\n1 \u0026lt;= nums.length \u0026lt;= 10^5 nums[i] 要么是 0 要么是 1 。\n方法 1 滑动窗口 代码如下：\nfunc longestSubarray(nums []int) int { var ( l, r int windowHasZero bool // 当前窗口内是否有 0 zeroIndex int res int ) for r \u0026lt; len(nums) { if nums[r] == 0 \u0026amp;\u0026amp; !windowHasZero { windowHasZero = true zeroIndex = r } else if nums[r] == 0 \u0026amp;\u0026amp; windowHasZero { z := zeroIndex // 更新当前位置为新的 zeroIndex zeroIndex = r res = max(res, r-l-1) // l 移动到之前 0 所在位置之后 l = z + 1 } r++ } res = max(res, r-l-1) return res } func max(x, y int) int { if x \u0026gt; y { return x } return y } ","date":"2021年08月12日","permalink":"/posts/leetcode-1493-shan-diao-yi-ge-yuan-su-yi-hou-quan-wei-1-de-zui-chang-zi-shu-zu/","summary":"给你一个二进制数组 nums ，你需要从中删掉一个元素。\n请你在删掉元素的结果数组中，返回最长的且只包含 1 的非空子数组的长度。","title":"leetcode 1493. 删掉一个元素以后全为 1 的最长子数组"},{"contents":"使用国内镜像，目前已知Github国内镜像网站有 github.com.cnpmjs.org 和 git.sdut.me/ 。速度根据各地情况而定，在clone某个项目的时候将 github.com 替换为 github.com.cnpmjs.org 即可。\n来源：https://blog.csdn.net/hzlarm/article/details/115415038\n","date":"2021年08月11日","permalink":"/posts/git-clone-su-du-man/","summary":"使用国内镜像，目前已知Github国内镜像网站有 github.com.cnpmjs.org 和 git.sdut.me/ 。速度根据各地情况而定，在clone某个项目的时候将 github.","title":"git clone 速度慢的解决方法"},{"contents":"在一如既往地执行 git push 到 Github 时，出现了一个从未遇到过的错误：\nremote: fatal error in commit_refs To https://github.com/xxx ! [remote rejected] master -\u0026gt; master (failure) error: failed to push some refs to 'https://github.com/xxx' 解决 git fsck git gc 参考： https://stackoverflow.com/questions/37341960/how-do-i-fix-remote-fatal-error-in-commit-refs-errors-trying-to-push-with-git#comment75666427_37342002\n","date":"2021年08月11日","permalink":"/posts/git-push-error-remote-fatal-error-in-commit_refs/","summary":"在一如既往地执行 git push 到 Github 时，出现了一个从未遇到过的错误：","title":"git push error: remote: fatal error in commit_refs"},{"contents":"引子 定义一个全局变量 chan，然后创建两个 main goroutine，一个往全局 chan 里发送数据，一个从全局 chan 里读取数据，会发生什么？\n这个奇葩问题是我在写一个 server 项目时遇到的，当时的情景是：想为 server 添加一个心跳检测功能，客户端每隔一段时间就向服务端发送一个心跳包，服务端这边维持一个定时器，如果收到心跳包则重置，如果定时器到时则认为客户端已断开，关闭 socket 连接。考虑到心跳包产生的网络传输开销，突发奇想，可不可以用一个全局 channel 来代替？\n于是便定义了一个全局 channel，客户端每隔一段时间就向 channel 中写入数据，服务端使用 select 来读取 channel。但是测试时却发现，客户端处的发送语句 ch \u0026lt;- struct{}{} 被阻塞了，于是又将全局 channel 改为带缓冲的，缓冲为 1，此时再次运行，客户端的发送语句没有被阻塞了，但是服务端那边却压根收不到数据。\n情景再现 创建一个 global 包，在其下新建一个 go 文件，内容如下：\npackage global var Ch = make(chan int64, 1) 创建一个 cli 包及其之下的 go 文件：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;gomodtest/sockchan/global\u0026quot; \u0026quot;time\u0026quot; ) func main() { select { case \u0026lt;-time.After(time.Second * 8) : fmt.Println(\u0026quot;timeout !\u0026quot;) case v := \u0026lt;-global.Ch: fmt.Println(v) } } 最后创建一个 sev 包：\npackage main import ( \u0026quot;gomodtest/sockchan/global\u0026quot; \u0026quot;fmt\u0026quot; ) func main() { fmt.Println(\u0026quot;send data to global chan\u0026quot;) global.Ch \u0026lt;- 1 fmt.Println(\u0026quot;send ok\u0026quot;) } 目录结构：\n├── cli │ └── cli.go ├── global │ └── var.go └── sev └── sev.go 先运行 cli.go，再开一个 termin 运行 sev.go：\nsev.go 输出如下：\n➜ sev go run sev.go send data to global chan send ok cli.go 输出如下\n➜ cli go run cli.go timeout ! 结果说明，可以向 chan 中写入数据，但是不能从 chan 中读取数据。\n原因 两个 main 代表两个进程，进程间通信当然不能使用 channel 了（如此弱智的问题）\n","date":"2021年08月08日","permalink":"/posts/liang-ge-main-goroutine-cao-zuo-yi-ge-channel-hui-zen-yang/","summary":"引子 定义一个全局变量 chan，然后创建两个 main goroutine，一个往全局 chan 里发送数据，一个从全局 chan 里读取数据，会发生什么？","title":"两个 main goroutine 操作一个 channel 会怎样"},{"contents":"引子 （以后 mysql 的输出还是截图好一些，复制粘贴格式会很难看）\n有如下表，结构如下：\n+-------+-------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+----------------+ | id | int | NO | PRI | NULL | auto_increment | | name | varchar(50) | YES | MUL | NULL | | | age | int | YES | | NULL | | +-------+-------------+------+-----+---------+----------------+ 创建一个 (name, age) 联合索引，并执行下面的 SQL 语句：\nexplain select * from stu_join_index where age = '12'; 运行后发现 type 为 index，这代表这次查询走了索引。\n（2022.3.20 更新：index 代表的是会对整个索引树进行扫描，其实也等同于全表查询，至于 index 相比 all 效率会不会更高，查阅了一些资料，都说 index 效率要更快一些）\n+----+-------------+----------------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ | 1 | SIMPLE | stu_join_index | NULL | index | name_age | name_age | 212 | NULL | 5 | 20.00 | Using where; Using index | +----+-------------+----------------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ 这就奇怪了，按照 Mysql 最左前缀原则，建立的索引是 (name, age) ，应该只有（name），（name，age）会走索引啊？再尝试查询一下（age, name），看会不会走索引：\nexplain select * from stu_join_index where age = '12' and name = 'aa'; +----+-------------+----------------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ | 1 | SIMPLE | stu_join_index | NULL | index | name_age | name_age | 212 | NULL | 5 | 20.00 | Using where; Using index | +----+-------------+----------------+------------+-------+---------------+----------+---------+------+------+----------+--------------------------+ 可以看到这里依然走了索引。\n补充：Extra 显示的是 Using where; Using index，其中的 Using index 代表：从表中仅使用索引树中的信息就能获取查询语句的列的信息, 而不必进行其他额外查找（seek）去读取实际的行记录。当查询的列是单个索引的部分的列时, 可以使用此策略。（简单的翻译就是：使用索引来直接获取列的数据，而不需回表，即覆盖索引）。\n后来在网上找到了一个同样问题的文章（https://segmentfault.com/a/1190000022690969），他的情况和我一样，创建了一张表，表中总共有四个字段。 id 为主键，还有一个由 name，age，address 组成的联合索引，当他执行下面的 SQL 时：\nEXPLAIN select * from user where address='beijing'; 同样发现 type 字段为 index。\n最终在评论区找到了答案：\n总结下，不是最左原则失效了。是因为一开始设计的字段只有四个：(id,name,age,address)，此时的联 合索引是（name, age, address），id 自带一个主键索引。相当于，四个字段都有索引。因为覆盖索引 的缘故，怎么查都会走索引。当增加一个字段（use）后，该字段没有索引，因此 select * 的时候不会 触发覆盖索引，因此不会走索引。但是如果只查询 select name,age,address,id 的话依然会走索引\n于是我进行了进一步测试，为表添加一个字段并插入一条数据：\nalter table stu_join_index add column score int; update stu_join_index set score = 90 where name = 'aa'; 此时的表信息：\n+----+------+------+-------+ | id | name | age | score | +----+------+------+-------+ | 1 | aa | 12 | 90 | | 2 | bb | 22 | NULL | | 3 | cc | 66 | NULL | | 4 | dd | 88 | NULL | | 5 | ee | 23 | NULL | +----+------+------+-------+ 此时执行 (score, age)，看看结果如何：\nexplain select * from stu_join_index where score = 90 and age = 12; +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | stu_join_index | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 20.00 | Using where | +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ 此时 type 变为了 ALL，代表要扫描全表。\n因为 (name, age) 相当于创建了 name 单列索引和 (name, age) 联合索引。对于多个字段的联合索引，也同理。如 index(a, b, c) 联合索引，则相当于创建了 a 单列索引，(a, b) 联合索引，和 (a, b, c )联合索引。 (score, age) 既没有联合索引，各自也没有单列索引，所以自然要扫描全表。\n一些疑问 1. where 条件只有 name 如果 where 条件只有 score ，或者只有 age ，结果会怎样呢？\nexplain select * from stu_join_index where age = '12'; explain select * from stu_join_index where score = 90; 运行结果：\n+----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | stu_join_index | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 20.00 | Using where | +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ 结果会扫描全表。这时因为 socre 和 age 都没有单列索引。\n但是当 where 条件只有 name 时会怎样呢？\nexplain select * from stu_join_index where name = 'aa'; 运行结果：\n+----+-------------+----------------+------------+------+---------------+----------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+------+---------------+----------+---------+-------+------+----------+-------+ | 1 | SIMPLE | stu_join_index | NULL | ref | name_age | name_age | 203 | const | 1 | 100.00 | NULL | +----+-------------+----------------+------------+------+---------------+----------+---------+-------+------+----------+-------+ 此时 type 为 ref，在上面已经说过，(name, age) 联合索引会同时创建一个 name 单列索引。但是这里为什么是 ref 而不是 index 呢？\nref 使用非唯一索引扫描或唯一索引的前缀扫描，返回匹配某个单独值的记录行。\n出现该连接类型的条件是： 查找条件列使用了索引而且不为主键和unique。其实，意思就是虽然使用了索引，但该索引列的值并不唯一，有重复。这样即使使用索引快速查找到了第一条数据，仍然不能停止，要进行目标值附近的小范围扫描。但它的好处是它并不需要扫全表，因为索引是有序的，即便有重复值，也是在一个非常小的范围内扫描。\n我的猜测：\n在没有添加 score 字段前，联合索引 (name, age) 是覆盖索引，但是添加了 score 后便不再是覆盖索引了\n覆盖索引依然是 ref\n2. 为什么 name 为非字符串时不走索引 在 1 中提到，\nexplain select * from stu_join_index where name = 'aa'; 的 type 为 ref，那么如果把查询条件改为非字符串会怎样呢？（name 为 varchar 类型）\nexplain select * from stu_join_index where name = 1; 结果：\n+----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ | 1 | SIMPLE | stu_join_index | NULL | ALL | name_age | NULL | NULL | NULL | 5 | 20.00 | Using where | +----+-------------+----------------+------------+------+---------------+------+---------+------+------+----------+-------------+ 结果变为了 ALL，为什么不是字符串就会索引失效呢？\n回答\n在博客 https://blog.csdn.net/qq_28194001/article/details/90488782 中得出结论：\n当我们使用的字段是数值类型时，加引号或者不加引号（sql中单引号和双引号实现相同效果）都不影响 索引的使用 当我们的字段是字符串类型时，不加引号的查询无法使用索引，加引号的查询才可正常使用索引 3. 再来看一种情况： 此时执行 (name, score, age)，看看结果如何：\nexplain select * from stu_join_index where name = 'aa' and score = 90 and age = 12; +----+-------------+----------------+------------+------+---------------+----------+---------+-------------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+------+---------------+----------+---------+-------------+------+----------+-------------+ | 1 | SIMPLE | stu_join_index | NULL | ref | name_age | name_age | 208 | const,const | 1 | 20.00 | Using where | +----+-------------+----------------+------------+------+---------------+----------+---------+-------------+------+----------+-------------+ 这里明明不符合最左前缀原则，为什么 type 会是 ref？\n4. 再次执行下面的语句（在文章一开始还没有添加 score 字段时，执行过该语句）：\nexplain select * from stu_join_index where age = '12' and name = 'aa'; +----+-------------+----------------+------------+------+---------------+----------+---------+-------------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------------+------------+------+---------------+----------+---------+-------------+------+----------+-------+ | 1 | SIMPLE | stu_join_index | NULL | ref | name_age | name_age | 208 | const,const | 1 | 100.00 | NULL | +----+-------------+----------------+------------+------+---------------+----------+---------+-------------+------+----------+-------+ 为什么加了 score 字段后， type 从 index 变为了 ref？\n","date":"2021年08月07日","permalink":"/posts/mysql-zui-zuo-qian-zhui-yuan-ze-shi-xiao/","summary":"引子 （以后 mysql 的输出还是截图好一些，复制粘贴格式会很难看）\n有如下表，结构如下：","title":"Mysql 最左前缀原则失效"},{"contents":"这是一道面试高频题\n无名管道 无名管道只能用于父子进程之间的通信，代码如下：\n// // Created by zz on 2021/3/5. // #include \u0026quot;../lib/apue.h\u0026quot; // 无名管道：只能在父子进程之间通信 void pipe_t() { int fp[2]; // pipe()：创建一个管道，经由参数 fd 返回两个文件描述符，fp[0] 为读打开，fp[1] 为写打开 int err = pipe(fp); // 成功返回 0，失败返回 -1 err == 0 ? printf(\u0026quot;ok\u0026quot;) : printf(\u0026quot;create pipe error\u0026quot;); } // 通过管道，从父进程传输数据到子进程 void parent_to_child_pipe() { int n; int fd[2]; pid_t pid; char line[MAXLINE]; if (pipe(fd) \u0026lt; 0) { // 由父进程创建一个管道，子进程会继承父进程所创建的管道 err_sys(\u0026quot;pipe error\u0026quot;); } if ( (pid = fork()) \u0026lt; 0) { // fork 创建一个子进程 err_sys(\u0026quot;fork error\u0026quot;); } else if (pid \u0026gt; 0) { // 父进程 close(fd[0]); // 因为是 父 -\u0026gt; 子，所以关闭父进程的读端 fd[0] printf(\u0026quot;父进程发送数据...\\n\u0026quot;); write(fd[1], \u0026quot;hello world\\n\u0026quot;, 12); } else { // 子进程 // 该子进程会继承父进程所创建的管道 close(fd[1]); // 因为是 父 -\u0026gt; 子，所以关闭子进程的写端 fd[1] printf(\u0026quot;子进程读取数据...\\n\u0026quot;); n = read(fd[0], line, sizeof(line)); write(STDOUT_FILENO, line, n); } exit(0); } int main() { parent_to_child_pipe(); } 运行结果：\n➜ ./pipe 父进程发送数据... 子进程读取数据... hello world 命名管道 命名管道（FIFO）可以用于任意两个进程之间的通信。要想使用，首先需要创建一个 FIFO 文件，之后可以通过对其的读写来完成通信操作。\n为了体现出 任意两个进程 这一特点，会创建两个 c 文件，一个是 fifo_write.c，它负责创建一个 FIFO 文件并向其中写入 “Hello, World!”，代码如下：\n#include \u0026quot;../lib/apue.h\u0026quot; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define FIFO_NAME \u0026quot;/tmp/my_fifo\u0026quot; int main() { int res = 0; // 如果文件不存在，则创建一个管道 if (access(FIFO_NAME, F_OK) == -1) { res = mkfifo(FIFO_NAME, 0777); if (res == -1) { printf(\u0026quot;mkfifo error: %s \\n\u0026quot;, strerror(errno)); return 0; } } // 以只写方式打开管道 int pipe_fd = open(FIFO_NAME, O_WRONLY); if (pipe_fd == -1) { printf(\u0026quot;open fifo error: %s \\n\u0026quot;, strerror(errno)); return 0; } char buf[1024]; memset(\u0026amp;buf, '\\0', 1024); strcpy(buf, \u0026quot;Hello, World!\u0026quot;); // 向 fifo 中写入数据 int n = write(pipe_fd, buf, sizeof(buf)); if (n \u0026lt; 0) { printf(\u0026quot;write data to fifo error: %s \\n\u0026quot;, strerror(errno)); return 0; } printf(\u0026quot;process %d write %d bytes to fifo \\n\u0026quot;, getpid(), n); } 另一个文件时 fifo_read.c，它负责从 FIFO 管道中读取数据：\n// // Created by zz on 2021/8/6. // #include \u0026quot;../lib/apue.h\u0026quot; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define FIFO_NAME \u0026quot;/tmp/my_fifo\u0026quot; int main() { // 只读方式打开管道 int pipe_fd = open(FIFO_NAME, O_RDONLY); if (pipe_fd == -1) { printf(\u0026quot;open fifo error: %s \\n\u0026quot;, strerror(errno)); return 0; } char buf[1024]; memset(\u0026amp;buf, '\\0', 1024); // 从管道中读取数据 int n = read(pipe_fd, buf, 1024); if (n \u0026lt; 0) { printf(\u0026quot;read data from fifo error: %s \\n\u0026quot;, strerror(errno)); return 0; } printf(\u0026quot;process %d read from fifo: %s \\n\u0026quot;, getpid(), buf); } 注意上面的代码创建的是 阻塞 命名管道，运行结果如下：\n先运行 fifo_write.c：\n# 编译 fifo_write.c ➜ gcc -o fw fifo_write.c ../lib/apue.c # 运行 ./fw # 运行后，发现整个程序阻塞了，没有任何反应 再创建一个 terminal，运行 fifo_read.c\n# 编译 ➜ gcc -o fr fifo_read.c ../lib/apue.c # 运行 ./fr # 输出结果 process 7601 read from fifo: Hello, World! 此时再看 fifo_write 的 terminal，发现已经停止阻塞了：\nprocess 7517 write 1024 bytes to fifo 是不是发现实名管道和 Go 中的无缓冲 channel 非常相似呢？当向 channel 写入数据时，整个 goroutine 会进入阻塞状态，直到有另外一个 goroutine 从该 channel 中取出数据，才会停止阻塞。从 channel 中读取数据也是一样的，只有 channel 中存在数据才能读取，否则就会进入阻塞。\n消息队列 （内容来自 https://blog.csdn.net/ljianhui/article/details/10287879）\n消息队列提供了一种从一个进程向另一个进程发送一个数据块的方法。 每个数据块都被认为含有一个类型，接收进程可以独立地接收含有不同类型的数据结构。我们可以通过发送消息来避免命名管道的同步和阻塞问题。但是消息队列与命名管道一样，每个数据块都有一个最大长度的限制。\nLinux 用宏 MSGMAX 和 MSGMNB 来限制一条消息的最大长度和一个队列的最大长度。\n消息队列的 API msgget 函数 该函数用来创建和访问一个消息队列。它的原型为： int msgget(key_t key, int msgflg); 与其他的 IPC 机制一样，程序必须提供一个键来命名某个特定的消息队列。msgflg 是一个权限标志，表示消息队列的访问权限，它与文件的访问权限一样。msgflg 可以与 IPC_CREAT 做或操作，表示当 key 所命名的消息队列不存在时创建一个消息队列，如果 key 所命名的消息队列存在时，IPC_CREAT 标志会被忽略，而只返回一个标识符。\n它返回一个以 key 命名的消息队列的标识符（非零整数），失败时返回-1.\nmsgsnd 函数 该函数用来把消息添加到消息队列中。它的原型为：\nint msgsend(int msgid, const void *msg_ptr, size_t msg_sz, int msgflg); msgid 是由 msgget 函数返回的消息队列标识符。\nmsg_ptr 是一个指向准备发送消息的指针，但是消息的数据结构却有一定的要求，指针 msg_ptr 所指向的消息结构一定要是以一个长整型成员变量开始的结构体，接收函数将用这个成员来确定消息的类型。所以消息结构要定义成这样：\nstruct my_message { long int message_type; /* The data you wish to transfer*/ }; msg_sz 是 msg_ptr 指向的消息的长度，注意是消息的长度，而不是整个结构体的长度，也就是说msg_sz 是不包括长整型消息类型成员变量的长度。\nmsgflg 用于控制当前消息队列满或队列消息到达系统范围的限制时将要发生的事情。\n如果调用成功，消息数据的一分副本将被放到消息队列中，并返回 0，失败时返回 -1.\nmsgrcv 函数 该函数用来从一个消息队列获取消息，它的原型为\nint msgrcv(int msgid, void *msg_ptr, size_t msg_st, long int msgtype, int msgflg); msgid, msg_ptr, msg_st 的作用也函数 msgsnd 函数的一样。\nmsgtype 可以实现一种简单的接收优先级。如果 msgtype 为 0，就获取队列中的第一个消息。如果它的值大于零，将获取具有相同消息类型的第一个信息。如果它小于零，就获取类型等于或小于 msgtype 的绝对值的第一个消息。（ps: ？？？这里没太看懂）\nmsgflg 用于控制当队列中没有相应类型的消息可以接收时将发生的事情。\n调用成功时，该函数返回放到接收缓存区中的字节数，消息被复制到由 msg_ptr 指向的用户分配的缓存区中，然后删除消息队列中的对应消息。失败时返回 -1.\nmsgctl函数 该函数用来控制消息队列，它与共享内存的 shmctl 函数相似，它的原型为：\nint msgctl(int msgid, int command, struct msgid_ds *buf); command 是将要采取的动作，它可以取3个值， IPC_STAT：把 msgid_ds 结构中的数据设置为消息队列的当前关联值，即用消息队列的当前关联值覆盖 msgid_ds 的值。 IPC_SET：如果进程有足够的权限，就把消息列队的当前关联值设置为 msgid_ds 结构中给出的值 IPC_RMID：删除消息队列\nbuf 是指向 msgid_ds 结构的指针，它指向消息队列模式和访问权限的结构。msgid_ds 结构至少包括以下成员：\nstruct msgid_ds { uid_t shm_perm.uid; uid_t shm_perm.gid; mode_t shm_perm.mode; }; 成功时返回 0，失败时返回 -1.\n示例 下面是 mq_recv 的代码，负责从消息队列中取出消息。\n// // Created by zz on 2021/8/6. // #include \u0026lt;sys/msg.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; struct msg_st { long int msg_type; char text[BUFSIZ]; }; int main() { int running = 1; int msgid = -1; struct msg_st data; long int msgtype = 0; //注意1 // 建立消息队列 msgid = msgget((key_t) 1234, 0666 | IPC_CREAT); if (msgid == -1) { fprintf(stderr, \u0026quot;msgget failed with error: %d\\n\u0026quot;, errno); exit(EXIT_FAILURE); } // 从队列中获取消息，直到遇到 end 消息为止 while (running) { if (msgrcv(msgid, (void *) \u0026amp;data, BUFSIZ, msgtype, 0) == -1) { fprintf(stderr, \u0026quot;msgrcv failed with errno: %d\\n\u0026quot;, errno); exit(EXIT_FAILURE); } printf(\u0026quot;You wrote: %s\\n\u0026quot;, data.text); // 遇到end结束 if (strncmp(data.text, \u0026quot;end\u0026quot;, 3) == 0) running = 0; } // 删除消息队列 if (msgctl(msgid, IPC_RMID, 0) == -1) { fprintf(stderr, \u0026quot;msgctl(IPC_RMID) failed\\n\u0026quot;); exit(EXIT_FAILURE); } exit(EXIT_SUCCESS); } 下面是 mq_send 的代码，负责发送消息到消息队列。\n// // Created by zz on 2021/8/6. // #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/msg.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #define MAX_TEXT 512 struct msg_st { long int msg_type; char text[MAX_TEXT]; }; int main() { int running = 1; struct msg_st data; char buffer[BUFSIZ]; int msgid = -1; // 建立消息队列 msgid = msgget((key_t) 1234, 0666 | IPC_CREAT); if (msgid == -1) { fprintf(stderr, \u0026quot;msgget failed with error: %d\\n\u0026quot;, errno); exit(EXIT_FAILURE); } // 向消息队列中写消息，直到写入 end while (running) { //输入数据 printf(\u0026quot;Enter some text: \u0026quot;); fgets(buffer, BUFSIZ, stdin); data.msg_type = 1; // 注意2 strcpy(data.text, buffer); // 向队列发送数据 if (msgsnd(msgid, (void *) \u0026amp;data, MAX_TEXT, 0) == -1) { fprintf(stderr, \u0026quot;msgsnd failed\\n\u0026quot;); exit(EXIT_FAILURE); } // 输入 end 结束输入 if (strncmp(buffer, \u0026quot;end\u0026quot;, 3) == 0) running = 0; sleep(1); } exit(EXIT_SUCCESS); } 运行结果：\n为了验证消息队列可以避免命名管道的同步和阻塞问题，这里先运行 mq_send，向消息队列中发送 3 条消息：\n➜ ./mq_send Enter some text: 123 Enter some text: 456 Enter some text: 789 # 发送了 3 条消息 再开启一个 terminal 运行 mq_recv：\n➜ ./mq_recv You wrote: 123 You wrote: 456 You wrote: 789 # 从消息队列中取出了消息 如果先运行 mq_recv，则程序会阻塞，直到消息队列中有数据。\n消息类型 （ps: 这部分先不实践了）\n这里主要说明一下消息类型是怎么一回事，注意 msgreceive.c 文件 main 函数中定义的变量 msgtype（注释为注意1），它作为 msgrcv 函数的接收信息类型参数的值，其值为 0，表示获取队列中第一个可用的消息。再来看看 msgsend.c 文件中 while 循环中的语句 data.msg_type = 1（注释为注意2），它用来设置发送的信息的信息类型，即其发送的信息的类型为 1 。所以程序 msgreceive 能够接收到程序 msgsend 发送的信息。\n如果把注意1，即 msgreceive.c 文件 main 函数中的语句由 long int msgtype = 0; 改变为 l ong int msgtype = 2; 会发生什么情况，msgreceive 将不能接收到程序 msgsend 发送的信息。因为在调用 msgrcv 函数时，如果 msgtype（第四个参数）大于零，则将只获取具有相同消息类型的第一个消息，修改后获取的消息类型为2，而 msgsend 发送的消息类型为 1 ，所以不能被 msgreceive 程序接收。重新编译 msgreceive.c 文件并再次执行，其结果如下：\n消息队列与命名管道的比较 消息队列跟命名管道有不少的相同之处，通过与命名管道一样，消息队列进行通信的进程可以是不相关的进程，同时它们都是通过发送和接收的方式来传递数据的。在命名管道中，发送数据用 write ，接收数据用read，则在消息队列中，发送数据用 msgsnd，接收数据用 msgrcv。而且它们对每个数据都有一个最大长度的限制。\n与命名管道相比，消息队列的优势在于，1、消息队列也可以独立于发送和接收进程而存在，从而消除了在同步命名管道的打开和关闭时可能产生的困难。2、同时通过发送消息还可以避免命名管道的同步和阻塞问题，不需要由进程自己来提供同步方法。3、接收程序可以通过消息类型有选择地接收数据，而不是像命名管道中那样，只能默认地接收。\n消息队列的缺点 一是通信不及时，二是附件也有大小限制，这同样也是消息队列通信不足的点。\n消息队列不适合比较大数据的传输，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 MSGMAX 和 MSGMNB，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。\n消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。\nsocket 如果 socket 类型为 TCP 和 UDP ，那么可以在不同主机的进程间进行通信。 如果 socket 类型为 UNIX，可以在同一主机下的进程间通信。 TCP 和 UDP 比较熟悉了，这里主要介绍一下 UNIX，代码示例采用 Go：\nunxi_write\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;net\u0026quot; \u0026quot;os\u0026quot; ) func main() { filename := \u0026quot;/tmp/gounix.sock\u0026quot; l, err := net.Listen(\u0026quot;unix\u0026quot;, filename) if err != nil { log.Println(\u0026quot;listen error: \u0026quot;, err) os.Remove(filename)\t// 出现错误一般是因为文件已存在，所以需要删除 return } log.Println(\u0026quot;创建 UNIX Domain Socket 成功\u0026quot;) defer l.Close() for { conn, err := l.Accept() if err != nil { log.Println(\u0026quot;accept error: \u0026quot;, err) continue } // 向 unix 文件中写入数据 conn.Write([]byte(\u0026quot;Hello, World!\u0026quot;)) } } unix_read\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net\u0026quot; ) func main() { filename := \u0026quot;/tmp/gounix.sock\u0026quot; conn, err := net.Dial(\u0026quot;unix\u0026quot;, filename) if err != nil { log.Println(\u0026quot;dial error: \u0026quot;, err) return } b := make([]byte, 1024) _, err = conn.Read(b) if err != nil { log.Println(\u0026quot;read error: \u0026quot;, err) return } fmt.Printf(\u0026quot;b: %v\\n\u0026quot;, string(b)) } 怎么感觉 unix socket 和 命名管道是一样的。。。\n信号 发送进程调用 kill 向接收进程发送一个信号，接收进程使用 signal 对该信号进行注册监听，当捕获到该信号时，可以进行相应的处理。 演示代码如下：\n#include \u0026lt;signal.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; // handler 用来处理接收到的信号 void handler() { printf(\u0026quot;[parent] receive a SIGINT signal, finish the process!\\n\u0026quot;); exit(0); } int main() { int pid; pid = fork(); if (pid \u0026lt; 0) { printf(\u0026quot;fork error!\u0026quot;); return 0; } if (pid == 0) { // child printf(\u0026quot;[child] process id: %d, parent id: %d \\n\u0026quot;, getpid(), getppid()); sleep(5); // 睡眠 5 秒 // 使用 kill 函数向指定的进程发送一个信号 // 这里向父进程发送一个 SIGINT 信号 kill(getppid(), SIGINT); printf(\u0026quot;[child] send a SIGINT signal\\n\u0026quot;); } else { // parent printf(\u0026quot;[parent] process id: %d \\n\u0026quot;, getpid()); printf(\u0026quot;[parent] wait signal... \\n\u0026quot;); // 注册一个信号处理函数，当捕获到 SIGINT 时，调用 handler 函数进行处理 (void) signal(SIGINT, handler); } sleep(10); // 等待父子进程执行完毕 } ","date":"2021年08月06日","permalink":"/posts/jin-cheng-jian-tong-xin-de-fang-shi/","summary":"这是一道面试高频题\n无名管道 无名管道只能用于父子进程之间的通信，代码如下：\n// // Created by zz on 2021/3/5.","title":"进程间通信的方式"},{"contents":" 已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,2,4,5,6,7] 在变化后可能得到： 若旋转 4 次，则可以得到 [4,5,6,7,0,1,2] 若旋转 7 次，则可以得到 [0,1,2,4,5,6,7] 注意，数组 [a[0], a[1], a[2], \u0026hellip;, a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], \u0026hellip;, a[n-2]] 。\n给你一个元素值 互不相同 的数组 nums ，它原来是一个升序排列的数组，并按上述情形进行了多次旋转。请你找出并返回数组中的 最小元素 。\n示例 1：\n输入：nums = [3,4,5,1,2] 输出：1 解释：原数组为 [1,2,3,4,5] ，旋转 3 次得到输入数组。 示例 2：\n输入：nums = [4,5,6,7,0,1,2] 输出：0 解释：原数组为 [0,1,2,4,5,6,7] ，旋转 4 次得到输入数组。 示例 3：\n输入：nums = [11,13,15,17] 输出：11 解释：原数组为 [11,13,15,17] ，旋转 4 次得到输入数组。\n提示：\nn == nums.length 1 \u0026lt;= n \u0026lt;= 5000 -5000 \u0026lt;= nums[i] \u0026lt;= 5000 nums 中的所有整数 互不相同 nums 原来是一个升序排序的数组，并进行了 1 至 n 次旋转\n方法 1 二分法 和基本二分一样，初始变量 l 置于第一个元素，r 置于最后一个元素，m 取 (l + r) \u0026raquo; 1，通过判断 nums[m] 和 nums[l]、nums[r] 的大小关系，来判断哪部分是有序的。\n比如 [3, 4, 5, 1, 2]，m = 5，l = 3，r = 2，m \u0026gt; l，所以 [l, m] 这部分有序。 又比如 [7, 0, 1, 2, 3, 4, 5]，m = 2，l = 7，r = 5，m \u0026lt; r，所以 [m, r] 这部分有序。 还有一种特殊情况：[0, 1, 2, 4, 5, 6, 7] 旋转 7 次后，依然是原数组 [0, 1, 2, 4, 5, 6, 7]，这种情况在后面会说。\n首先我们假设当前数组是一个常规的旋转数组，比如 [3, 4, 5, 1, 2]，这种旋转数组有一个特点：整个数组被划分为两个有序子数组，大的子数组在前面，小的在后面。题目要找的是最小值，该值只可能是较小子数组的第一个，所以可以推断出以下策略：\n如果 nums[m] \u0026lt; nums[r]，则代表 [m, r] 这部分有序，既然有序，那么这部分中最小的便是 m，举个例子： [7, 0, 1, 2, 3, 4, 5, 6]，l = 7，m = 2，r = 6，代表 [m, r] 这部分有序，即 [2, 3, 4, 5, 6] 这部分，其中最小的是 m = 2。 但是 m 仅仅是这部分中的最小值，未必是整个数组中的最小值，它的前面仍然可能存在更小的值。比如上面的例子中，整个数组中最小的是 0。 所以此时的下一步做法是：将 r 移动到 m，这里为什么不能移动到 m - 1 呢？因为 m 也可能是最小的数，比如 [3, 4, 0, 1, 2] 这个例子，如果 r 移动到 m - 1，移动到了 4，这样就已经错过正确答案了。\n如果 nums[m] \u0026gt; nums[l]，则代表 [l, m] 这部分有序，它们在数组的左边（常规旋转数组），这意味着它们是数组中较大的那部分，比如： [3, 4, 5, 1, 2]，l = 3，r = 2，m = 5，[l, r] 这部分有序，即 [3, 4, 5] 这部分，它们在数组中是较大的部分，由于最小值只可能是较小子数组的第一个，所以这部分必然没有最小值，将 l 移动到 m + 1 处（这里和 nums[m] \u0026lt; nums[r] 的策略不同了）。\n特殊情况：[0, 1, 2, 3, 4, 5, 6, 7]，此时处理也很简单，只要先判断 nums[m] \u0026lt; nums[r] 关系即可，这样 r 就会从 7 移动到 3，忽略了右边那些较大元素。\n解决了 l 和 r 移动的策略，剩下的就是问题就是何时返回何值，通过几个例子分析出结论： [7, 0, 1, 2, 3, 4, 5]，在 n 次循环后，l，r，m 都处于元素 0 处。 [6, 7, 0, 1, 2, 3, 4, 5]，在 n 次循环后，l，m 处于元素 0 处，r 在元素 1 处。 [5, 6, 7, 0, 1, 2, 3, 4,]，在 n 次循环后，l，r，m 都处于元素 0 处。 [0, 1, 2, 3, 4, 5, 6, 7]，在 n 次循环后，l，m 处于元素 0 处，r 此时在下标 [-1] 处。\n观察发现，最终都是 l 指向最小元素，且此时 l 和 r 的关系是 l \u0026gt;= r ，所以可以得出结论：循环推出条件为 l \u0026gt;= r，最终结果返回为 nums[l]。\n代码如下：\nfunc findMin(nums []int) int { l, r := 0, len(nums) - 1 for l \u0026lt; r { m := (l + r) \u0026gt;\u0026gt; 1 if nums[m] \u0026lt; nums[r] { r = m } else if nums[m] \u0026gt;= nums[l] { l = m + 1 } } return nums[l] } ","date":"2021年08月06日","permalink":"/posts/leetcode-153-xun-zhao-xuan-zhuan-pai-xu-shu-zu-zhong-de-zui-xiao-zhi/","summary":"已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,2,4,5,6,7] 在变化后可能得到： 若旋转 4 次，则可以得到 [4,5,6,7,0,1,2] 若旋转 7 次，则可以得到 [0,1,2,4,5,6,7] 注意，数组 [a[0], a[1], a[2], \u0026hellip;, a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], \u0026hellip;, a[n-2]] 。","title":"leetcode 153. 寻找旋转排序数组中的最小值"},{"contents":"首先明确几个概念 逻辑地址：是程序编译后，生成的目标模块进行编址时都是从0号单元开始编址，称之为目标模块的相对地址，即为逻辑地址。\n页：将虚拟内存划分的块，对应的大小就叫页面大小。\n页框：将物理内存划分的块。\n页和页框二者一一对应，一个页放入一个页框，（理论上）页的大小和页框的大小相等。\n页表：就是一个页和页框一一对应的关系表。【存放在内存中】 关系表只是起到一个索引的作用，说白了就是能根据关系表能查到某一个页面和哪一个页框所对应。\n一级页表 操作系统中，一页一般为 4 KB，所以在 4 GB （32 位系统）下一共有 4 GB / 4 KB = 4 GB * 1024 (4096 MB) * 1024(4194304KB) / 4KB = 1048576 个页，每个页需要一个 PTE 来保存映射信息，所以需要和页同等数量的 1048576 个 PTE，每个 PTE 占 4 byte，一共需要 4 MB 的内存。\n每个进程都有自己的虚拟地址空间，也就是说每个进程都有自己的页表，如果当前有 100 个进程，那么仅仅是页表就占用了 400 MB 的内存，这还是在 32 位下的情况，在 64 位系统中会占用更多。\n此外，一个进程一般而言不会占用这么多的内存（4 GB，整整占满），所以其页表中会有一些页表项并未映射物理内存，但是却依然占用了内存，这显然也是一种浪费。\n多级页表 图片来源：https://www.cnblogs.com/xiaolincoding/p/13213699.html\n在一级页表中我们知道，在 32 位和页大小 4KB 的环境下，一个进程的页表需要装下 1048576 （1024 * 1024）个页表项，我们可以把这单个页表项拆分为二级页表，第一级页表有 1024 个页表项，每个页表项指向一个二级页表，每个二级页表项包含了 1024 个页表项，算下来也是 1024 * 1024 个页表项。\n那么二级页表是如何解决一级页表存在的内存占用问题的呢？在二级页表中，第一级页表有 1024 个页表项，共占用 4 KB 内存，如果当前页表项有被映射，那么就继续创建其对应的二级页表，同样的，如果当前页表项没有被映射，就不创建二级页表，相应的内存就省下了。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= 0.804MB，这对比单级页表的 4MB 是不是一个巨大的节约？\n再来看看 深入理解计算机系统 中的 第 9.6.3 节 多级页表 中是如何描述的： 在上图中，片 2 到 7 是未被分配的。然而，如果在片 i 中至少有一个页是分配了的，那么一级 PTE i 就指向 一个二级页表的基址。例如，在上图中，片0、 1 和 8的 所有或者部分已被分配，所以它们的一级 PTE 就指向二级页表。\n这种方法从两个方面减少了内存要求。第一 ，如果一级页表中的一个 PTE 是空的，那么相应的二级页表就根本不会存在。这代表着一种巨大的潜在节约，因为对于一个典型的程序，4GB 的虚拟地址空间的大部分都会是未分配的。第二，只有一级页表才需要总是在主存中；虚拟内存系统可以在需要时创建、页面调入或调出二级页表，这就减少了主存的压力；只有最经常使用的二级页表才需要缓存在主存中 。\n","date":"2021年08月05日","permalink":"/posts/duo-ji-ye-biao/","summary":"首先明确几个概念 逻辑地址：是程序编译后，生成的目标模块进行编址时都是从0号单元开始编址，称之为目标模块的相对地址，即为逻辑地址。\n页：将虚拟内存划分的块，对应的大小就叫页面大小。\n页框：将物理内存划分的块。\n页和页框二者一一对应，一个页放入一个页框，（理论上）页的大小和页框的大小相等。","title":"多级页表为什么节省内存"},{"contents":" 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。\n实现 MyStack 类：\nvoid push(int x) 将元素 x 压入栈顶。 int pop() 移除并返回栈顶元素。 int top() 返回栈顶元素。 boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。\n注意：\n你只能使用队列的基本操作 —— 也就是 push to back、peek/pop from front、size 和 is empty 这些操作。 你所使用的语言也许不支持队列。 你可以使用 list （列表）或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。\n示例：\n输入： [\u0026ldquo;MyStack\u0026rdquo;, \u0026ldquo;push\u0026rdquo;, \u0026ldquo;push\u0026rdquo;, \u0026ldquo;top\u0026rdquo;, \u0026ldquo;pop\u0026rdquo;, \u0026ldquo;empty\u0026rdquo;] [[], [1], [2], [], [], []] 输出： [null, null, null, 2, 2, false]\n解释： MyStack myStack = new MyStack(); myStack.push(1); myStack.push(2); myStack.top(); // 返回 2 myStack.pop(); // 返回 2 myStack.empty(); // 返回 False\n提示：\n1 \u0026lt;= x \u0026lt;= 9 最多调用100 次 push、pop、top 和 empty 每次调用 pop 和 top 都保证栈不为空\n进阶：你能否实现每种操作的均摊时间复杂度为 O(1) 的栈？换句话说，执行 n 个操作的总时间复杂度 O(n) ，尽管其中某个操作可能需要比其他操作更长的时间。你可以使用两个以上的队列。\n方法 1 两个队列 代码：\ntype MyStack struct { q1 *list.List q2 *list.List } /** Initialize your data structure here. */ func Constructor() MyStack { return MyStack{ q1: list.New(), q2: list.New(), } } /** Push element x onto stack. */ func (m *MyStack) Push(x int) { m.q2.PushBack(x) for m.q1.Len() \u0026gt; 0 { pop := m.q1.Remove(m.q1.Front()).(int) m.q2.PushBack(pop) } m.q1, m.q2 = m.q2, m.q1 } /** Removes the element on top of the stack and returns that element. */ func (m *MyStack) Pop() int { return m.q1.Remove(m.q1.Front()).(int) } /** Get the top element. */ func (m *MyStack) Top() int { return m.q1.Front().Value.(int) } /** Returns whether the stack is empty. */ func (m *MyStack) Empty() bool { return m.q1.Len() == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ 方法 2 一个队列 其实用两个队列有些多余，反而使程序逻辑变复杂了，只要一个队列即可。\n代码：\ntype MyStack struct { queue *list.List } /** Initialize your data structure here. */ func Constructor() MyStack { return MyStack{ queue: list.New(), } } /** Push element x onto stack. */ func (m *MyStack) Push(x int) { queueLen := m.queue.Len() m.queue.PushBack(x) for i := 0; i \u0026lt; queueLen; i++ { pop := m.queue.Remove(m.queue.Front()).(int) m.queue.PushBack(pop) } } /** Removes the element on top of the stack and returns that element. */ func (m *MyStack) Pop() int { pop := m.queue.Remove(m.queue.Front()).(int) return pop } /** Get the top element. */ func (m *MyStack) Top() int { return m.queue.Front().Value.(int) } /** Returns whether the stack is empty. */ func (m *MyStack) Empty() bool { return m.queue.Len() == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ ","date":"2021年08月04日","permalink":"/posts/leetcode-225-yong-dui-lie-shi-xian-zhan/","summary":"请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。\n实现 MyStack 类：","title":"leetcode 225. 用队列实现栈"},{"contents":" 设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。\npush(x) —— 将元素 x 推入栈中。 pop() —— 删除栈顶的元素。 top() —— 获取栈顶元素。 getMin() —— 检索栈中的最小元素。 示例:\n输入： [\u0026ldquo;MinStack\u0026rdquo;,\u0026ldquo;push\u0026rdquo;,\u0026ldquo;push\u0026rdquo;,\u0026ldquo;push\u0026rdquo;,\u0026ldquo;getMin\u0026rdquo;,\u0026ldquo;pop\u0026rdquo;,\u0026ldquo;top\u0026rdquo;,\u0026ldquo;getMin\u0026rdquo;] [[],[-2],[0],[-3],[],[],[],[]]\n输出： [null,null,null,null,-3,null,0,-2]\n解释： MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); \u0026ndash;\u0026gt; 返回 -3. minStack.pop(); minStack.top(); \u0026ndash;\u0026gt; 返回 0. minStack.getMin(); \u0026ndash;\u0026gt; 返回 -2.\n提示：\npop、top 和 getMin 操作总是在 非空栈 上调用。\n方法 两个栈 代码如下：\ntype MinStack struct { s1, s2 *list.List } /** initialize your data structure here. */ func Constructor() MinStack { return MinStack{ s1: list.New(), s2: list.New(), } } func (m *MinStack) Push(val int) { m.s1.PushBack(val) if m.s2.Len() == 0 { m.s2.PushBack(val) } else if m.s2.Len() \u0026gt; 0 \u0026amp;\u0026amp; val \u0026lt;= m.s2.Back().Value.(int) { m.s2.PushBack(val) } } func (m *MinStack) Pop() { if m.s1.Len() \u0026gt; 0 { if m.s1.Back().Value.(int) == m.s2.Back().Value.(int) { m.s2.Remove(m.s2.Back()) } m.s1.Remove(m.s1.Back()) } } func (m *MinStack) Top() int { return m.s1.Back().Value.(int) } func (m *MinStack) GetMin() int { if m.s2.Len() \u0026gt; 0 { return m.s2.Back().Value.(int) } return -1 } /** * Your MinStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(val); * obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.GetMin(); */ ","date":"2021年08月04日","permalink":"/posts/leetcode-155-zui-xiao-zhan/","summary":"设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。","title":"leetcode 155. 最小栈"},{"contents":" 小Q在周末的时候和他的小伙伴来到大城市逛街，一条步行街上有很多高楼，共有n座高楼排成一行。 小Q从第一栋一直走到了最后一栋，小Q从来都没有见到这么多的楼，所以他想知道他在每栋楼的位置处能看到多少栋楼呢？（当前面的楼的高度大于等于后面的楼时，后面的楼将被挡住）\n输入例子1: [5,3,8,3,2,5]\n输出例子1: [3,3,5,4,4,4]\n例子说明1: 当小Q处于位置3时，他可以向前看到位置2,1处的楼，向后看到位置4,6处的楼，加上第3栋楼，共可看到5栋楼。当小Q处于位置4时，他可以向前看到位置3处的楼，向后看到位置5,6处的楼，加上第4栋楼，共可看到4栋楼。\n对例子的进一步说明：上面所说的位置序号是从 1 开始的，所以位置 3 的楼高为 8。\n从 8 的左边看，可以看到高度为 5，3 的这两栋楼（5 在左边，3 在右边，所以先看到 3 再看到 5，如果是 [3, 5, 8]，那么就只能看到 5 而看不到 3 了，因为被挡住了）。\n从 8 的右边看，可以看到高度为 3，5 的这两栋楼，注意，在 3，5 之间的 2 是看不到的，因为被 3 挡住了。\n最小栈 看评论里的，一开始没想到最小栈的方法\npackage main import ( \u0026quot;container/list\u0026quot; ) /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param heights int整型一维数组 * @return int整型一维数组 */ func findBuilding( heights []int ) []int { // write code here var ( res = make([]int, len(heights), len(heights)) stack = list.New() // min stack //rs := list.New() // stack l = len(heights) ) // 每个位置都能看见自己，所以初值都为 1 for i := 0; i \u0026lt; len(res); i++ { res[i] = 1 } // 每栋楼左边能看到的数量 for i := 0; i \u0026lt; l-1; i++ { for stack.Len() \u0026gt; 0 \u0026amp;\u0026amp; stack.Back().Value.(int) \u0026lt;= heights[i] { stack.Remove(stack.Back()) } stack.PushBack(heights[i]) res[i+1] += stack.Len() } stack.Init() // clear stack // 每栋楼右边能看到的数量 for i := l-1; i \u0026gt; 0; i-- { for stack.Len() \u0026gt; 0 \u0026amp;\u0026amp; stack.Back().Value.(int) \u0026lt;= heights[i] { stack.Remove(stack.Back()) } stack.PushBack(heights[i]) res[i-1] += stack.Len() } return res } 暴力法 自己写的，运行超时，5/10 组用例通过，仅做保留记录，无参考价值\npackage main /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param heights int整型一维数组 * @return int整型一维数组 */ func findBuilding( heights []int ) []int { // write code here var ( //isSelf bool res = make([]int, len(heights), len(heights)) ) for i := 0; i \u0026lt; len(heights); i++ { var curMaxHeight = -1 // // 当前位置一共可以看见几栋楼，因为可以看见自己所在的楼，所以初值为 1 var total = 1 // 当前位置的左边可以看见几栋楼 for j := i - 1; j \u0026gt;= 0; j-- { if heights[j] \u0026lt; curMaxHeight { continue } curMaxHeight = max(heights[j], curMaxHeight) total++ } curMaxHeight = -1 // 重置 // 当前位置的右边一共可以看见几栋楼 for j := i + 1; j \u0026lt; len(heights); j++ { if heights[j] \u0026lt; curMaxHeight { continue } curMaxHeight = max(heights[j], curMaxHeight) total++ } res[i] = total } return res } func max(x, y int) int { if x \u0026gt; y { return x } return y } ","date":"2021年08月03日","permalink":"/posts/niu-ke-teng-xun-2020-bi-shi-ti-di-er-ti-guang-jie/","summary":"小Q在周末的时候和他的小伙伴来到大城市逛街，一条步行街上有很多高楼，共有n座高楼排成一行。 小Q从第一栋一直走到了最后一栋，小Q从来都没有见到这么多的楼，所以他想知道他在每栋楼的位置处能看到多少栋楼呢？（当前面的楼的高度大于等于后面的楼时，后面的楼将被挡住）\n输入例子1: [5,3,8,3,2,5]","title":"[牛客][腾讯2020笔试题] 第二题. 逛街"},{"contents":" 说明：该题出自牛客 —— 腾讯2020校园招聘-后台\n时间限制：C/C++ 1秒，其他语言2秒\n空间限制：C/C++ 256M，其他语言512M\n小Q想要给他的朋友发送一个神秘字符串，但是他发现字符串的过于长了，于是小Q发明了一种压缩算法对字 符串中重复的部分进行了压缩，对于字符串中连续的m个相同字符串S将会压缩为[m|S](m为一个整数且 1\u0026lt;=m\u0026lt;=100)，例如字符串ABCABCABC将会被压缩为[3|ABC]，现在小Q的同学收到了小Q发送过来的字符 串，你能帮助他进行解压缩么？\n输入例子1: \u0026ldquo;HG[3|B[2|CA]]F\u0026rdquo;\n输出例子1: \u0026ldquo;HGBCACABCACABCACAF\u0026rdquo;\n例子说明1: HG[3|B[2|CA]]F −\u0026gt; HG[3|BCACA]F −\u0026gt; HGBCACABCACABCACAF\n代码 这道题和 leetcode 394. 字符串解码 基本相似，只是多了一个 “|” 符号，只要添加 “|” 对应的逻辑即可，观察发现 “|” 是在数字之后出现的，所以遍历到 “|” 时需要将当前数字变量添加到数字栈，同时将当前数字变量置 0。 同时也需要更改遇到 “[” 时的逻辑，此时只需要添加当前字符串变量到字符串栈，并清空当前字符串变量。可以参考 lc394 的思路： https://zengh1.github.io/post/leetcode-394-zi-fu-chuan-jie-ma/。\npackage main import ( \u0026quot;container/list\u0026quot; \u0026quot;unicode\u0026quot; \u0026quot;strings\u0026quot; ) /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param str string字符串 * @return string字符串 */ func compress( str string ) string { // write code here mulstack := list.New() // 保存数字的栈 letstack := list.New() // 保存字母的栈 var ( mul int res strings.Builder ) for _, c := range str { if unicode.IsLetter(c) { res.WriteRune(c) } else if unicode.IsNumber(c) { mul = mul*10 + int(c-'0') } else if c == '[' { letstack.PushBack(res.String()) res.Reset() } else if c == '|' { mulstack.PushBack(mul) mul = 0 } else if c == ']' { popmul := mulstack.Remove(mulstack.Back()).(int) var temp string for i := 0; i \u0026lt; popmul; i++ { temp += res.String() } popchar := letstack.Remove(letstack.Back()).(string) res.Reset() res.WriteString(popchar + temp) } } return res.String() } ","date":"2021年08月03日","permalink":"/posts/teng-xun-2020-bi-shi-ti-ya-suo-suan-fa/","summary":"说明：该题出自牛客 —— 腾讯2020校园招聘-后台\n时间限制：C/C++ 1秒，其他语言2秒","title":"[牛客][腾讯2020笔试题] 第一题. 压缩算法"},{"contents":"RDB RDB 可以将某个时间点上的数据库状态保存到一个 RDB 文件中（该文件的文件名可以在 redis.conf 中的 dbfilename 进行配置，默认为 dump.rdb），该文件是一个经过压缩的二进制文件，里面保存了数据库中的键值对，通过该文件可以还原生成 RDB 文件的数据库状态。\n如何使用 使用 SAVE 或者 BGSAVE 来生成 RDB 文件，二者的区别是 SAVE 会阻塞进程直到 RDB 文件创建完毕为止，阻塞期间不能处理任何请求，而 BGSAVE 会 fork 一个子进程取创建 RDB，不会阻塞主进程。\nRDB 的坑 1. 生成的 dump.rdb 所在位置 在 redis.conf 中指明：\n# The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir /usr/local/var/db/redis/ 一些博客中说的是在 redis-cli 中使用 config get dir 查看，但是\n2. 客户端执行 shutdown 或者 直接 ctrl+c 退出服务端，会自动 save 当时我想做一个测试：先 set 几条数据，然后执行 save 对这几条将数据持久化，再执行 flushdb 清空数据库，当再次启动 redis-server 时，会读取 dump.rdb ，将 set 的那几条数据恢复过来。\ntips: flushdb 清空数据库后不会执行 save，而 flushall 清空后会执行 save\n然而当我实践时却事与愿违，redis-server 的启动日志上显示了读取 rdb，但使用 keys * 却显示 empty，起初以为是自己对 rdb 工作机制不够了解，找了半天资料，最后才发现是因为退出会自动执行 save，当退出时，redis-server 输出如下：\n56163:M 02 Aug 2021 16:20:34.361 # User requested shutdown... 56163:M 02 Aug 2021 16:20:34.361 * Saving the final RDB snapshot before exiting. 56163:M 02 Aug 2021 16:20:34.362 * DB saved on disk 56163:M 02 Aug 2021 16:20:34.362 # Redis is now ready to exit, bye bye... 所以出现上面情况的原因在于，redis-server 退出时自动执行 save，将当前执行过 flushdb 命令的数据库又持久化了一份，并覆盖掉了之前的 RDB 文件。\n解决方法：让 redis-server 非正常退出即可，比如使用 kill -9 （ kill 是无效的）。\nAOF 与 RDB 持久化通过保存数据库中的键值对来记录数据库状态不同，AOF 持久化是通过保存 Redis 服务器所执行的写命令来记录数据库状态的。\nAOF 默认关闭，要想开启需要修改 redis.conf 中的 appendonly 为 yes。和 RDB 一样，当 Redis 退出时，会自动进行持久化：\n56925:M 02 Aug 2021 17:21:03.892 # User requested shutdown... 56925:M 02 Aug 2021 17:21:03.892 * Calling fsync() on the AOF file. 56925:M 02 Aug 2021 17:21:03.892 * Saving the final RDB snapshot before exiting. 56925:M 02 Aug 2021 17:21:03.892 * DB saved on disk 56925:M 02 Aug 2021 17:21:03.892 * Removing the pid file. 56925:M 02 Aug 2021 17:21:03.893 # Redis is now ready to exit, bye bye... AOF 生成的默认文件名为 appendonly.aof，其中保存的是协议化后的写命令，是文本文件，所以可以使用 cat 打印查看，该 db 存储了两个key：a:b 和 b:b，其对应的 AOF 文件如下：\n*2 $6 SELECT $1 0 *3 $3 set $1 a $1 a *3 $3 set $1 b $1 b 如何使用 通过 BGREWRITEAOF 命令进行持久化。\nAOF 持久化的实现 AOF 持久化功能的实现可以分为 命令追加（append）、文件写入，文件同步（sync） 三个步骤。\n命令追加：服务器在执行完一个写命令后，会以协议格式追加到 aof_buf 缓冲区的末尾。 文件写入：此时不会把数据直接写入到 AOF 文件，而是先将 aof_buf 中的数据保存到一个内存缓冲区。 文件同步：将内存缓冲区的数据写入到 AOF 文件中。 这里要注意写入和同步的区别\n写入和同步的区别 在现代 OS 中，为了提高文件的写入操作，当用户调用到 write 函数将数据写入文件时，os 先将数据写入到一个内存缓冲区里，正常是等到缓冲区满了或是规定时间到了，才真正地将缓冲区里的数据写入磁盘，这时才是持久化完成。\n类似你用记事本写东西一样，写完之后你会 Ctrl+v（保存），但是在没执行 Ctrl+v 的时候你也能看到自己写的，这时因为保存在内存缓冲区里了，然后你保存了，这才保存到磁盘了，所以我们可以把你写东西当做是对 aof 文件的写入，你执行 Ctrl+v 才是同步到磁盘操作。\n这样虽然大大提高了效率，但是很不安全，在你写了好多字时，卡，忽然电脑停机了，写的东西全没了，就问你气不气？\n所以 appendfsync 的选项值中的 always、everysec 都可以强制让 OS 立即将缓冲区里的数据写入硬盘。always 是写到缓冲区了立马就同步到磁盘，everysec 是写到缓冲区的数据每秒就同步一次，丢失了也就丢失了上一秒内的数据，也不是很气。\n写入同步的策略取决于配置文件中的 appendfsync 选项，在 Redis 设计与实现 中有详细说明，这里就不介绍了。\nAOF 重写 TODO\n一个问题：子进程在重写过程中，父进程依然可以写入新的 kv，此时会怎么处理？ 重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？\n为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。\n在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。\n当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。\n主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：\n将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 信号函数执行完后，主进程就可以继续像往常一样处理命令了。\n","date":"2021年08月02日","permalink":"/posts/redis-chi-jiu-hua-zhi-aof-yu-rdb/","summary":"RDB RDB 可以将某个时间点上的数据库状态保存到一个 RDB 文件中（该文件的文件名可以在 redis.","title":"Redis 持久化之 AOF 与 RDB"},{"contents":"何为停等协议 首先假设有这样一个简单的可靠传输协议，主机 A 发送数据分组给 主机 B，由于网络传输过程中可能会出现数据损坏，所以主机 B会进行校验和检测，如果数据完整则发送 ACK，否则发送 NAK，如果收到 ACK，则可以继续发送下一个数据分组（如果有），收到 NAK 则重新发送当前数据分组。\n在收到 主机 B 的回复之前，主机 A 不会进行任何操作，这便是停等协议，类似于串行操作。\n存在的问题及解决方法 1. 长时间得不到回应 解决方法： 2. ACK、NAK 丢失 既然是网络传输，那么不仅报文会丢失，ACK、NAK 同样也有丢失的可能，如果丢失会产生什么问题呢？看如下场景： 在上图中，因为 ACK 的丢失，触发超时重传，又发送了一次数据给对端，但是对端已经接收到了这个数据，属于冗余数据，无需再次接收。那么可以丢弃吗？答案是不可以，因为对端并不知道它的 ACK 是否丢失，所以也无法分辨这次接收到的是冗余重传还是新的报文，冗余重传可以丢弃，但是新报文不能。\n解决方法：给每个分组带上序号，由于停等协议每发送一个数据分组就停止等待，只要保证每发送一个新的数据分组，其发送序号与上次发送的数据分组的序号不同就可以了，所以一次用一个比特（0 或 1）来编号就够了，01不断循环，这次发送编号为 0，下次发送则编号为 1，再下次为 0，如此往复。\n如上图所示，当接收到重传分组 data0 时，便可根据编号判断出这是一个冗余数据，便可以丢弃了。这里注意数据可以丢弃，但是依然要回复一个 ACK，告诉对方我已经收到了你的数据，下次不用再发了。\n3. 另一种情况 仅仅给分组带上序号就可以了吗？看如下情况： DATA0 的 ACK 延迟了，这导致 DATA0 重传，再发起重传后不久，延迟的 ACK 到达了，于是继续发送下一个数据 DATA1，之后又收到了重传 DATA0 返回的 ACK，那么问题来了：接收端如何辨别这次的 ACK 是 DATA0 返回的还是 DATA1 返回的？\n解决方法：给 ACK 也加上序号 出处 计算机网络微课堂——湖科大教书匠，https://www.bilibili.com/video/BV1c4411d7jb?p=25，图片及内容皆来自于此\n","date":"2021年07月30日","permalink":"/posts/ke-kao-chuan-shu-de-shi-xian-ji-zhi-ting-zhi-deng-dai-xie-yi-swstop-and-wait/","summary":"何为停等协议 首先假设有这样一个简单的可靠传输协议，主机 A 发送数据分组给 主机 B，由于网络传输过程中可能会出现数据损坏，所以主机 B会进行校验和检测，如果数据完整则发送 ACK，否则发送 NAK，如果收到 ACK，则可以继续发送下一个数据分组（如果有），收到 NAK 则重新发送当前数据分组。","title":"可靠传输的实现机制 —— 停止-等待协议SW（Stop-and-Wait）"},{"contents":"看看 map 这个重要数据结构在 go 中是如何实现的\nhmap map 的结构体\n// A header for a Go map. type hmap struct { // count 代表哈希表中的元素个数，调用len(map)时，返回的就是该字段值。 // // Note: the format of the hmap is also encoded in cmd/compile/internal/reflectdata/reflect.go. // Make sure this stays in sync with the compiler's definition. count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 // B 桶的数目是 2 的 B 次幂，因为选择桶使用的是与运算的方法 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) // 使用的溢出桶的数量 noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details // hash0 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定， // 并在调用哈希函数时作为参数传入 hash0 uint32 // hash seed // buckets 桶的位置，实际类型为 []bmap buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. // oldbuckets 用于在扩容阶段保存旧桶的位置 // 如果 oldbuckets == nil，则代表以及迁移完成 // 判断函数为 growing() oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing // 记录渐进式扩容阶段下一个要迁移的旧桶编号 // 何为渐进式？ // 如果触发扩容，此时不会一次性把 kv 全部从旧桶迁移到新桶，而是当执行插入（包括修改）、删除操作时， // 对一部分 kv 进行迁移，这样就可以把迁移的时间分摊到多次 map 操作中，防止瞬间性能抖动 // ps: 每次最多只会搬迁 2 个 bucket，迁移是否完成的判断依据是 oldbuckets 字段是否为空 nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) // 指向一个 mapextra，mapextra 里面记录的是溢出桶的相关信息 extra *mapextra // optional fields } bmap bmap 是一个桶，编译期间会动态地创建一个新的结构：\n来自 Go 语言设计与实现： runtime.bmap 中的其他字段在运行时也都是通过计算内存地址的方式访问的， 所以它的定义中就不包含这些字段，不过我们能根据编译期间的 cmd/compile/internal/gc.bmap 函数重建它的结构\n但是在 go 1.16 中没有找到这个函数，所以在其他博客找了这个结构体，如下：\ntype bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } 桶中存放 kv 的方式如下：\n计算出的hash 值的高八位为 tophash，用于在一个独立的桶中区别出键，低位用于选择桶， 例如下面是一个计算出来的 hash：\n高 8 位 66883387 391851 01010 低 5 位\n当 B 等于 5 时，那么我们选择的哈希值低位也是 5 位，即 01010，它的十进制值为 10，代表 10 号桶。 再用哈希值的高 8 位，找到此 key 在桶中的位置。\n桶里面会最多装 8 个 key/value，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果的 低B位是相同的，而多个 key 装入一个桶就是 go 用来处理 hash 冲突 的方式。在桶内，会根据 key 的 hash 值的高 8 位作为 tophash，tophash 的作用是在一个独立的桶中区别出键，可以在查找 key 时进行快速判断，快速的原因在于：\n在某个 bmap 里快速判等 key 时使用，若 hash 高八位不相等就不用进一步比较 key 是否相等了，判断 key 是否相等会根据 key 的类型找到对应的equal 函数，然后调用函数拿到判等结果；而判断 tophash 就没那么复杂了。来源：https://www.bilibili.com/video/BV1Sp4y1U7dJ，【Golang】Map长啥样儿？里的评论\n计算 tophash 的函数是：func tophash(hash uintptr) uint8\nbmap 的结构图示： 在8个键值对数据后面有一个overflow指针，因为桶中最多只能装 8 个键值对，如果有多余的键值对落到了当前桶，那么就需要再构建一个桶（称为溢出桶），通过 overflow 指针链接起来。\n// A bucket for a Go map. type bmap struct { // tophash generally contains the top byte of the hash value // for each key in this bucket. If tophash[0] \u0026lt; minTopHash, // tophash[0] is a bucket evacuation state instead. tophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt elems. // NOTE: packing all the keys together and then all the elems together makes the // code a bit more complicated than alternating key/elem/key/elem/... but it allows // us to eliminate padding which would be needed for, e.g., map[int64]int8. // Followed by an overflow pointer. } 创建 map // map 初始化的流程： // 1. 入参校验，判断 key 的类型是否合法，必须为可比较类型 // 2. 底层调用 makemap 函数，计算得到合适的 B，map 容量最多可容纳 6.5 * 2^B 个元素， // 6.5 为装载因子阈值常量。 // 装载因子的计算公式是：装载因子 = 填入表中的元素个数/散列表的长度，装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降 // makemap implements Go map creation for make(map[k]v, hint). // If the compiler has determined that the map or the first bucket // can be created on the stack, h and/or bucket may be non-nil. // If h != nil, the map can be created directly in h. // If h.buckets != nil, bucket pointed to can be used as the first bucket. func makemap(t *maptype, hint int, h *hmap) *hmap { //println(\u0026quot;[make map] start...\u0026quot;) // 1. 计算哈希占用的内存是否溢出或者超出能分配的最大值 mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem \u0026gt; maxAlloc { hint = 0 } //println(\u0026quot;[make map] cap: \u0026quot;, hint, \u0026quot;, use mem: \u0026quot;, mem) //log.Printf(\u0026quot;[make map] cap: %d, use mem: %d \\n\u0026quot;, hint, mem) // initialize Hmap if h == nil { h = new(hmap) } // 2. 调用 runtime.fastrand 获取一个随机的哈希种子 h.hash0 = fastrand() // Find the size parameter B which will hold the requested # of elements. // For hint \u0026lt; 0 overLoadFactor returns false since hint \u0026lt; bucketCnt. B := uint8(0) // 3. 根据传入的 hint 计算出需要的最小需要的桶的数量 for overLoadFactor(hint, B) { B++ } //println(\u0026quot;B is \u0026quot;, B) //log.Printf(\u0026quot;B is %d, that mean the total bocket number is 2^B = %v \\n\u0026quot;, B, math2.Pow(2, float64(B))) h.B = B // 分配初始哈希表 // 如果 B 为 0，那么 buckets 字段后续会在 mapassign（map 的添加函数） 方法中 lazily 分配 // // allocate initial hash table // if B == 0, the buckets field is allocated lazily later (in mapassign) // If hint is large zeroing this memory could take a while. if h.B != 0 { var nextOverflow *bmap // 4. 使用 runtime.makeBucketArray 创建用于保存桶的数组 // runtime.makeBucketArray 会根据传入的 B 计算出的需要创建的桶数量 // 并在内存中分配一片连续的空间用于存储数据 h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h } 创建保存桶的数组 // makeBucket 为 map 创建用于保存 buckets 的数组。 // 当桶的数量小于 24 时，由于数据较少、使用溢出桶的可能性较低，会省略创建的过程以减少额外开销； // 当桶的数量多于 24 时，会额外创建 2^B − 4 个溢出桶； // // makeBucketArray initializes a backing array for map buckets. // 1\u0026lt;\u0026lt;b is the minimum number of buckets to allocate. // dirtyalloc should either be nil or a bucket array previously // allocated by makeBucketArray with the same t and b parameters. // If dirtyalloc is nil a new backing array will be alloced and // otherwise dirtyalloc will be cleared and reused as backing array. func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap) { base := bucketShift(b) nbuckets := base // 对于小的 b 值（小于4），即桶的数量小于 16 时，使用溢出桶的可能性很小。 // 对于此情况，就避免计算开销。 // For small b, overflow buckets are unlikely. // Avoid the overhead of the calculation. if b \u0026gt;= 4 { // 当桶的数量大于等于 16 个时，正常情况下就会额外创建 2^(b-4) 个溢出桶 // Add on the estimated number of overflow buckets // required to insert the median number of elements // used with this value of b. nbuckets += bucketShift(b - 4) sz := t.bucket.size * nbuckets up := roundupsize(sz) if up != sz { nbuckets = up / t.bucket.size } } // 这里，dirtyalloc 分两种情况。如果它为 nil，则会分配一个新的底层数组。 // 如果它不为 nil，则它指向的是曾经分配过的底层数组，该底层数组是由之前同 // 样的 t 和 b 参数通过 makeBucketArray 分配的，如果数组不为空，需要把 // 该数组之前的数据清空并复用。 if dirtyalloc == nil { buckets = newarray(t.bucket, int(nbuckets)) } else { // dirtyalloc was previously generated by // the above newarray(t.bucket, int(nbuckets)) // but may not be empty. buckets = dirtyalloc size := t.bucket.size * nbuckets if t.bucket.ptrdata != 0 { memclrHasPointers(buckets, size) } else { memclrNoHeapPointers(buckets, size) } } // 即 b 大于等于 4 的情况下，会预分配一些溢出桶。 // 为了把跟踪这些溢出桶的开销降至最低，使用了以下约定： // 如果预分配的溢出桶的 overflow 指针为 nil，那么可以通过指针碰撞（bumping the pointer）获得更多可用桶。 //（关于指针碰撞：假设内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为 // 分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”） // 对于最后一个溢出桶，需要一个安全的非 nil 指针指向它。 if base != nbuckets { // We preallocated some overflow buckets. // To keep the overhead of tracking these overflow buckets to a minimum, // we use the convention that if a preallocated overflow bucket's overflow // pointer is nil, then there are more available by bumping the pointer. // We need a safe non-nil pointer for the last overflow bucket; just use buckets. nextOverflow = (*bmap)(add(buckets, base*uintptr(t.bucketsize))) last := (*bmap)(add(buckets, (nbuckets-1)*uintptr(t.bucketsize))) last.setoverflow(t, (*bmap)(buckets)) } return buckets, nextOverflow } 查找 查找有两个函数，mapaccess1 对应 h[key] 这种形式，mapaccess2 对应 v, ok := h[key] ，ok 表示该 key 是否存在。\nmapaccess1 // map 查找 key // // mapaccess1 returns a pointer to h[key]. Never returns nil, instead // it will return a reference to the zero object for the elem type if // the key is not in the map. // NOTE: The returned pointer may keep the whole map live, so don't // hold onto it for very long. func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if raceenabled \u0026amp;\u0026amp; h != nil { callerpc := getcallerpc() pc := funcPC(mapaccess1) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026amp;\u0026amp; h != nil { msanread(key, t.key.size) } // 如果 map 为空或者元素个数为 0，返回零值 if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 } return unsafe.Pointer(\u0026amp;zeroVal[0]) } // 写和读冲突，即并发操作 if h.flags\u0026amp;hashWriting != 0 { throw(\u0026quot;concurrent map read and map write\u0026quot;) } // 计算哈希值，并且加入 hash0 引入随机性 // 不同类型的 key，会使用不同的 hash 算法 hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) // b 就是 bucket 的地址 b := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize))) // 如果 oldbuckets 不为空，那么证明 map 发生了扩容 // 如果有扩容发生，老的 buckets 中的数据可能还未搬迁至新的 buckets 里 // 所以需要先在老的 buckets 中找 if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { // There used to be half as many buckets; mask down one more power of two. m \u0026gt;\u0026gt;= 1 } oldb := (*bmap)(add(c, (hash\u0026amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) { b = oldb } } // 计算出当前 key 的高 8 位的 hash，也就是 tophash top := tophash(hash) bucketloop: // 双重循环遍历：外层循环是从桶到溢出桶遍历；内层是桶中的 cell 遍历 // 跳出循环的条件有三种： // 第一种是已经找到 key 值； // 第二种是当前桶再无溢出桶； // 第三种是当前桶中有 cell 位的 tophash 值是 emptyRest // 它代表此时的桶后面的 cell 还未利用，所以无需再继续遍历。 // 前面已经通过 hash 值的低位确定是几号桶了，但是桶可能还连接着溢出桶，所以使用 for 循序 // 来遍历该桶以及其连接的溢出桶 for ; b != nil; b = b.overflow(t) { // 每个桶内最多存放 8 个 key/value，每个 kv 对应一个 tophash，所以 // 遍历这 8 个 tophash 来查找 key for i := uintptr(0); i \u0026lt; bucketCnt; i++ { // tophash 不匹配 if b.tophash[i] != top { // 如果是 emptyRest，代表此时的桶后面的 cell 还未利用，所以无需再继续遍历 if b.tophash[i] == emptyRest { break bucketloop } // 继续查找 continue } // 到这里说明找到了 key，得到 key 的地址 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } // tophash 相同还需要判断 key 是否相等 // 如果 key 也相同则代表找到了，返回 // 如果 key 不相同，则进入下轮循环 if t.key.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e } } } // 所有的 bucket 都未找到，则返回零值 return unsafe.Pointer(\u0026amp;zeroVal[0]) } mapaccess2 func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) { if raceenabled \u0026amp;\u0026amp; h != nil { callerpc := getcallerpc() pc := funcPC(mapaccess2) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026amp;\u0026amp; h != nil { msanread(key, t.key.size) } if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 } return unsafe.Pointer(\u0026amp;zeroVal[0]), false } if h.flags\u0026amp;hashWriting != 0 { throw(\u0026quot;concurrent map read and map write\u0026quot;) } hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { // There used to be half as many buckets; mask down one more power of two. m \u0026gt;\u0026gt;= 1 } oldb := (*bmap)(add(c, (hash\u0026amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) { b = oldb } } top := tophash(hash) bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } if t.key.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e, true } } } return unsafe.Pointer(\u0026amp;zeroVal[0]), false } 添加 // map 的添加函数 // Like mapaccess, but allocates a slot for the key if it is not present in the map. func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { //println(\u0026quot;insert key into map...\u0026quot;) // 如果 h 是空指针，赋值会引起 panic // 例如以下语句 // var m map[string]int // m[\u0026quot;k\u0026quot;] = 1 if h == nil { panic(plainError(\u0026quot;assignment to entry in nil map\u0026quot;)) } if raceenabled { callerpc := getcallerpc() pc := funcPC(mapassign) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled { msanread(key, t.key.size) } // 有其他 goroutine 正在往 map 中写 key，会抛出以下错误 if h.flags\u0026amp;hashWriting != 0 { throw(\u0026quot;concurrent map writes\u0026quot;) } // 通过 key 和哈希种子，算出对应哈希值 hash := t.hasher(key, uintptr(h.hash0)) //println(\u0026quot;this key hash is \u0026quot;, hash) // 将 flags 的值与 hashWriting 做按位或运算 // 因为在当前 goroutine 可能还未完成 key 的写入，再次调用 t.hasher 会发生 panic。 // Set hashWriting after calling t.hasher, since t.hasher may panic, // in which case we have not actually done a write. h.flags ^= hashWriting if h.buckets == nil { h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) } again: // bucketMask 返回值是 2 的 B 次方减 1 // 因此，通过 hash 值与 bucketMask 返回值做按位与操作， // 返回的在 buckets 数组中的第几号桶 bucket := hash \u0026amp; bucketMask(h.B) //println(\u0026quot;this key will store in bucket \u0026quot;, bucket) // 如果 map 正在搬迁（即 h.oldbuckets != nil）中,则同时进行搬迁工作 // 这里涉及到了渐进式扩容这一概念，即扩容不是一次性迁移所有键，而是在其他 // 操作中进行小部分迁移 if h.growing() { //println(\u0026quot;the map is growing!\u0026quot;) growWork(t, h, bucket) } // 计算出上面求出的第几号 bucket 的内存位置 b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) // 计算出 tophash top := tophash(hash) //println(\u0026quot;this key's tophash is: \u0026quot;, top) // 是否当前 key 是否已经添加成功了，如果为 nil 则表示未成功 var inserti *uint8 var insertk unsafe.Pointer // key 的地址 var elem unsafe.Pointer // value 的地址 bucketloop: for { // 遍历桶中的 8 个 cell，找到一个空位插入 for i := uintptr(0); i \u0026lt; bucketCnt; i++ { //println(\u0026quot;cur bucket tophash is \u0026quot;, b.tophash[i]) // 如果当前 cell 的 tophash 不等于当前 key 的 tophash if b.tophash[i] != top { //println(\u0026quot;b.tophash[i] != top\u0026quot;) // 如果 cell 位为空，那么就可以在对应位置进行插入 if isEmpty(b.tophash[i]) \u0026amp;\u0026amp; inserti == nil { inserti = \u0026amp;b.tophash[i] // 更新 inserti，表示已经添加成功 insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) } // emptyRest 表示当前 cell 及之后都为空，此时已经找到空位了， // 所以没必要再遍历了 if b.tophash[i] == emptyRest { break bucketloop } continue } // 第二种情况是 cell 位的 tophash 值和当前的 tophash 值相等 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } // 即使当前 cell 位的 tophash 值相等，不一定它对应的 key 也是相等的， // 所以还要做一个 key 值判断，key 值不同则 continue 继续寻找空位 if !t.key.equal(key, k) { //println(\u0026quot;tophash equal but key not equal\u0026quot;) continue } // 如果已经有该 key 了，就更新它 // already have a mapping for key. Update it. if t.needkeyupdate() { //println(\u0026quot;tophash equal and key equal, update the value\u0026quot;) typedmemmove(t.key, k, key) } // 这里获取到了要插入 key 对应的 value 的内存地址 elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) // 如果顺利到这，就直接跳到 done 的结束逻辑中去 goto done } //println(\u0026quot;range this bucket over, not find empty cell to insert key, continue range overflow buckets\u0026quot;) // 如果桶中的 8 个 cell 遍历完，还未找到对应的空 cell 或覆盖 cell， // 那么就进入它的溢出桶中去遍历 ovf := b.overflow(t) // 如果连溢出桶中都没有找到合适的 cell，跳出循环 if ovf == nil { //println(\u0026quot;overflow buckets not find empty cell, need grow\u0026quot;) break } b = ovf } // 在已有的桶和溢出桶中都未找到合适的 cell 供 key 写入，那么有可能会触发以下两种情况 // 情况一： // 判断当前 map 的装载因子是否达到设定的 6.5 阈值，或者当前 map 的溢出桶数量是否过多。 // 如果存在这两种情况之一，则进行扩容操作。 // hashGrow() 实际并未完成扩容，对哈希表数据的搬迁（复制）操作是通过 growWork() 来完成的。 // 重新跳入 again 逻辑，在进行完 growWork() 操作后，再次遍历新的桶。 // Did not find mapping for key. Allocate new cell \u0026amp; add entry. // If we hit the max load factor or we have too many overflow buckets, // and we're not already in the middle of growing, start growing. if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } // 情况二： // 在不满足情况一的条件下，会为当前桶再新建溢出桶，并将 tophash， // key 插入到新建溢出桶的对应内存的 0 号位置 if inserti == nil { // The current bucket and all the overflow buckets connected to it are full, allocate a new one. newb := h.newoverflow(t, b) inserti = \u0026amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) } // 在插入位置存入新的 key 和 value // store new key/elem at insert position if t.indirectkey() { kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem } if t.indirectelem() { vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem } typedmemmove(t.key, insertk, key) *inserti = top // map 中的 key 数量 + 1 h.count++ done: if h.flags\u0026amp;hashWriting == 0 { throw(\u0026quot;concurrent map writes\u0026quot;) } h.flags \u0026amp;^= hashWriting if t.indirectelem() { elem = *((*unsafe.Pointer)(elem)) } return elem } 删除 func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) { if raceenabled \u0026amp;\u0026amp; h != nil { callerpc := getcallerpc() pc := funcPC(mapdelete) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026amp;\u0026amp; h != nil { msanread(key, t.key.size) } if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 } return } if h.flags\u0026amp;hashWriting != 0 { throw(\u0026quot;concurrent map writes\u0026quot;) } hash := t.hasher(key, uintptr(h.hash0)) // Set hashWriting after calling t.hasher, since t.hasher may panic, // in which case we have not actually done a write (delete). h.flags ^= hashWriting bucket := hash \u0026amp; bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) bOrig := b top := tophash(hash) search: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break search } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) k2 := k if t.indirectkey() { k2 = *((*unsafe.Pointer)(k2)) } if !t.key.equal(key, k2) { continue } // Only clear key if there are pointers in it. if t.indirectkey() { *(*unsafe.Pointer)(k) = nil } else if t.key.ptrdata != 0 { memclrHasPointers(k, t.key.size) } e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { *(*unsafe.Pointer)(e) = nil } else if t.elem.ptrdata != 0 { memclrHasPointers(e, t.elem.size) } else { memclrNoHeapPointers(e, t.elem.size) } b.tophash[i] = emptyOne // If the bucket now ends in a bunch of emptyOne states, // change those to emptyRest states. // It would be nice to make this a separate function, but // for loops are not currently inlineable. if i == bucketCnt-1 { if b.overflow(t) != nil \u0026amp;\u0026amp; b.overflow(t).tophash[0] != emptyRest { goto notLast } } else { if b.tophash[i+1] != emptyRest { goto notLast } } for { b.tophash[i] = emptyRest if i == 0 { if b == bOrig { break // beginning of initial bucket, we're done. } // Find previous bucket, continue at its last entry. c := b for b = bOrig; b.overflow(t) != c; b = b.overflow(t) { } i = bucketCnt - 1 } else { i-- } if b.tophash[i] != emptyOne { break } } notLast: h.count-- // Reset the hash seed to make it more difficult for attackers to // repeatedly trigger hash collisions. See issue 25237. if h.count == 0 { h.hash0 = fastrand() } break search } } if h.flags\u0026amp;hashWriting == 0 { throw(\u0026quot;concurrent map writes\u0026quot;) } h.flags \u0026amp;^= hashWriting } 扩容 扩容策略分以下两种情况：\n判断已经达到装载因子的临界点，即元素个数 \u0026gt;= 桶（bucket）总数 * 6.5，这时候说明大部分的桶可能都快满了（即平均每个桶存储的键值对达到6.5个），如果插入新元素，有大概率需要挂在溢出桶（overflow bucket）上，判断 函数为 overLoadFactor\n判断溢出桶是否太多，当桶总数 \u0026lt; 2 ^ 15 时，如果溢出桶总数 \u0026gt;= 桶总数， 则认为溢出桶过多。当桶总数 \u0026gt;= 2 ^ 15 时，直接与 2 ^ 15 比较，当溢出 桶总数 \u0026gt;= 2 ^ 15 时，即认为溢出桶太多了，判断函数：tooManyOverflowBuckets\n在某些场景下，比如不断的增删，这样会造成 overflow 的 bucket 数量增多，但负载因子 又不高，未达不到第 1 点的临界值，就不能触发扩容来缓解这种情况。这样会造成桶的使用率不高， 值存储得比较稀疏，查找插入效率会变得非常低，因此有了第 2 点判断指标。这就像是一座空城，房 子很多，但是住户很少，都分散了，找起人来很困难\n两种情况官方采用了不同的解决方案\n针对 1，将 B + 1（桶数量为 2 ^ B，B+1 则代表翻倍），新建一个 buckets 数组， 新的 buckets 大小是原来的 2 倍，然后旧 buckets 数据搬迁到新的 buckets。该方法我们称之为增量扩容。\n针对 2，并不扩大容量，buckets 数量维持不变，重新做一遍类似增量扩容的搬迁动作，把松散的键值对 重新排列一次，把在 overflow bucket 中的 key 移动到 bucket 中来以使 bucket 的使用率 更高，进而保证更快的存取。该方法我们称之为等量扩容。\n判断扩容条件函数 tooManyOverflowBuckets 判断溢出桶是否太多，当桶总数 \u0026lt; 2 ^ 15 时，如果溢出桶总数 \u0026gt;= 桶总数，则认为溢出桶过多。当桶总数 \u0026gt;= 2 ^ 15 时，直接与 2 ^ 15 比较，当溢出桶总数 \u0026gt;= 2 ^ 15 时，即认为溢出桶太多了。\n// tooManyOverflowBuckets reports whether noverflow buckets is too many for a map with 1\u0026lt;\u0026lt;B buckets. // Note that most of these overflow buckets must be in sparse use; // if use was dense, then we'd have already triggered regular map growth. func tooManyOverflowBuckets(noverflow uint16, B uint8) bool { // If the threshold is too low, we do extraneous work. // If the threshold is too high, maps that grow and shrink can hold on to lots of unused memory. // \u0026quot;too many\u0026quot; means (approximately) as many overflow buckets as regular buckets. // See incrnoverflow for more details. if B \u0026gt; 15 { B = 15 } // The compiler doesn't see here that B \u0026lt; 16; mask B to generate shorter shift code. return noverflow \u0026gt;= uint16(1)\u0026lt;\u0026lt;(B\u0026amp;15) } overLoadFactor 判断已经达到装载因子的临界点，即元素个数 \u0026gt;= 桶（bucket）总数 * 6.5，这时候说明大部分的桶可能都快满了 （即平均每个桶存储的键值对达到 6.5 个），如果插入新元素，有大概率需要挂在溢出桶（overflow bucket）上。\n// overLoadFactor reports whether count items placed in 1\u0026lt;\u0026lt;B buckets is over loadFactor. func overLoadFactor(count int, B uint8) bool { return count \u0026gt; bucketCnt \u0026amp;\u0026amp; uintptr(count) \u0026gt; loadFactorNum*(bucketShift(B)/loadFactorDen) } 扩容函数 hashGrow // hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets // 挂到了 hmap.oldbuckets 字段上。 func hashGrow(t *maptype, h *hmap) { // If we've hit the load factor, get bigger. // Otherwise, there are too many overflow buckets, // so keep the same number of buckets and \u0026quot;grow\u0026quot; laterally. bigger := uint8(1) // 如果没有达到负载因子临界点，则设置为等量扩容 if !overLoadFactor(h.count+1, h.B) { bigger = 0 // bigger 设置为 0 h.flags |= sameSizeGrow } // 记录老 buckets 的位置 oldbuckets := h.buckets // 分配一个新的数组用来存放新桶 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } // commit the grow (atomic wrt gc) h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 if h.extra != nil \u0026amp;\u0026amp; h.extra.overflow != nil { // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil { throw(\u0026quot;oldoverflow is not nil\u0026quot;) } h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil } if nextOverflow != nil { if h.extra == nil { h.extra = new(mapextra) } h.extra.nextOverflow = nextOverflow } // the actual copying of the hash table data is done incrementally // by growWork() and evacuate(). } growWork // 真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 // mapassign() 和 mapdelete() 函数中。也就是插入（包括修改）、删除 key 的时候，都 // 会尝试进行搬迁 buckets 的工作。它们会先检查 oldbuckets 是否搬迁完毕（检查 oldbuckets // 是否为 nil），再决定是否进行搬迁工作。 // growWork 会搬迁 0~2 个桶 func growWork(t *maptype, h *hmap, bucket uintptr) { // make sure we evacuate the oldbucket corresponding // to the bucket we're about to use evacuate(t, h, bucket\u0026amp;h.oldbucketmask()) // evacuate one more oldbucket to make progress on growing if h.growing() { // 再多搬迁一个桶 evacuate(t, h, h.nevacuate) } } 疑问 如果两个 key 落入同一个桶，且恰巧它们的 tophash 也相同，此时的添加和查找流程会怎样？ 对于 查找 而言，如果 tophash 相同还会进一步判断 key 是否相同，如果 key 相同则返回，不同则继续查找。\n添加 也是类似的，如果 tophash 和 key 都相同，说明该 key 已经存在，会执行更新操作，否则继续寻找空位，也就是说，在一个桶中可能出现相同相同的 tophash。\ngo map 的负载因子是多少？ 负载因子是 6.5，在源码中表现为：\n// Maximum average load of a bucket that triggers growth is 6.5. // Represent as loadFactorNum/loadFactorDen, to allow integer math. loadFactorNum = 13 loadFactorDen = 2 参考 Go是如何设计Map的 — 机器铃砍菜刀，https://mp.weixin.qq.com/s/q3qyc5uf3IMVt4KQD12IKQ\n【Golang】Map长啥样儿？— 幼麟实验室，https://www.bilibili.com/video/BV1Sp4y1U7dJ\n","date":"2021年07月29日","permalink":"/posts/go-map-yuan-ma-yue-du/","summary":"看看 map 这个重要数据结构在 go 中是如何实现的","title":"go map源码阅读"},{"contents":" 给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。\n如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n进阶：\n你可以设计一个只使用常数额外空间的算法来解决此问题吗？ 你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。\n示例 1：\n输入：head = [1,2,3,4,5], k = 2 输出：[2,1,4,3,5]\n示例 2：\n输入：head = [1,2,3,4,5], k = 3 输出：[3,2,1,4,5]\n示例 3：\n输入：head = [1,2,3,4,5], k = 1 输出：[1,2,3,4,5]\n示例 4：\n输入：head = [1], k = 1 输出：[1]\n提示：\n列表中节点的数量在范围 sz 内 1 \u0026lt;= sz \u0026lt;= 5000 0 \u0026lt;= Node.val \u0026lt;= 1000 1 \u0026lt;= k \u0026lt;= sz\n方法 这里借用 leetcode 题解的图片，比较直观，只要照着图上的流程，基本就可以编写出代码了。 图片来源：https://leetcode-cn.com/problems/reverse-nodes-in-k-group/solution/tu-jie-kge-yi-zu-fan-zhuan-lian-biao-by-user7208t/\n代码 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func reverseKGroup(head *ListNode, k int) *ListNode { s := \u0026amp;ListNode{Next: head} var ( pre = s // start 的前继节点 next = pre // end 的后继节点 start *ListNode // 要翻转的第一个节点 end = pre // 要翻转的最后一个节点 ) for pre != nil { start = pre.Next // end 移动到要翻转的最后一个节点 for i := 0; i \u0026lt; k; i++ { end = end.Next if end == nil { return s.Next } } next = end.Next end.Next = nil // 断链 rh, rt := reverse(start) // 反转 start - end // 重新连接链表 pre.Next = rh rt.Next = next pre = rt end = rt } return s.Next } func reverse(head *ListNode) (rhead, rtail *ListNode) { var ( cur = head prev, next *ListNode ) for cur != nil { next = cur.Next cur.Next = prev prev = cur cur = next } return prev, head } ","date":"2021年07月29日","permalink":"/posts/leetcode-25-k-ge-yi-zu-fan-zhuan-lian-biao/","summary":"给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。","title":"leetcode 25. K 个一组翻转链表"},{"contents":" 给定二叉树其中的一个结点，请找出其中序遍历顺序的下一个结点并且返回。\n注意，树中的结点不仅包含左右子结点，而且包含指向父结点的指针。\n示例: 输入:{8,6,10,5,7,9,11},8 返回:9 解析:这个组装传入的子树根节点，其实就是整颗树，中序遍历{5,6,7,8,9,10,11}，根节点8的下一个节点就是9，应该返回{9,10,11}，后台只打印子树的下一个节点，所以只会打印9，如下图，其实都有指向左右孩子的指针，还有指向父节点的指针，下图没有画出来 特别说明：leetcode 上没有这道题，测评请前往牛客，评测地址：https://www.nowcoder.com/questionTerminal/9023a0c988684a53960365b889ceaf5e\n分析 有如下情况：\n该节点有右子树，那么下一个节点是右子树的最左节点，比如上图中的 8，其右子树为 10，10 的最左节点为 9，所以 8 的下一个节点是 9。\n该节点无右子树，那么又有以下两种情况：\n该节点为其父节点的左子节点，比如 9 就是其父节点 10 的左子节点，此时下一个节点就是父节点。\n该节点为其父节点的右子节点，比如 7 就是其父节点 6 的右子节点，此时下一个节点是：左子节点是其父节点 的节点，说起来比较绕，就是左子节点为 6 的节点，在图中是 8。\n再举一个例子：如果给定的节点为 11 ，其父节点为 10，但是没有任何一个节点的左子节点为 10，此时只能继续向上找，看 10 的父节点 8 的情况，同样的，没有任何一个节点的左子节点为 8，继续向上，但此时 8 已经没有父节点了，所以返回 nil\n代码 package main /* type TreeLinkNode struct { Val int Left *TreeLinkNode Right *TreeLinkNode Next *TreeLinkNode } */ func GetNext(pNode *TreeLinkNode) *TreeLinkNode { // 右子树不为空 if pNode.Right != nil { p := pNode.Right // 右子树中最左边的节点 for p.Left != nil { p = p.Left } return p } else {\t// 右子树为空 // 如果 pNode 是其父节点的左子节点（注意父节点的非空判断） if pNode.Next != nil \u0026amp;\u0026amp; pNode.Next.Left == pNode { // 则下一个节点是父节点 return pNode.Next } else { // pNode 是父节点的右子节点 p := pNode.Next\t// pNode 的父节点 // 不断向上查找，直到找到一个节点的左子节点为 p 或者 p == nil for p != nil \u0026amp;\u0026amp; p.Next != nil { // 如果 p（父节点）的父节点的左子节点不是 p // 则继续向上查找 if p.Next.Left != p { p = p.Next } else { return p.Next } } } } return nil } ","date":"2021年07月28日","permalink":"/posts/bian-cheng-ti-er-cha-shu-de-xia-yi-ge-jie-dian/","summary":"给定二叉树其中的一个结点，请找出其中序遍历顺序的下一个结点并且返回。\n注意，树中的结点不仅包含左右子结点，而且包含指向父结点的指针。\n示例: 输入:{8,6,10,5,7,9,11},8 返回:9 解析:这个组装传入的子树根节点，其实就是整颗树，中序遍历{5,6,7,8,9,10,11}，根节点8的下一个节点就是9，应该返回{9,10,11}，后台只打印子树的下一个节点，所以只会打印9，如下图，其实都有指向左右孩子的指针，还有指向父节点的指针，下图没有画出来 特别说明：leetcode 上没有这道题，测评请前往牛客，评测地址：https://www.","title":"[编程题]二叉树的下一个结点"},{"contents":"MySQL 面经汇总，收集于牛客，不断更新中\n1. char 和 varchar 的区别 char 是固定长度，varchar 长度可变，存储时，前者不管实际存储数据的长度，直接规定 char 规定的长度分配存储空间；而后者会根据实际存储的数据分配最终的存储空间。\n以char(32)和varchar(32)举例：（这里的 32 表示字符数）\nchar(32) varchar(32) 占用空间 固定32字符（如果数据长度不够32将用空格补齐） 跟随实际存储内容长度，但不超过32 空格处理 检索时会去掉尾部空格（数据本身有空白符也会被去掉） 不会对空格处理 是否记录字段长度 否 是。额外拿出空间记录字段数据长度（字符数） 适用场景 存储的数据长度基本一致，不需要空格，eg 手机号、UUID、密码加密后的密文 数据长度不一定，长度范围变化较大的场景 小问题：char(1) 和 varchar(1) 的区别？两个都只能保存单个字符，但是 varchar 要多占一个或两个存储位置用来记录存储长度信息\n2. 建立一个联合索引，问 select a，b，c 和 select a，c，b 有什么区别 MySQL 的 最左前缀匹配原则\n3. varchar 类型，插入的数据超过设置的长度会怎样 来源： https://www.hegongshan.com/2020/04/22/mysql-varchar/\n网上基本都说字符串会被自动截断，并报出一个 warning，但是，我在本机上测试时（MySQL 8.0.16），却报了如下错误：\nERROR 1406 (22001): Data too long for column 'name' at row 1 这是怎么回事呢？后来，我发现，这两种情况是由于SQL模式不同造成的。\n在MySQL中，有如下三种最重要的SQL模式（官网称之为The Most Important SQL Modes）\nANSI\n宽松模式\n此模式更改语法和行为，使其更接近标准SQL\nSTRICT_TRANS_TABLES\n严格模式\n在该模式下,如果一个值不能插入到一个事务表中,则中断当前的操作,对非事务表不做限制\nTRADITIONAL\n在向列中插入错误值时，此模式“给出错误而不是警告”。\nTRADITIONAL 是一个组合模式，它包含了 STRICT_TRANS_TABLES\n使用 select @@sql_mode 查看当前的 sql_mode。\nset @@sql_mode = 'traditional' 更改当前 sql_mode。\n总结\n当 varchar 超过限制长度时，\n1.如果当前的 SQL 模式为宽松模式，那么将会按照从前往后的顺序，对字符串进行截断，并提示一个警告；\n2.如果当前的 SQL 模式为严格模式，那么将会报出一个错误。\nMYSQL 事务的 ACID A：原子性，保证事务要不全部执行成功，要不全部失败 C：一致性：保证事务从一个正确的状态转换到另一个正确的状态 I：隔离性：事务和事务之间是隔离的，并且有不同的隔离级别 D：持久性，事务最终会保存到硬盘上，对数据库的更改是永久性的\nMySQL 原子性怎么保证 通过 undo log（回滚日志），\nredolog 工作原理 宕机恢复 undolog 在宕机时怎么保证原子性 幻读和不可重复读的区别 不可重复读与幻读的区别可以通俗的理解为：前者是数据变了，后者是数据的行数变了\n乐观锁悲观锁 索引设计原则 为频繁查询的字段建立索引 ","date":"2021年07月25日","permalink":"/posts/mysql-mian-jing/","summary":"MySQL 面经汇总，收集于牛客，不断更新中\n1. char 和 varchar 的区别 char 是固定长度，varchar 长度可变，存储时，前者不管实际存储数据的长度，直接规定 char 规定的长度分配存储空间；而后者会根据实际存储的数据分配最终的存储空间。","title":"MySQL 面经汇总"},{"contents":"Linux epoll 的条件触发( Level Trigger )和边缘触发( Edge Trigger )\nLT 效果演示 一个简单的小 demo，从标准输入 stdin 读入数据，并输出到标准输出 stdout，这里设置了每次读取的 BUF_SIZE 为 2。\n#include \u0026lt;sys/epoll.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;string.h\u0026gt; #define BUF_SIZE 2 int main() { int epfd, nfds; char buf[BUF_SIZE]; struct epoll_event event, events[5]; epfd = epoll_create(1); event.data.fd = STDIN_FILENO; event.events = EPOLLIN /*| EPOLLET*/; epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, \u0026amp;event); while (1) { nfds = epoll_wait(epfd, events, 5, -1); //printf(\u0026quot;%d events already\\n\u0026quot;, nfds); for (int i = 0; i \u0026lt; nfds; i++) { if (events[i].data.fd == STDIN_FILENO) { printf(\u0026quot;trigger once!\\n\u0026quot;); memset(buf, '\\0', BUF_SIZE); read(events[i].data.fd, buf, BUF_SIZE); printf(\u0026quot;read content: %s\\n\u0026quot;, buf); } } } } 运行程序，输入一个长度为 6 的字符串，因为设置了每次读 2 字节，所以一次并不能读完： 运行结果如下： 可以看到，在 LT 模式下，只要缓冲区中还有数据，该事件就依然会触发（这里最后一次触发不是很明白，输出的内容也是空，可能读取的是换行符？）\nET 效果演示 ET 的代码和 LT 基本一样，只需要在 events 处小小改动 event.events = EPOLLIN | EPOLLET\n#include \u0026lt;sys/epoll.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;string.h\u0026gt; #define BUF_SIZE 2 int main() { int epfd, nfds; char buf[BUF_SIZE]; struct epoll_event event, events[5]; epfd = epoll_create(1); event.data.fd = STDIN_FILENO; event.events = EPOLLIN | EPOLLET; // set et epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, \u0026amp;event); while (1) { nfds = epoll_wait(epfd, events, 5, -1); //printf(\u0026quot;%d events already\\n\u0026quot;, nfds); for (int i = 0; i \u0026lt; nfds; i++) { if (events[i].data.fd == STDIN_FILENO) { printf(\u0026quot;trigger once!\\n\u0026quot;); memset(buf, '\\0', BUF_SIZE); read(events[i].data.fd, buf, BUF_SIZE); printf(\u0026quot;read content: %s\\n\u0026quot;, buf); } } } } 同样的输入，运行结果如下： 在 ET 模式下，整个事件只会触发一次，即便缓冲区中还有残余数据未被处理，要想再次触发，只有在终端中再次输入： 这里输入了 adaasdad ，使得事件得以再次触发。\n这个 demo 说明了，如果使用 ET 模式，就一定要在一次事件中，一次性把 socket 上的数据收取干净才行，否则可能造成数据的遗漏。\nET 的问题 为了解决上面的问题，需要在一个死循环中不断调用 read，直到读完为止。修改后的代码如下：\n#include \u0026lt;sys/epoll.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;string.h\u0026gt; #define BUF_SIZE 2 int main() { int epfd, nfds; char buf[BUF_SIZE]; struct epoll_event event, events[5]; epfd = epoll_create(1); event.data.fd = STDIN_FILENO; event.events = EPOLLIN | EPOLLET; // set et epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, \u0026amp;event); while (1) { nfds = epoll_wait(epfd, events, 5, -1); //printf(\u0026quot;%d events already\\n\u0026quot;, nfds); for (int i = 0; i \u0026lt; nfds; i++) { if (events[i].data.fd == STDIN_FILENO) { printf(\u0026quot;trigger once!\\n\u0026quot;); memset(buf, '\\0', BUF_SIZE); // 不断读取直到全部读完 while (1) { int n = read(events[i].data.fd, buf, BUF_SIZE); if (n \u0026lt;= 0) { printf(\u0026quot;read EOF\\n\u0026quot;); break; } printf(\u0026quot;read content: %s\\n\u0026quot;, buf); } } } } } ET 需要和 NONBLOCK 搭配使用 上面的 demo 只是一个简单的演示，实际中 epoll 更多的应用在网络编程中，这将涉及到多个连接的并发问题，比如有一下一个使用 epoll ET 实现的 echo 服务：\n#include \u0026quot;../pkg/net/net.h\u0026quot; #include \u0026lt;iostream\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt; #define BUF_SIZE 4 // 将调用 read 函数时使用的缓冲大小缩减为 4 个字节 #define EPOLL_SIZE 50 // g++ -o echo_et echo_et.cc ../pkg/net/net.cpp int main() { Server s(\u0026quot;tcp\u0026quot;, \u0026quot;0.0.0.0\u0026quot;, \u0026quot;8080\u0026quot;); s.Listen(100); char buf[BUF_SIZE]; int epfd = epoll_create(EPOLL_SIZE); struct epoll_event *events; struct epoll_event event; events = (epoll_event*) malloc(sizeof(struct epoll_event) * EPOLL_SIZE); event.data.fd = s.Sockfd(); event.events = EPOLLIN | EPOLLET; // ET epoll_ctl(epfd, EPOLL_CTL_ADD, s.Sockfd(), \u0026amp;event); for (; ;) { int okcnt = epoll_wait(epfd, events, EPOLL_SIZE, -1); if (okcnt == -1) { cout \u0026lt;\u0026lt; \u0026quot;epoll wait error\u0026quot; \u0026lt;\u0026lt; endl; break; } // 插入验证 epoll_wait 函数调用次数的语句 cout \u0026lt;\u0026lt; \u0026quot;trigger once!\u0026quot; \u0026lt;\u0026lt; endl; for (int i = 0; i \u0026lt; okcnt; i++) { if (events[i].data.fd == s.Sockfd()) { auto conn = s.Accept(); event.events = EPOLLIN | EPOLLET; event.data.fd = conn-\u0026gt;Connfd(); epoll_ctl(epfd, EPOLL_CTL_ADD, conn-\u0026gt;Connfd(), \u0026amp;event); cout \u0026lt;\u0026lt; \u0026quot;connected client: \u0026quot; \u0026lt;\u0026lt; conn-\u0026gt;Connfd() \u0026lt;\u0026lt; endl; } else { while (true) { int n = read(events[i].data.fd, buf, BUF_SIZE); if (n == 0) { epoll_ctl(epfd, EPOLL_CTL_DEL, events[i].data.fd, nullptr); close(events[i].data.fd); printf(\u0026quot;closed client: %d\\n\u0026quot;, events[i].data.fd); break; } else { write(events[i].data.fd, buf, n); } } } } } } 上面的程序是不具备并发能力的，原因在于这段代码：\nwhile (true) { int n = read(events[i].data.fd, buf, BUF_SIZE); if (n == 0) { epoll_ctl(epfd, EPOLL_CTL_DEL, events[i].data.fd, nullptr); close(events[i].data.fd); printf(\u0026quot;closed client: %d\\n\u0026quot;, events[i].data.fd); break; } else { write(events[i].data.fd, buf, n); } } 因为 ET 的特性，需要一次性将数据全部读完，所以需要在一个死循环中不断调用 read 来读取数据。\n但是 socket 建立的连接默认是阻塞模式的，这将导致 read 函数在最后一次被阻塞，比如要读取数据 1234，每次读 2 个，第一次循环读取 12，第二次循环读取 34，读取完后进入第三次循环，但是因为此时已经无数据可读了，所以会阻塞在 read 处，除非对端主动关闭返回 0，否则将永远阻塞。而整个程序又是单线程的，这将导致其他连接无法被处理。\n解决方法是将 socket 设置为非阻塞（NONBLOCK），当无数据可读时，会立马返回 EAGAIN 错误（ps：EAGAIN 和 EWOULDBLOCK 是一样的）而不是阻塞。只需要将 accpet 返回的 conn 设置为非阻塞即可：\nif (events[i].data.fd == s.Sockfd()) { auto conn = s.Accept(); // set conn nonblock util::SetNonBlock(conn-\u0026gt;Connfd()); event.events = EPOLLIN | EPOLLET; event.data.fd = conn-\u0026gt;Connfd(); epoll_ctl(epfd, EPOLL_CTL_ADD, conn-\u0026gt;Connfd(), \u0026amp;event); cout \u0026lt;\u0026lt; \u0026quot;connected client: \u0026quot; \u0026lt;\u0026lt; conn-\u0026gt;Connfd() \u0026lt;\u0026lt; endl; } int util::SetNonBlock(int fd) { int old_opt = fcntl(fd, F_GETFL); int new_opt = old_opt | O_NONBLOCK; fcntl(fd, F_SETFL, new_opt); return old_opt; } 为什么 LT 不需要设置非阻塞 LT 下的 echo 服务：\n// // Created by root on 7/18/21. // #include \u0026quot;../pkg/net/net.h\u0026quot; #include \u0026lt;iostream\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt; #define BUF_SIZE 4 // 将调用 read 函数时使用的缓冲大小缩减为 4 个字节 #define EPOLL_SIZE 50 int main() { Server s(\u0026quot;tcp\u0026quot;, \u0026quot;0.0.0.0\u0026quot;, \u0026quot;8080\u0026quot;); s.Listen(100); char buf[BUF_SIZE]; int epfd = epoll_create(EPOLL_SIZE); struct epoll_event *events; struct epoll_event event; events = (epoll_event*) malloc(sizeof(struct epoll_event) * EPOLL_SIZE); event.data.fd = s.Sockfd(); event.events = EPOLLIN; epoll_ctl(epfd, EPOLL_CTL_ADD, s.Sockfd(), \u0026amp;event); for (; ;) { int okcnt = epoll_wait(epfd, events, EPOLL_SIZE, -1); if (okcnt == -1) { cout \u0026lt;\u0026lt; \u0026quot;epoll wait error\u0026quot; \u0026lt;\u0026lt; endl; break; } // 插入验证 epoll_wait 函数调用次数的语句 cout \u0026lt;\u0026lt; \u0026quot;trigger once!\u0026quot; \u0026lt;\u0026lt; endl; for (int i = 0; i \u0026lt; okcnt; i++) { if (events[i].data.fd == s.Sockfd()) { auto conn = s.Accept(); event.events = EPOLLIN; event.data.fd = conn-\u0026gt;Connfd(); epoll_ctl(epfd, EPOLL_CTL_ADD, conn-\u0026gt;Connfd(), \u0026amp;event); cout \u0026lt;\u0026lt; \u0026quot;connected client: \u0026quot; \u0026lt;\u0026lt; conn-\u0026gt;Connfd() \u0026lt;\u0026lt; endl; } else { int n = read(events[i].data.fd, buf, BUF_SIZE); if (n == 0) { epoll_ctl(epfd, EPOLL_CTL_DEL, events[i].data.fd, nullptr); close(events[i].data.fd); printf(\u0026quot;closed client: %d\\n\u0026quot;, events[i].data.fd); } else { write(events[i].data.fd, buf, n); } } } } } LT 模式下，只要缓冲区还有数据就会触发事件，所以不需要一次全部读取，每次触发事件只会 read 一次（触发事件即表示 read 可读），而不是像 ET 一样在死循环里一直 read，自然也就不会发生阻塞问题了。比如读取 1234，第一次读取 12 退出循环，因为还有数据，继续触发事件，读取剩余的 34。\n","date":"2021年07月23日","permalink":"/posts/epoll-de-lt-he-et/","summary":"Linux epoll 的条件触发( Level Trigger )和边缘触发( Edge Trigger )","title":"epoll 的 LT 和 ET"},{"contents":"heap（堆），是一个用数组表示的完全二叉树，常用于构建优先队列，排序和快速找出一个集合中的最小值（或者最大值）。在 go 标准库下的 container 包中提供了这种数据结构。\n源码 interface 要想使用堆，需要实现 Interface 接口，这个接口中内嵌了一个 sort.Interface 接口，所以一共需要包含 5 个方法：Len() 用于返回长度，Swap() 用于交换两个值，Less() 用于定义比较两个值的规则，Push() 和 Pop() 分别代表添加到末尾和弹出末尾元素。通过 Less() 可以自定义堆为最大堆还是最小堆。\ntype Interface interface { sort.Interface Push(x interface{}) // 添加 x 到末尾 Pop() interface{} // 弹出末尾元素并返回 } 一开始我不是很明白为什么要实现 Push 和 Pop 两个方法，因为这两个方法的定义已经很明确了，后来看了 heap 包下的 example_pq_test.go 才明白了原因，主要在于一些操作还需要做一些额外的工作，比如优先级队列的实现，其实现的 Pop() 如下：\nfunc (pq *PriorityQueue) Pop() interface{} { old := *pq n := len(old) item := old[n-1] old[n-1] = nil // avoid memory leak // 将 index 置为 -1 是为了标识该数据已经出了优先级队列了 item.index = -1 *pq = old[0 : n-1] return item } Push func Push(h Interface, x interface{}) { h.Push(x) // 从下到上堆化 up(h, h.Len()-1) } Push 会先执行自己实现的 h.Push 方法，将 x 添加到末尾，之后再执行 up 进行堆化。\nup // up 代表上浮，从下到上堆化 // j 代表需要堆化的元素 index，up 会从该元素开始，不断向上调整堆 // 在 Push 中会调用 up func up(h Interface, j int) { for { // j 的父节点 i := (j - 1) / 2 // parent // i == j ：此时 i 是最后一个元素 // // 在解释 !h.Less(j, i) 之前，先来看看 Less(i, j int) 接口的定义： // // func (m myheap) Less(i, j int) bool { //\treturn m[i] \u0026lt; m[j]\t// 这代表实现的堆为最小堆 // } // // func (m myheap) Less(i, j int) bool { //\treturn m[i] \u0026gt; m[j]\t// 这代表实现的堆为最大堆 // } // // // 1. 最小堆，如果 j（子节点）大于 i（父节点），情况如下图： // //\t3\ti //\t\\ //\t5 j // // 此时便不需要交换 i 和 j，!h.Less(j, i) 对应 !m[j] \u0026lt; m[i]，即 m[j] \u0026gt; m[i] // // 最大堆同理 if i == j || !h.Less(j, i) { break } // 否则需要进行交换，还是以最小堆为例： // //\t5\ti //\t\\ //\t3 j // // 此时不满足 !h.Less(j, i)，此时 i（父节点）大于 j（子节点）， // 不满足最小堆的定义，所以需要进行交换 h.Swap(i, j) // up 代表上浮，所以更新 j 为其父节点 i j = i } } Push 的流程图示 执行以下代码：\ntype myheap []int // myheap 是一个最小堆 func TestHeapPushPop(t *testing.T) { h := new(myheap) heap.Push(h, 5) heap.Push(h, 10) heap.Push(h, 2) heap.Push(h, 99) heap.Push(h, 3) heap.Push(h, 233) heap.Push(h, 1) } Pop func Pop(h Interface) interface{} { n := h.Len() - 1 // 堆顶是最小（或最大）元素，将其交换到末尾，pop 会移除该元素 h.Swap(0, n) // FIXME down 的作用不确定，以下为猜测 // down 会将第二小（或大）的元素移动到堆顶，便于下次操作 down(h, 0, n) return h.Pop() } down func down(h Interface, i0, n int) bool { i := i0 for { j1 := 2*i + 1 // j1 是 i 的左子节点 // j1 \u0026gt; n 代表子节点不存在 // j1 == n 代表子节点是堆中最后一个节点，down 操作会忽略最后一个元素 if j1 \u0026gt;= n || j1 \u0026lt; 0 { // j1 \u0026lt; 0 after int overflow break } // j 用来保存子节点中较小（大）的那个，默认为左子节点 j := j1 // left child // j2 := j1 + 1，这表示 j2 是 i 的右子节点 // 这里是 j2 \u0026lt; n 而不是 j2 \u0026lt;= n，因为 down 操作会忽略最后一个元素 // 如果是最小堆，则选出 j1（左子节点）和 j2（右子节点）中较小的那个 // 如果是最大堆，则选出 j1（左子节点）和 j2（右子节点）中较大的那个 if j2 := j1 + 1; j2 \u0026lt; n \u0026amp;\u0026amp; h.Less(j2, j1) { // 满足条件，则更新 j 为右子节点 j = j2 // = 2*i + 2 // right child } // 判断 j（子节点） 和 i（父节点）的关系 // 如果是最小堆，!h.Less(j, i) 代表 j \u0026gt; i，即父节点小于较小的子节点， // 此时已经满足最小堆的特性了，直接 break // 如果是最大堆，!h.Less(j, i) 代表 j \u0026lt; i，与上面同理 if !h.Less(j, i) { break } // 到这里说明 j（子节点） 和 i（父节点）不满足堆的特性， // 如果是最小堆，说明此时 i \u0026gt; j，需要交换 // 如果是最大堆，说明此时 i \u0026lt; j，需要交换 h.Swap(i, j) // down 代表下沉，更新 i 为其子节点 j，进行下一轮循环 i = j } // 如果没有执行过 i = j，则不满足 i \u0026gt; i0 条件， // 这代表没有进行 down 操作 return i \u0026gt; i0 } Pop 的流程如下图： 写在最后 上面只对 heap 包中的核心函数进行了分析，剩余还有 Init，Remove，Fix() 函数，但是其内部也是调用了 up 和 down 这两个函数，所以就不一一分析了。\n","date":"2021年07月22日","permalink":"/posts/go-containerheap-yuan-ma-yue-du/","summary":"heap（堆），是一个用数组表示的完全二叉树，常用于构建优先队列，排序和快速找出一个集合中的最小值（或者最大值）。在 go 标准库下的 container 包中提供了这种数据结构。","title":"go container/heap 源码阅读"},{"contents":"输入整数数组 arr ，找出其中最小的 k 个数\n输入整数数组 arr ，找出其中最小的 k 个数。例如，输入4、5、1、6、2、7、3、8 这 8 个数字，则最小的 4 个数字是 1、2、3、4。\n示例 1：\n输入：arr = [3,2,1], k = 2 输出：[1,2] 或者 [2,1]\n示例 2：\n输入：arr = [0,1,2,1], k = 1 输出：[0]\n方法1 排序 使用任意一种排序算法进行排序，排序后取前 k 个即可，这里使用快排：\nfunc getLeastNumbers(arr []int, k int) []int { quickSort(arr) return arr[:k] } func quickSort(arr []int) { if len(arr) \u0026lt;= 1 { return } q := arr[0] l, r := 0, len(arr)-1 for l \u0026lt; r { for r \u0026gt; l \u0026amp;\u0026amp; arr[r] \u0026gt; q { r-- } arr[l] = arr[r] for r \u0026gt; l \u0026amp;\u0026amp; arr[l] \u0026lt;= q { l++ } arr[r] = arr[l] } arr[l] = q quickSort(arr[:r]) quickSort(arr[r+1:]) } 方法2 优化快排 题目只是要求找出 top k，但是方法 1 却对整个数组进行了排序，显然是不必要的。这里可以利用快排的性质做一些优化，快排的核心是，每轮都会选出一个基准点，以该点为分隔线，将比它小的都放在左边，比它大的放在右边，可以利用这个特性对 topk 进行优化。\n当挑选出基准点 p 时，将其与 k 进行比对，有以下几种情况：\np \u0026lt; k\n比如：-1, 1, 2, 5, 3, 5, 9，k = 5，第一轮求得 p = [2] = 2\n此时 2 作为基准点，其左边元素都比它小，右边元素都比它大，题目要求的是求前 5 个最小元素，现在已经确定了 -1，1，2 三个元素，还差 5 - 3 = 2 个，且这两个元素一定在 2 的右边，所以可以再对右边部分 [5, 3, 5, 9] 进行快排，同时 k 需要更改为 k - p - 1 = 5 - 2 - 1 = 2。\n上面的结果：5, 3, 5, 9，k = 2，p = [2] = 5，此时 k = p，结束（k = p 的情况在后面会说明）。至此，数组排序为 -1, 1, 2, 5, 3, 5, 9，可以看到并没有对整个数组进行排序。最后只需要返回排序后数组的前 k 个元素即可，即 -1, 1, 2, 5, 3（前 k 个元素不需要有序）\np \u0026gt; k\n比如：-1, 2, 3, 4, 1, 5, 8, 9, k = 3，第一轮求得 p = [5] = 5\np 的左边都是小于 p 的，而此时 p \u0026gt; k，代表我们只需要 3 个元素，但是 p 之前却包含了 5 个元素，这超过了我们的所需数量，所以需要再从这 5 个元素找出最小的 3 个，即再对 p 的左边部分进行快排。\np = k\n比如：-1, 1, 2, 5, 3, 5, 9，k = 2，第一轮求得 p = [2] = 2\n此时 p = k，这代表着：p 左边的元素数量刚好等于所需的数量 k，此时数组的 [0:k-1] 部分已经是所需了，无需任何操作，直接返回即可，\n通过上诉操作可以避免对整个数组进行排序，提示了效率。\nfunc getLeastNumbers(arr []int, k int) []int { quickSort(arr, k) return arr[:k] } func quickSort(arr []int, k int) { if len(arr) \u0026lt;= 1 { return } q := arr[0] l, r := 0, len(arr)-1 for l \u0026lt; r { for r \u0026gt; l \u0026amp;\u0026amp; arr[r] \u0026gt; q { r-- } arr[l] = arr[r] for r \u0026gt; l \u0026amp;\u0026amp; arr[l] \u0026lt;= q { l++ } arr[r] = arr[l] } arr[l] = q if l \u0026gt; k { quickSort(arr[:l], k) } else if l \u0026lt; k { quickSort(arr[l+1:], k-l-1) } return } 方法 3 堆 type myHeap []int func (h myHeap) Len() int { return len(h) } func (h myHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h myHeap) Less(i, j int) bool { return h[i] \u0026gt; h[j] } func (h *myHeap) Push(x interface{}) { *h = append(*h, x.(int)) } func (h *myHeap) Pop() interface{} { p := (*h)[len(*h)-1] *h = (*h)[:len(*h)-1] return p } func (h myHeap) Top() int { return h[0] } func getLeastNumbers(arr []int, k int) []int { if len(arr) == 0 || k == 0 { return nil } h := new(myHeap) for i := 0; i \u0026lt; len(arr); i++ { if h.Len() \u0026lt; k { heap.Push(h, arr[i]) } else { if h.Top() \u0026gt; arr[i] { heap.Pop(h) heap.Push(h, arr[i]) } } } return *h } ","date":"2021年07月21日","permalink":"/posts/jian-zhi-offer-40-zui-xiao-de-k-ge-shu/","summary":"输入整数数组 arr ，找出其中最小的 k 个数","title":"剑指 Offer 40. 最小的 k 个数"},{"contents":"反转链表的指定部分\n给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u0026lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回反转后的链表 。\n示例1：\n1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5\t反转前\n1 -\u0026gt; 4 -\u0026gt; 3 -\u0026gt; 2 -\u0026gt; 5\t反转后\n输入：head = [1,2,3,4,5], left = 2, right = 4 输出：[1,4,3,2,5]\n说明：反转第二个元素到第四个元素这部分（从 1 开始）\n进阶： 你可以使用一趟扫描完成反转吗？\n方法1 断链，反转，再拼接 这是一个比较直观，容易想到的方法，首先找到 left 和 right 对应的节点，在上面的例子中，left 对应节点为 2，right 对应节点为 4，然后将 right 断链，即 right.Next = nil，此时链表变为：1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4。同时还要记录 left 的前一个节点和 right 的后一个节点，用于之后恢复链表。\n再定义一个反转链表函数，和 lc206 反转链表 一样，传入参数为 left 节点，函数会把 left 到 right 这部分反转（right 已经断链了，所以不会影响 right 后面的部分），反转过后 2 -\u0026gt; 3 -\u0026gt; 4 变为 4 -\u0026gt; 3 -\u0026gt; 2，但是这时 1 依然还指向 2，所以这里需要更新 1 的指向，将 1 指向反转后的链表头，此时链表为 1 -\u0026gt; 4 -\u0026gt; 3 -\u0026gt; 2，然后再将链表尾指向先前保存的 right 的后一个节点，1 -\u0026gt; 4 -\u0026gt; 3 -\u0026gt; 2 -\u0026gt; 5，大功告成。\n代码：\n/** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func reverseBetween(head *ListNode, left int, right int) *ListNode { var ( leftPrev, leftNode *ListNode rightNext, rightNode *ListNode count = 1 ) h := head for h != nil { // 找到 left 对应的节点及该节点的 prev 节点 if count != left \u0026amp;\u0026amp; leftNode == nil { leftPrev = h } else if count == left { leftNode = h } // 找到 right 对应的节点及该节点的 next 节点 if count == right \u0026amp;\u0026amp; rightNode == nil { rightNode = h rightNext = h.Next } h = h.Next count++ } if rightNode != nil { rightNode.Next = nil // 断链 } // 把 leftNode 到 rightNode 这部分反转 reverse(leftNode) //fmt.Println(leftPrev, leftNode, rightNode, rightNext) //printList(rightNode) // 反转之后，rightNode 成了头节点，leftNode 成了最后一个节点 // 此时再将 leftPrev 接上 rightNode，leftNode 接上 rightNext if leftPrev != nil { leftPrev.Next = rightNode } else { // 如果 leftPrev 为 nil leftNode.Next = rightNext return rightNode } if leftNode != nil { leftNode.Next = rightNext } return head } func reverse(head *ListNode) { var ( cur = head next *ListNode prev *ListNode ) for cur != nil { next = cur.Next cur.Next = prev prev = cur cur = next } } // 调试函数 func printList(head *ListNode) { h := head for h != nil { fmt.Print(h.Val, \u0026quot; -\u0026gt; \u0026quot;) h = h.Next } } 上面的代码有不少的边界判断，导致略显冗余杂乱，可能更好的解决方法是设置一个哨兵节点，这样就可以应对 一些边界情况了，比如 left 是第一个元素，这时 prev 将是 nil，当 leftPrev.Next 时就会报错，但设置了哨兵节点后，prev 是哨兵节点，这样 leftPrev.Next 就不会产生错误了，但是这里我还没有实践，因为还有一种更好的方法。\n方法2 一次遍历 方法 1 虽然可以 ac，但是不满足题目的进阶要求：只遍历链表一次。在参考了题解后，了解了另一种方法，但是该方法略微有些繁琐，文字难以描述，直接上图更好理解：\n代码如下：\nfunc reverseBetween(head *ListNode, left int, right int) *ListNode { var ( s = \u0026amp;ListNode{Next: head} prev = s cur, next *ListNode ) for i := 0; i \u0026lt; left-1; i++ { prev = prev.Next } cur = prev.Next next = cur.Next for i := 0; i \u0026lt; right-left; i++ { cur.Next = next.Next next.Next = prev.Next prev.Next = next next = cur.Next } return s.Next } ","date":"2021年07月20日","permalink":"/posts/leetcode-92-fan-zhuan-lian-biao-ii/","summary":"反转链表的指定部分\n给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u0026lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回反转后的链表 。","title":"LeetCode 92. 反转链表 II"},{"contents":"select 错误使用记录，加深对 select 的理解\n错误的代码 有如下一个使用多路复用 select 实现的 echo（回响）服务器：\nint main() { char buf[BUF_SIZE]; // 简单封装了一下 socket 的创建流程 Server s(\u0026quot;tcp\u0026quot;, \u0026quot;0.0.0.0\u0026quot;, \u0026quot;8080\u0026quot;); s.Listen(1024); fd_set readfds; // 监听服务端socket，当有客户连接时会触发事件 FD_ZERO(\u0026amp;readfds); FD_SET(s.Sockfd(), \u0026amp;readfds); int maxfd = s.Sockfd(); for (; ;) { int okcnt = select(maxfd+1, \u0026amp;readfds, nullptr, nullptr, nullptr); if (okcnt == -1) { printf(\u0026quot;select error\\n\u0026quot;); break; } if (okcnt == 0) { continue; } // 遍历 select 数组 for (int i = 0; i \u0026lt; maxfd+1; ++i) { // 找到所有值为 1 的 fd，这表示该 fd 已经准备就绪了 if (FD_ISSET(i, \u0026amp;readfds)) { // 如果就绪的 fd 为 server,说明有新的连接请求 if (i == s.Sockfd()) { // 调用 accept 接收客户端的连接请求 auto conn = s.Accept(); int connfd = conn-\u0026gt;Connfd(); // 更新 maxfd if (connfd \u0026gt; maxfd) { maxfd = connfd; } // 将 conn 的 fd 添加到 select 监听集合中 FD_SET(connfd, \u0026amp;readfds); printf(\u0026quot;connected client: %d\\n\u0026quot;, conn-\u0026gt;Connfd()); // 如果就绪的 fd 不是 server fd，则是 conn fd } else { printf(\u0026quot;conn %d is already\\n\u0026quot;, i); // 可以对这些准备就绪的 conn 进行 echo 处理了 int n = read(i, buf, BUF_SIZE); if (n == 0) { // EOF // FD_CLR(i, \u0026amp;readfds); close(i); printf(\u0026quot;close conn: %d\\n\u0026quot;, i); } else { write(i, buf, n); } } } } } s.Close(); return 0; } 运行结果 编译并运行：\ng++ -o echo_server_wrong_example echo_server_wrong_example.cc ./echo_server_wrong_example 新建一个终端并开启第一个客户端：\nnc localhost 8080 此时只连接到服务端，不发送任何数据，服务端的日志：\nconnected client: 4 conn 4 is already 随后再新建一个终端并开启第二个客户端，并尝试发送数据：\nnc localhost 8080 1 但是没有任何回应，查看服务端日志：\nconnected client: 4 conn 4 is already 根本没有接收第二个服务端的连接！说明编写的这个程序是没有并发能力的，一定是某个地方写错了\n再返回去看第一个客户端是否能正常收到响应：\nnc localhost 8080 1 // 发送的 1\t// 响应的 第一个客户端是可以正常工作的\n再将第一个服务端结束，服务端日志：\nclose conn: 4 此时切换到第二个客户端，发现原来阻塞的请求得到了响应：\nnc localhost 8080 1 l 服务端日志：\n// 之前的 connected client: 4 conn 4 is already close conn: 4 // 新的 connected client: 4 conn 4 is already 并且之后的所有请求都能够得到响应\n此时再启动第一个客户端：\n服务端日志：\nconnected client: 5 conn 5 is already 尝试发送数据：\nkk kk adasd adasd 全部能够得到响应\n再切到第二个客户端，并发送数据，结果发现阻塞了，服务端日志没有任何变化\n正确的代码 修改为正确的程序：\n// // Created by root on 7/17/21. // #include \u0026quot;../pkg/net/net.h\u0026quot; #include \u0026lt;sys/select.h\u0026gt; #define BUF_SIZE 100 // g++ -o echo_server echo_server.cc ../pkg/net/net.cpp int main() { char buf[BUF_SIZE]; Server s(\u0026quot;tcp\u0026quot;, \u0026quot;0.0.0.0\u0026quot;, \u0026quot;8080\u0026quot;); s.Listen(1024); fd_set readfds; FD_ZERO(\u0026amp;readfds); // 监听服务端socket，当有客户连接时会触发事件 FD_SET(s.Sockfd(), \u0026amp;readfds); int maxfd = s.Sockfd(); for (; ;) { // 这句话非常重要，否则会出现多条连接只有一条能正常工作，其他全部阻塞的 bug // readfds 保存的是要监听的 fd 集合，但是每次调用 select 后，会将这些监 // 听 fd 中已经准备好的置 1，未准备好的置 0，如果使用这个已被更改的 fdset， // 可能会导致一些异常情况发生，所以这里将 fdset 拷贝一份，调用 select 时传 // 入拷贝值，这样 select 的更改就不会影响到原 fdset fd_set cpyset = readfds; // okcnt：准备就绪的 fd 数量 int okcnt = select(maxfd+1, \u0026amp;cpyset, nullptr, nullptr, nullptr); if (okcnt == -1) { printf(\u0026quot;select error\\n\u0026quot;); break; } if (okcnt == 0) { continue; } // 遍历 select 数组 for (int i = 0; i \u0026lt; maxfd+1; ++i) { // 找到所有值为 1 的 fd，这表示该 fd 已经准备就绪了 if (FD_ISSET(i, \u0026amp;cpyset)) { // 如果就绪的 fd 为 server,说明有新的连接请求 if (i == s.Sockfd()) { // 调用 accept 接收客户端的连接请求 auto conn = s.Accept(); int connfd = conn-\u0026gt;Connfd(); // 更新 maxfd if (connfd \u0026gt; maxfd) { maxfd = connfd; } // 将 conn 的 fd 添加到 select 监听集合中 FD_SET(connfd, \u0026amp;readfds); printf(\u0026quot;connected client: %d\\n\u0026quot;, conn-\u0026gt;Connfd()); // 如果就绪的 fd 不是 server fd，则是 conn fd } else { printf(\u0026quot;conn %d is already\\n\u0026quot;, i); // 可以对这些准备就绪的 conn 进行 echo 处理了 int n = read(i, buf, BUF_SIZE); if (n == 0) { // EOF // FD_CLR(i, \u0026amp;readfds); close(i); printf(\u0026quot;close conn: %d\\n\u0026quot;, i); } else { write(i, buf, n); } } } } } s.Close(); return 0; } 这段代码只更改了几个地方：\n29 行的 fd_set cpyset = readfds\n32 行的 select(maxfd+1, \u0026amp;cpyset, nullptr, nullptr, nullptr)，将第二个参数由 readfds 更改为 cpyset\n44 行的 if (FD_ISSET(i, \u0026amp;cpyset))，之前为 if (FD_ISSET(i, \u0026amp;readfds))\n分析 简单说明下程序的流程：\n先将服务端 socket 加入到 select 的监听集合，之后调用 select int okcnt = select(maxfd+1, \u0026amp;readfds, nullptr, nullptr, nullptr); 这里会阻塞，直到有 fd 准备好。\n当服务端 socket 准备好了以后，select 返回 1（因为只监听了一个 fd），继续向下执行 for (int i = 0; i \u0026lt; maxfd+1; ++i)，继续执行 if (FD_ISSET(i, \u0026amp;readfds))，这样会遍历 fdset 并找到准备就绪的 fd，此时只有服务端的 fd 准备好了，所以必然进入下面的语句if (i == s.Sockfd())，之后 FD_SET(connfd, \u0026amp;readfds)并且更新 maxfd。\n问题来了，FD_SET(connfd, \u0026amp;readfds)是希望将 connfd 添加到监听集合中，但是程序会错误的继续执行 for (int i = 0; i \u0026lt; maxfd+1; ++i)，并找到 connfd，因为刚刚的 FD_SET 操作把其置于 1，导致程序错误的认为该 fd 已经准备就绪，又因为不满足 if (i == s.Sockfd())，所以进入 else 分支，进入 read(i, buf, BUF_SIZE) 操作，但是因为该 fd 根本没有准备好，所以整个程序会进入阻塞。\n造成该问题的根本原因是 select 的机制问题：传入要监听的 fd 到 fdset 中，当其返回时，会将监听中已就绪的置1，未就绪的置 0，fdset 即充当了记录被监听 fd 的角色，又充当了记录已就绪 fd 的角色，这样会导致混乱的结果，在错误的程序中就是这样，readfds 充当了两个角色，导致了程序的不正确。\n解决方法就是额外创建一个 fdset，一个用来记录要监听的 fd，一个记录已就绪 fd，在正确的代码中， readfds就是用来记录要监听的 fd，cpyset 是用来记录已就绪 fd。\n每次 for 循环，cpyset 都会拷贝 readfds获得要监听的所有 fd，并传入 select，得到已就绪的 fd，之后的 FD_ISSET 也是基于cpyset，当要新记录 fd 时，执行语句FD_SET(connfd, \u0026amp;readfds)，将 fd 添加到 readfds 中，删除 fd 时也是 FD_CLR(i, \u0026amp;readfds)，这样二者各司其职，就可以保证程序的正常工作了。\n","date":"2021年07月18日","permalink":"/posts/linux-select-de-cuo-wu-shi-yong-an-li/","summary":"select 错误使用记录，加深对 select 的理解\n错误的代码 有如下一个使用多路复用 select 实现的 echo（回响）服务器：","title":"linux select 的错误使用案例"},{"contents":"给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度\n给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。\n示例 1: 输入: \u0026ldquo;bbbab\u0026rdquo;\n输出: 4\n一个可能的最长回文子序列为 \u0026ldquo;bbbb\u0026rdquo;。\n示例 2: 输入: \u0026ldquo;cbbd\u0026rdquo;\n输出: 2 一个可能的最长回文子序列为 \u0026ldquo;bb\u0026rdquo;。\n提示：\n1 \u0026lt;= s.length \u0026lt;= 1000 s 只包含小写英文字母\n方法1 动态规划 这道题和 LeetCode 5 最长回文子串 类似，（https://zengh1.github.io/post/leetcode-5-zui-chang-hui-wen-zi-chuan/ ）\n代码如下： cpp\nclass Solution { public: int longestPalindromeSubseq(string s) { int len = s.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(len, vector\u0026lt;int\u0026gt;(len, 0)); int maxlen = 1; for (int i = len - 1; i \u0026gt;= 0; i--) { for (int j = i; j \u0026lt; len; j++) { if (i == j) { dp[i][j] = 1; } else if (s[i] == s[j]) { dp[i][j] = dp[i+1][j-1] + 2; } else { dp[i][j] = max(dp[i+1][j], dp[i][j-1]); } maxlen = max(maxlen, dp[i][j]); } } return maxlen; } }; ","date":"2021年07月01日","permalink":"/posts/leetcode-516-zui-chang-hui-wen-zi-xu-lie/","summary":"给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度\n给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。","title":"LeetCode 516. 最长回文子序列"},{"contents":"给你一个字符串 s，找到 s 中最长的回文子串。\n给你一个字符串 s，找到 s 中最长的回文子串。\n示例 1：\n输入：s = \u0026ldquo;babad\u0026rdquo; 输出：\u0026ldquo;bab\u0026rdquo; 解释：\u0026ldquo;aba\u0026rdquo; 同样是符合题意的答案。\n示例 2：\n输入：s = \u0026ldquo;cbbd\u0026rdquo; 输出：\u0026ldquo;bb\u0026rdquo;\n示例 3：\n输入：s = \u0026ldquo;a\u0026rdquo; 输出：\u0026ldquo;a\u0026rdquo;\n示例 4：\n输入：s = \u0026ldquo;ac\u0026rdquo; 输出：\u0026ldquo;a\u0026rdquo;\n提示：\n1 \u0026lt;= s.length \u0026lt;= 1000 s 仅由数字和英文字母（大写和/或小写）组成\n方法1 动态规划 代码如下：\ncpp\nclass Solution { public: string longestPalindrome(string s) { vector\u0026lt;vector\u0026lt;bool\u0026gt;\u0026gt; dp(s.size(), vector\u0026lt;bool\u0026gt;(s.size())); string res; int maxlen = 0; for (int i = s.size(); i \u0026gt;= 0; i--) { for (int j = i; j \u0026lt; s.size(); j++) { if (i == j) { dp[i][j] = true; } else if (s[i] != s[j]) { dp[i][j] = false; } else if (i == j-1) { dp[i][j] = s[i] == s[j]; } else if (s[i] == s[j]) { dp[i][j] = dp[i+1][j-1]; } if (dp[i][j]) { if (j-i \u0026gt;= maxlen) { maxlen = j - i; res = s.substr(i, j-i+1); } } } } return res; } }; ","date":"2021年06月29日","permalink":"/posts/leetcode-5-zui-chang-hui-wen-zi-chuan/","summary":"给你一个字符串 s，找到 s 中最长的回文子串。\n给你一个字符串 s，找到 s 中最长的回文子串。","title":"LeetCode 5. 最长回文子串"},{"contents":"给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。\n给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。\n示例 1:\n输入: num1 = \u0026ldquo;2\u0026rdquo;, num2 = \u0026ldquo;3\u0026rdquo; 输出: \u0026ldquo;6\u0026rdquo;\n示例 2:\n输入: num1 = \u0026ldquo;123\u0026rdquo;, num2 = \u0026ldquo;456\u0026rdquo; 输出: \u0026ldquo;56088\u0026rdquo;\n说明：\nnum1 和 num2 的长度小于110。 num1 和 num2 只包含数字 0-9。 num1 和 num2 均不以零开头，除非是数字 0 本身。 不能使用任何标准库的大数类型（比如 BigInteger）或直接将输入转换为整数来处理。\n方法1 模拟数学 在草稿纸上写一下乘法的运算步骤，并将其改写为代码形式：\n// 45 //\t* 123 // --------------\t// 135\t// + 90 // 45 // --------------- // 5535 func multiply(num1 string, num2 string) string { var ( flag int // 保存进位 res string pown int // 确定要补几个 0 ) if num1 == \u0026quot;0\u0026quot; || num2 == \u0026quot;0\u0026quot; { return \u0026quot;0\u0026quot; } for i := len(num1) - 1; i \u0026gt;= 0; i-- { var r strings.Builder for j := len(num2) - 1; j \u0026gt;= 0; j-- { v1 := int(num1[i] - '0') v2 := int(num2[j] - '0') tmp := v1*v2 + flag flag = tmp / 10 tmp %= 10 r.WriteByte(byte(tmp + '0')) } if flag != 0 { r.WriteByte(byte(flag) + '0') flag = 0 } rstr := reverseStr(r.String()) // 45 //\t* 123 // --------------\t每步后面补 n-1 个 0，n 为 位数 // 135\t135\t补 0 个 // 90 =\u0026gt; 900\t补 1 个 // 45\t4500 补 2 个 for i := 0; i \u0026lt; pown; i++ { rstr += \u0026quot;0\u0026quot; } pown++ // 字符串相加 res = strAdd(res, rstr) } return res } func reverseStr(s string) string { b := []byte(s) for i, j := 0, len(s)-1; i \u0026lt; j; i, j = i+1, j-1 { b[i], b[j] = b[j], b[i] } return string(b) } func strAdd(s1, s2 string) string { var ( flag int res strings.Builder ) for i, j := len(s1)-1, len(s2)-1; i \u0026gt;= 0 || j \u0026gt;= 0; i, j = i-1, j-1 { var v1, v2 int if i \u0026gt;= 0 { v1 = int(s1[i] - '0') } else { v1 = 0 } if j \u0026gt;= 0 { v2 = int(s2[j] - '0') } else { v2 = 0 } sum := v1 + v2 + flag flag = sum / 10 sum %= 10 res.WriteByte(byte(sum) + '0') } if flag != 0 { res.WriteByte(byte(flag) + '0') } str := reverseStr(res.String()) return str } 方法2 优化 方法一的做法是从右往左遍历乘数，将乘数的每一位与被乘数相乘得到对应的结果，再将每次得到的结果累加，整个过程中涉及到较多字符串相加的操作。如果使用数组代替字符串存储结果，则可以减少对字符串的操作。具体的方法如下：\n代码如下：\ngo\nfunc multiply(num1 string, num2 string) string { if num1 == \u0026quot;0\u0026quot; || num2 == \u0026quot;0\u0026quot; { return \u0026quot;0\u0026quot; } n1l := len(num1) n2l := len(num2) n := make([]int, n1l+n2l, n1l+n2l) for i := n1l - 1; i \u0026gt;= 0; i-- { v1 := num1[i] - '0' for j := n2l - 1; j \u0026gt;= 0; j-- { v2 := num2[j] - '0' n[i+j+1] += int(v1) * int(v2) } } var ( flag int res strings.Builder start int ) for i := len(n) - 1; i \u0026gt;= 0; i-- { n[i] += flag flag = n[i] / 10 n[i] %= 10 } //fmt.Println(n) if n[0] == 0 { start = 1 } for ; start \u0026lt; len(n); start++ { s := strconv.Itoa(n[start]) res.WriteString(s) } return res.String() } cpp\nclass Solution { public: string multiply(string num1, string num2) { if (num1 == \u0026quot;0\u0026quot; || num2 == \u0026quot;0\u0026quot;) { return \u0026quot;0\u0026quot;; } int n1l = num1.size(); int n2l = num2.size(); vector\u0026lt;int\u0026gt; v(n1l + n2l); for (int i = n1l - 1; i \u0026gt;= 0; i--) { for (int j = n2l - 1; j \u0026gt;= 0; j--) { v[i+j+1] += (num1[i] - '0') * (num2[j] - '0'); } } int flag = 0; string res; int start = 0; for (int i = v.size() - 1; i \u0026gt;= 0; i--) { v[i] += flag; flag = v[i] / 10; v[i] %= 10; } // for (auto vv : v) { // cout \u0026lt;\u0026lt; vv \u0026lt;\u0026lt; endl; // } if (v[0] == 0) { start = 1; } for (; start \u0026lt; v.size(); start++) { res += char(v[start]+'0'); } return res; } }; ","date":"2021年06月23日","permalink":"/posts/leetcode-43-zi-fu-chuan-xiang-cheng/","summary":"给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。","title":"LeetCode 43. 字符串相乘"},{"contents":"最不经常使用（LFU）缓存算法\n请你为 最不经常使用（LFU）缓存算法设计并实现数据结构。\n实现 LFUCache 类：\nLFUCache(int capacity) - 用数据结构的容量 capacity 初始化对象 int get(int key) - 如果键存在于缓存中，则获取键的值，否则返回 -1。 void put(int key, int value) - 如果键已存在，则变更其值；如果键不存在，请插入键值对。当缓存达到其容量时，则应该在插入新项之前，使最不经常使用的项无效。在此问题中，当存在平局（即两个或更多个键具有相同使用频率）时，应该去除 最近最久未使用 的键。 注意「项的使用次数」就是自插入该项以来对其调用 get 和 put 函数的次数之和。使用次数会在对应项被移除后置为 0 。\n为了确定最不常使用的键，可以为缓存中的每个键维护一个 使用计数器 。使用计数最小的键是最久未使用的键。\n当一个键首次插入到缓存中时，它的使用计数器被设置为 1 (由于 put 操作)。对缓存中的键执行 get 或 put 操作，使用计数器的值将会递增。\n示例：\n输入： [\u0026ldquo;LFUCache\u0026rdquo;, \u0026ldquo;put\u0026rdquo;, \u0026ldquo;put\u0026rdquo;, \u0026ldquo;get\u0026rdquo;, \u0026ldquo;put\u0026rdquo;, \u0026ldquo;get\u0026rdquo;, \u0026ldquo;get\u0026rdquo;, \u0026ldquo;put\u0026rdquo;, \u0026ldquo;get\u0026rdquo;, \u0026ldquo;get\u0026rdquo;, \u0026ldquo;get\u0026rdquo;] [[2], [1, 1], [2, 2], [1], [3, 3], [2], [3], [4, 4], [1], [3], [4]] 输出： [null, null, null, 1, null, -1, 3, null, -1, 3, 4]\n解释： // cnt(x) = 键 x 的使用计数 // cache=[] 将显示最后一次使用的顺序（最左边的元素是最近的） LFUCache lFUCache = new LFUCache(2); lFUCache.put(1, 1); // cache=[1,_], cnt(1)=1 lFUCache.put(2, 2); // cache=[2,1], cnt(2)=1, cnt(1)=1 lFUCache.get(1); // 返回 1 // cache=[1,2], cnt(2)=1, cnt(1)=2 lFUCache.put(3, 3); // 去除键 2 ，因为 cnt(2)=1 ，使用计数最小 // cache=[3,1], cnt(3)=1, cnt(1)=2 lFUCache.get(2); // 返回 -1（未找到） lFUCache.get(3); // 返回 3 // cache=[3,1], cnt(3)=2, cnt(1)=2 lFUCache.put(4, 4); // 去除键 1 ，1 和 3 的 cnt 相同，但 1 最久未使用 // cache=[4,3], cnt(4)=1, cnt(3)=2 lFUCache.get(1); // 返回 -1（未找到） lFUCache.get(3); // 返回 3 // cache=[3,4], cnt(4)=1, cnt(3)=3 lFUCache.get(4); // 返回 4 // cache=[3,4], cnt(4)=2, cnt(3)=3\n提示：\n0 \u0026lt;= capacity, key, value \u0026lt;= 104 最多调用 105 次 get 和 put 方法\n进阶：你可以为这两种操作设计时间复杂度为 O(1) 的实现吗？\n方法1\t双哈希表+链表 哈希表1：\nkey 记录使用次数\nvalue 为一条链表\n因为可能有多个键使用次数相同，所以用链表将这些相同频次的键连接起来，此外，该链表也可以记录这些键的使用时间，链表头为最近使用过的，链表尾为最近最久未使用的。当需要淘汰键时，移除链表尾即可，当需要插入时，添加到链表头。\n哈希表2：\nkey 与键相同，做快速映射\nvalue 为链表中的一个节点\n此外，还需要一个 min 值来保存全局最少使用次数，当需要淘汰时便。\n大体思路 由题可知，当执行 get 和 put 操作时，都会使得键的使用次数增加，而使用次数增加后，就需要从当前链表移动到其他链表中，操作思路为：先获取到当前键的使用次数，通过 哈希表1 定位到所在链表，并从中移除，再通过哈希表1 获取 使用次数+1 处的链表，并将键添加到链表头，（这两步都需要注意映射的链表是否为 null，否则会导致空指针错误），此外还需要更新该 key 的使用次数。\nget 首先从 哈希表2 中查找，如果找到了，则执行上面的移动操作，并返回值，未找到则返回 -1。\nput 首先需要查找是否存在，如果存在则只更新 value，同时执行移动操作，返回。如果不存在，则需要先判断是否需要淘汰，淘汰可以通过全局最少使用次数 min 从 哈希表1 中定位到链表，并移除该链表的尾元素，同时也要从 哈希表2 中移除。如果不需要淘汰，则直接添加到两个哈希表即可，因为是新值，所以使用次数为 1，要将 min 置为 1。\n这里还有一个问题，如何更新 min 的值呢？当需要移动时，检查一下移动后，该链表长度是否为 0，如果为空则更新 min。例如：\n哈希表1 情况如下：\n1 -\u0026gt; [3, 3] 2 -\u0026gt; [1, 1] 此时 min = 1\n当执行 get 3 操作时： 2 -\u0026gt; [3, 3] -\u0026gt; [1, 1] 此时应该更新 min = 2\n当 [3, 3] 从 1 移动到 2 后，1 对应的链表长度为 0，且 [3, 3] 的使用次数 = min = 1，所以需要将 min 更新为 2。\n代码： package lfu import \u0026quot;container/list\u0026quot; type LFUCache struct { // 记录出现次数，key 为次数，value 为链表， // 因为同一个次数可能有多个值，按照规定，次数相同时按使用时间来比较， // 所以用链表来记录顺序，新节点插入到链表头，当需要淘汰时，淘汰链表尾 cm map[int]*list.List // 以键值 key 为索引，每个索引存放对应缓存在 cm 中链表里的内存地址 m map[int]*list.Element // 最大容量 cap int // 全局最少使用次数，当需要执行淘汰操作时，通过该变量可以在 O(1) 内定位到 cm 的对应链表 min int } // node 链表中存储的值 type node struct { key int val int cnt int // 使用次数 } func Constructor(capacity int) LFUCache { return LFUCache{ cm: make(map[int]*list.List), m: make(map[int]*list.Element), cap: capacity, min: 0, } } func (c *LFUCache) Get(key int) int { if c.cap == 0 { return -1 } if e, ok := c.m[key]; ok { n := e.Value.(*node) // 将 n 从旧的链表中移除，因为 get() 后其使用次数增加了 if c.cm[n.cnt] != nil { c.cm[n.cnt].Remove(e) } // 如果移除后旧链表为空，则更新全局最小使用次数 min if c.cm[n.cnt] == nil || c.cm[n.cnt].Len() == 0 { // 链表为空了，cm 保存也就没意义了，可以从 cm 中删除 delete(c.cm, n.cnt) if c.min == n.cnt { c.min++ } } // 更新 n 的使用次数 n.cnt++ // 获取新的链表，并将 n 添加到头部 if c.cm[n.cnt] == nil { c.cm[n.cnt] = list.New() } ele := c.cm[n.cnt].PushFront(n) // 更新 m 中的信息 c.m[key] = ele return e.Value.(*node).val } return -1 } func (c *LFUCache) Put(key int, value int) { // 需要先判断是否存在，再判断容量是否溢出，例如 // LFU 容量为 2，现有如下数据： // 1 -\u0026gt; [1,5] -\u0026gt; [2,6] // 使用 1 次对应的链表 // 此时 push [1,2]，如果先判断容量溢出，则会触发移除操作， // 将 [2,6] 移除，导致结果为 1 -\u0026gt; [1,2] -\u0026gt; [1,5]， // 正确结果应该为： // 1 -\u0026gt; [2,6] // 2 -\u0026gt; [1,2] // 如果该 key 已经存在 if no, ok := c.m[key]; ok { n := no.Value.(*node) if c.cm[n.cnt] != nil { c.cm[n.cnt].Remove(no) } if c.cm[n.cnt] == nil || c.cm[n.cnt].Len() == 0 { delete(c.cm, n.cnt) if c.min == n.cnt { c.min++ } } n.cnt++ // 使用次数 +1 n.val = value // 更新 val if c.cm[n.cnt] == nil { c.cm[n.cnt] = list.New() } ele := c.cm[n.cnt].PushFront(n) c.m[key] = ele return } // 如果添加该节点后容量溢出，则需要淘汰使用次数最少的节点 if len(c.m) == c.cap { // 根据 min 字段，取出最小使用次数对应的链表 l := c.cm[c.min] if l != nil { //fmt.Println(l.Back()) // 该链表的最后一个即为最长时间未使用的节点，将其移除 rm := l.Remove(l.Back()) // 同时也从 m 中移除 delete(c.m, rm.(*node).key) } } // key 不存在 n := \u0026amp;node{ key: key, val: value, cnt: 1, } if _, ok := c.cm[1]; !ok { c.cm[1] = list.New() } l := c.cm[1] // 添加到计数链表中 node := l.PushFront(n) // 添加到 m 中 c.m[key] = node // 更新全局最小使用次数 min 为 1 c.min = 1 } ","date":"2021年06月16日","permalink":"/posts/leetcode-460-lfu-huan-cun/","summary":"最不经常使用（LFU）缓存算法\n请你为 最不经常使用（LFU）缓存算法设计并实现数据结构。\n实现 LFUCache 类：","title":"LeetCode 460. LFU 缓存"},{"contents":" 25 匹马，5条跑道，每个跑道最多能有 1 匹马进行比赛，（就是说最多同时可以有 5 匹马一起比赛）， 不能使用计时器，且每匹马的速度都很稳定（意思是在上一场比赛中 A 马比 B 马快，则下一场比赛中A马 依然比 B 马快），问最少比多少次能比出前3名和前5名？\n前 3 名 首先将 25 匹马分成 5 组，每组 5 匹马。\n1 - 5 次 每组分别进行比赛，选出每组的第一名，假设总体比赛情况为：\nA组： [A1 A2 A3 A4 A5] B组： [B1 B2 B3 B4 B5] C组： [C1 C2 C3 C4 C5] D组： [D1 D2 D3 D4 D5] E组： [E1 E2 E3 E4 E5] 每组第一名为 [A1, B1, C1, D1, E1]\n第 6 次 再对这 5 个第一名进行比赛，这里面的第一名便是所有马中最快的，这里假设结果为 [A1, B1, C1, D1, E1]，第一名为 A1。\n此时第一名已经确定了，还剩下二三名需要确定。\n第 7 次 第二名可能是紧随 A1 其后的 A2，同时也可能是 B 组的第一名 B1，因为不能使用计时器，所以无法确定到底是哪个，解决办法就是让 A2 和 B1 进行一次比赛，决出第二名，但是跑道一共有 5 条，现在却只有两匹马参赛，空出来的 3 条跑道岂不是白白浪费了，这里可以利用起来，决出第三名。\n可能出现的情况：\n第二名为 A2 那么第三名将从 A2 之后的 A3，以及 B 组的第一 B1 二者中确定。 第二名为 B1 第三名将从 A2，B2，以及 C 组的头榜 C1 中确定。 第二三名的全部情况：\n[A2, A3] [A2, B1] [B1, B2] [B1, A2] [B1, C1]\n所以用上面的 5 匹马 [A2, A3, B1, B2, C1] 进行一次比赛即可确定第2，3名，其中的第1，2名则为全部马匹中的第2，3名。\n综上，最少需要 7 次即可确定马匹中的前 3 名。\n前 5 名 1-7 次 和 前 3 名 一样，通过这 7 次可以确定出所有马匹中的 1，2，3名。全部情况为： [A1, A2, A3] [A1, A2, B1] [A1, B1, B2] [A1, B1, A2] [A1, B1, C1]\n第 8 次 第 8 次需要根据第7场的所有可能的比赛情况进行分析。\n第二名 = A2，第三名 = A3，那么第四名为 A4 或者 B1。\n如果第四名为 A4，那么第五名为 A5 或者 B1。 如果第四名为 B1，那么第五名为 A4、B2 或者 C1。 所以用 [A4, B1, A5, B2, C1] 即可确定出第 4，5名，一共需要 8 次。\n第二名 = A2，第三名 = B1，那么第四名为 A3 或者 B2 或者 C1。\n如果第四名为 A3，那么第五名为 A4 或者 B2。 如果第四名为 B2，那么第五名为 B3 或者 A3。 如果第四名为 C1，那么第五名从 A3, B2, C2, D1 中产生。 综上，用 [A3, B2, B3, C1, A4, C2, D1] 七匹马才可以确定4， 5名，而一次比赛最多只能有 5 匹马，所以需要比赛 2 次，一共需要 9 次。\n第二名 = B1，第三名 = B2，那么第四名为 B3、A2 或者 C1。\n如果第四名为 B3，那么第五名为 B4、A2、C1。 如果第四名为 A2，那么第五名为 A3、B3，C1。 如果第四名为 C1，那么第五名为 A2、B3、C2、D1。 综上，用 [A2, B3, B4, C1, A3, C2, D1] 七匹马才可以确定4， 5名，而一次比赛最多只能有 5 匹马，所以需要比赛 2 次，一共需要 9 次。\n第2名=B1，第3名=A2，和情况 2 一样。\n第2名=B1，第3名=C1。那么此种情况下第4名只能在A2、B2、C2、D1中产生。\n如果第4名=A2，那么第5名只能在A3、B2、C2、D1中产生。\n如果第4名=B2，那么第5名只能在A2、B3、C2、D1中产生。\n如果第4名=C2，那么第5名只能在A2、B2、C3、D1中产生。\n如果第4名=D1，那么第5名只能在A2、B2、C2、D2、E2中产生。\n那么，第4、5名需要在马匹[A2、B2、C2、D1、A3、B3、C3、D2、E1]九匹马中产生，因此也必须比赛两场，也就是到第 9 场决出胜负。\n综上，确定前 5 名最少需要 8 或 9 次。\n参考：https://www.iteye.com/blog/hxraid-662643，这篇博文讲解的非常棒，我这里只是将其用自己的话再叙述了一遍，以加深理解。\n","date":"2021年06月15日","permalink":"/posts/25-pi-ma-de-jiao-zhu/","summary":"25 匹马，5条跑道，每个跑道最多能有 1 匹马进行比赛，（就是说最多同时可以有 5 匹马一起比赛）， 不能使用计时器，且每匹马的速度都很稳定（意思是在上一场比赛中 A 马比 B 马快，则下一场比赛中A马 依然比 B 马快），问最少比多少次能比出前3名和前5名？","title":"25匹马的角逐"},{"contents":" 作者：luozhiyun 博客：https://www.luozhiyun.com/archives/475 本文使用的 Go 的源码1.15.7\n原文地址：https://www.luozhiyun.com/archives/475\n1. 三色标记法 1.1 三种颜色 黑色：该对象已经被标记过了，且该对象下的属性【这里不是很明白，是指该对象引用的对象？？】也全部都被标记过了（程序所需要的对象）\n说明：仅是该对象下的属性，其之下的属性的下面的属性不考虑在内。\n例如：A -\u0026gt; B -\u0026gt; C，当 A 已被标记，且 B 也被标记时，A 就会变成黑色，不需考虑 C 是否被标记，C 是否被标记影响的是 B 能否变成黑色，与 A 无关。\n灰色：该对象已经被标记过了，但该对象下的属性没有全被标记完（GC需要从此对象中去寻找垃圾）；\n白色：该对象没有被标记过（对象垃圾）；\n在垃圾收集器开始工作时，从 GC Roots 开始进行遍历访问，访问步骤可以分为下面几步：\nGC Roots 根对象会被标记成灰色； 然后从灰色集合中获取对象，将其标记为黑色，将该对象引用到的对象标记为灰色；【只会标记一次，不会递归标记，即不会标记引用对象所引用的对象】 重复步骤2，直到没有灰色集合可以标记为止； 结束后，剩下的没有被标记的白色对象即为 GC Roots 不可达，可以进行回收。 1.2 三色标记存在的问题 多标-浮动垃圾问题 文章中已经说的比较清楚了，注意的是，因为一个引用对象被错误的标记为灰色，会导致继续遍历扫描，导致该对象所引用的对象都会被错误标记，最终导致的可能不仅仅是一个对象而是一批对象未被正确回收。\n漏标-悬挂指针问题 文章中也比较清楚的说明了。\n疑问：\n上面两个问题产生的原因在于：在垃圾回收中途，程序又改变了对象之间的引用。为什么允许程序和垃圾回收同时执行？\n","date":"2021年06月04日","permalink":"/posts/liang-wan-zi-chang-wen-dai-ni-shen-ru-go-yu-yan-gc-yuan-ma-teng-xun-ji-zhu-gong-cheng-yue-du-bi-ji/","summary":"作者：luozhiyun 博客：https://www.luozhiyun.com/archives/475 本文使用的 Go 的源码1.","title":"【两万字长文带你深入Go语言GC源码——腾讯技术工程】阅读笔记"},{"contents":" 给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（幂集）。\n解集不能包含重复的子集。你可以按 任意顺序 返回解集。\n示例 1：\n输入：nums = [1,2,3] 输出：[[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]]\n示例 2：\n输入：nums = [0] 输出：[[],[0]]\n提示：\n1 \u0026lt;= nums.length \u0026lt;= 10 -10 \u0026lt;= nums[i] \u0026lt;= 10 nums 中的所有元素 互不相同\n方法1 回溯 代码：\nclass Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; subsets(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; res; vector\u0026lt;int\u0026gt; temp; backtrack(nums, res, temp, 0); return res; } void backtrack(vector\u0026lt;int\u0026gt; \u0026amp;nums, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;res, vector\u0026lt;int\u0026gt; \u0026amp;temp, int start) { res.emplace_back(temp); for (int i = start; i \u0026lt; nums.size(); i++) { temp.emplace_back(nums[i]); // i+1 ok // start+1 !ok backtrack(nums, res, temp, i+1); temp.pop_back(); } } }; ","date":"2021年06月03日","permalink":"/posts/leetcode-78-zi-ji/","summary":"给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（幂集）。","title":"leetcode 78. 子集"},{"contents":"1. 字节序 字节序：内存中存储多个字节的方式。\n例如一个 16 进制数，它由两个字节组成，内存中存储这两个字节有两种方式，分别为大端字节序和小端 字节序。\n大端：高位字节存放在低地址，低位字节存放在高地址。\n小端：高位字节存放在高地址，低位字节存放在低地址。\n如何区分字节的高低位和地址的高低位？\n0x12 0x34 高位字节 低位字节 ← 字节增长方向 低地址 高地址 → 内存增长方向\n1.1 网络字节序和主机字节序 主机字节序：某个给定系统所用的字节序称为主机字节序。不同的操作系统采用的字节序不同，比如 macos 采用的是大端，而 linux 采用的是小端。\n网络字节序：为了在信息传输时，屏蔽掉不同硬件结构上的字节序的差异，TCP/IP协议规定，所有在网络上传输的多字节整数都以大端序编码，所以大端序就是网络字节序。\n字节序转换的函数：\n#include \u0026lt;netinet/in.h\u0026gt; // 主机字节序转网络字节序，两个函数的区别在于长度不同 // 较短的函数可以用来转换端口号，较长的可以转换 ip 地址 uint16_t htons(uint16_t host16bitvalue); uint32_t htonl(uint32_t host32bitvalue); // 网络字节序转主机字节序 uint16_t ntonhs(uint16_t net16bitvalue); uint32_t ntonhs(uint32_t net32bitvalue); 2. socket api 2.1 创建 socket #include \u0026lt;sys/socket.h\u0026gt; int socket(int family, int type, int protocol); // 返回：若成功则为非负描述符，出错则为 -1 参数描述：\nfamily 用来选择通信协议，参数主要有以下一些常用类型\nAF_UNIX, AF_LOCAL：用于本地通信，Unix Domain Socket\nAF_INET：用于 IP4\nAF_INET6：用于 IP6\ntype 指明套接字类型\nSOCK_STREAM 字节流套接字，即 tcp\nSOCK_DGRAM 数据包套接字，即 udp\nSOCK_NONBLOCK 将 socket 设置为非阻塞 SOCK_CLOEXEC fork 子进程中关闭该 socket protocol 在前两个参数构成的协议集合下，再选择一个具体的协议，通常设置为 0 即可（代表默认协议）\n2.2 命名 socket socket 命名：将一个 socket 和 socket 地址绑定。\n#include \u0026lt;sys/socket.h\u0026gt; int bind(int sockfd, const struct *myaddr, socklen_t addrlen); // 成功返回 0，出错返回 -1 使用：\nstruct sockaddr_in servaddr; bzero(\u0026amp;servaddr, sizeof(servaddr));\t// 初始化结构体 servaddr.sin_family = AF_INEF; // ip4 servaddr.sin_addr.s_addr = htonl(INADDR_ANY); // 指定 ip，并转换为网络序 servaddr.port = htons(13);\t// 指定端口号，并转换为网络序 // 第二个参数需要强转成 struct sockaddr，sockaddr 是一个通用的套接字地址结构 bind(\u0026amp;servaddr, (struct sockaddr*) \u0026amp;servaddr, sizeof(servaddr)); 2.3 监听 socket #include \u0026lt;sys/socket.h\u0026gt; int listen(int sockfd, int backlog); // 成功返回 0，出错返回 -1 ","date":"2021年05月29日","permalink":"/posts/linux-gao-xing-neng-fu-wu-qi-bian-cheng-yue-du-bi-ji-di-wu-zhang-linux-wang-luo-bian-cheng-ji-chu-api/","summary":"1. 字节序 字节序：内存中存储多个字节的方式。\n例如一个 16 进制数，它由两个字节组成，内存中存储这两个字节有两种方式，分别为大端字节序和小端 字节序。","title":"【Linux 高性能服务器编程】阅读笔记 第五章 -- Linux 网络编程基础 api"},{"contents":" 给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x， 满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5]\n示例 1:\n输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 输出: 6 解释: 节点 2 和节点 8 的最近公共祖先是 6。\n示例 2:\n输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 4 输出: 2 解释: 节点 2 和节点 4 的最近公共祖先是 2, 因为根据定义最近公共祖先节点可以为节点本身。 \u0026gt;\n说明:\n所有节点的值都是唯一的。 p、q 为不同节点且均存在于给定的二叉搜索树中。\n分析 方法类似于 二叉树的最近公共祖先，但是因为是二叉搜索树，所以难度会降低很多，只要利用搜索树的性质即可。\n方法 1 找到 p 和 q 的路径 分别找出根节点到 p 的路径和根节点到 q 的路径，再求两条路径中的最后一个相同的节点即可，在普通二叉树中寻找路径需要通过全排列的方式，但是在搜索树中就非常简单了，利用好搜索树的性质即可。\n代码如下（这道题不支持 go，所以用 c++ 实现）：\n/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ #define print(name) for (auto v : name) {cout \u0026lt;\u0026lt; v-\u0026gt;val \u0026lt;\u0026lt; \u0026quot; \u0026quot;;}; cout \u0026lt;\u0026lt; endl; class Solution { public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { vector\u0026lt;TreeNode*\u0026gt; pv, qv; get_path(pv, root, p); get_path(qv, root, q); //print(pv); //print(qv); TreeNode *res = find_last_same(pv, qv); return res; } void get_path(vector\u0026lt;TreeNode*\u0026gt; \u0026amp;vec, TreeNode* root, TreeNode* need) { if (root == NULL) { return; } TreeNode *cur = root; for (;;) { //cout \u0026lt;\u0026lt; cur-\u0026gt;val \u0026lt;\u0026lt; p-\u0026gt;val \u0026lt;\u0026lt; endl; if (need-\u0026gt;val \u0026gt; cur-\u0026gt;val) { vec.emplace_back(cur); cur = cur-\u0026gt;right; } else if (need-\u0026gt;val \u0026lt; cur-\u0026gt;val) { vec.emplace_back(cur); cur = cur-\u0026gt;left; } else { vec.emplace_back(cur); break; } } } TreeNode* find_last_same(vector\u0026lt;TreeNode*\u0026gt; \u0026amp;v1, vector\u0026lt;TreeNode*\u0026gt; \u0026amp;v2) { int m = min(v1.size(), v2.size()); TreeNode* res; for (int i = 0; i \u0026lt; m; i++) { if (v1.at(i)-\u0026gt;val == v2.at(i)-\u0026gt;val) { res = v1.at(i); } } return res; } }; 这种方法虽然可以通过，但是较为繁琐，还有更简洁的写法。\n方法 2 迭代 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { for (;;) { if (p-\u0026gt;val \u0026gt; root-\u0026gt;val \u0026amp;\u0026amp; q-\u0026gt;val \u0026gt; root-\u0026gt;val) { root = root-\u0026gt;right; } else if (p-\u0026gt;val \u0026lt; root-\u0026gt;val \u0026amp;\u0026amp; q-\u0026gt;val \u0026lt; root-\u0026gt;val) { root = root-\u0026gt;left; } else { break; } } return root; } }; 这种方法就和 二叉树的最近公共祖先 类似了，通过判断 p 和 q 在同侧还是异侧，来决定父节点的值，具体可以参考这篇文章\n方法3 递归 方法和迭代一样，只是换成了递归的方式，代码如下：\n/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { if (p-\u0026gt;val \u0026gt; root-\u0026gt;val \u0026amp;\u0026amp; q-\u0026gt;val \u0026gt; root-\u0026gt;val) { return lowestCommonAncestor(root-\u0026gt;right, p, q); } else if (p-\u0026gt;val \u0026lt; root-\u0026gt;val \u0026amp;\u0026amp; q-\u0026gt;val \u0026lt; root-\u0026gt;val) { return lowestCommonAncestor(root-\u0026gt;left, p, q); } else { return root; } return NULL; } }; ","date":"2021年05月09日","permalink":"/posts/jian-zhi-offer-68-i-er-cha-sou-suo-shu-de-zui-jin-gong-gong-zu-xian/","summary":"给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x， 满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”","title":"剑指 Offer 68 - I. 二叉搜索树的最近公共祖先"},{"contents":" 输入一个正整数 target ，输出所有和为 target 的连续正整数序列（至少含有两个数）。\n序列内的数字由小到大排列，不同序列按照首个数字从小到大排列。\n示例 1：\n输入：target = 9 输出：[[2,3,4],[4,5]]\n示例 2：\n输入：target = 15 输出：[[1,2,3,4,5],[4,5,6],[7,8]]\n限制：\n1 \u0026lt;= target \u0026lt;= 10^5\n方法 1 滑动窗口 通过双指针维护一个窗口，并根据窗口中的和值，来进行相应的移动处理，具体为下：\n和值小于所需的值，扩大窗口，操作为右指针 + 1 和值大于所需的值，缩小窗口，操作为左指针 + 1 和值等于所需的值，记录结果，左右指针同时 + 1 流程如下图所示：\n代码如下：\nfunc findContinuousSequence(target int) [][]int { var res [][]int // 构建一个数组用于滑动窗口 n := make([]int, (target\u0026gt;\u0026gt;1)+1) for i := 0; i \u0026lt; len(n); i++ { n[i] = i + 1 } i, j := 0, 1 sum := n[i] + n[j] for i \u0026lt; j { if sum \u0026lt; target { j++ sum += n[j] } else if sum \u0026gt; target { sum -= n[i] i++ } else if sum == target { var temp []int for i1 := i; i1 \u0026lt;= j; i1++ { temp = append(temp, n[i1]) } res = append(res, temp) sum -= n[i] i++ // j 不是最后一个元素时，才执行下面的操作， // 否则 j++ 后会导致 n[j] 越界 if j \u0026lt; len(n)-1 { j++ sum += n[j] } } } return res } 这里有以下几点需要注意：\n如果需要移动 j 指针，则更新和值操作需要在移动后进行：\nj++ sum += n[j] 如果需要移动 i 指针，需要在移动前更新和值：\nsum -= n[i] i++ sum == target 时 防止 j 越界\n// j 不是最后一个元素时，才执行下面的操作， // 否则 j++ 后会导致 n[j] 越界 if j \u0026lt; len(n)-1 { j++ sum += n[j] } 优化一下 虽然结果是正确的，但是在参考了题解后，发现自己的代码还有一些可以优化的地方，首先是最开始的构建数组，这是完全没必要的，因为是连续正整数序列，所以可以直接把指针作为数字，而不是下标。\n此外，对于 sum == target 这种情况，我的处理逻辑是 i 和 j 都移动，但是实际上只需要移动 i 即可，这样可以省去越界判断，且可接减少移动次数。\n减少移动次数示例： i 和 j 都移动\ni = 4, j = 5, s = 9 == target i++ = 5, j++ = 6, sum= 11 \u0026gt; 9 i++ = 6，i == j ，break 只移动 i\ni = 4, j = 5, s = 9 == target i =5, j = 5, i == j ，break 代码如下：\nfunc findContinuousSequence(target int) [][]int { var res [][]int i, j, sum := 1, 2, 3 for i \u0026lt; j { if sum \u0026lt; target { j++ sum += j } else if sum \u0026gt; target { sum -= i i++ } else if sum == target { var temp []int for i1 := i; i1 \u0026lt;= j; i1++ { temp = append(temp, i1) } res = append(res, temp) sum -= i i++ } } return res } ","date":"2021年05月04日","permalink":"/posts/jian-zhi-offer-57-ii-he-wei-s-de-lian-xu-zheng-shu-xu-lie/","summary":"输入一个正整数 target ，输出所有和为 target 的连续正整数序列（至少含有两个数）。","title":"剑指 Offer 57 - II. 和为s的连续正数序列"},{"contents":" 给定一个保存员工信息的数据结构，它包含了员工 唯一的 id ，重要度 和 直系下属的 id\n比如，员工 1 是员工 2 的领导，员工 2 是员工 3 的领导。他们相应的重要度为 15 , 10 , 5 。那么 员工 1 的数据结构是 [1, 15, [2]] ，员工 2的 数据结构是 [2, 10, [3]] ，员工 3 的数据结构是 [3, 5, []] 。注意虽然员工 3 也是员工 1 的一个下属，但是由于 并不是直系 下属，因此没有体现在 员工 1 的数据结构中。\n现在输入一个公司的所有员工信息，以及单个员工 id ，返回这个员工和他所有下属的重要度之和。\n示例：\n输入：[[1, 5, [2, 3]], [2, 3, []], [3, 3, []]], 1 输出：11 解释： 员工 1 自身的重要度是 5 ，他有两个直系下属 2 和 3 ，而且 2 和 3 的重要度均为 3 。因此员工 1 的总重要度是 5 + 3 + 3 = 11 。\n提示：\n一个员工最多有一个 直系 领导，但是可以有多个 直系 下属 员工数量不超过 2000 。\n方法1 普通递归 /** * Definition for Employee. * type Employee struct { * Id int * Importance int * Subordinates []int * } */ func getImportance(employees []*Employee, id int) int { var res int dfs(employees, id, \u0026amp;res) return res } func dfs(emp []*Employee, id int, res *int) { for _, v := range emp { if v.Id == id { *res += v.Importance for _, s := range v.Subordinates { dfs(emp, s, res) } } } } 执行用时: 8 ms 内存消耗: 6.6 MB 方法2 递归 + 哈希表 通过哈希表可以在 O(1) 内通过 id 查找到对象，而无需遍历数组。\n/** * Definition for Employee. * type Employee struct { * Id int * Importance int * Subordinates []int * } */ func getImportance(employees []*Employee, id int) int { m := make(map[int]*Employee) var res int for _, v := range employees { m[v.Id] = v } dfs(m, id, \u0026amp;res) return res } func dfs(m map[int]*Employee, id int, res *int) { e := m[id] *res += e.Importance for _, v := range e.Subordinates { dfs(m, v, res) } } 执行用时: 16 ms 内存消耗: 6.8 MB 为什么反而更慢了。。。\n方法3 迭代 通过队列 + 哈希表来实现。\n/** * Definition for Employee. * type Employee struct { * Id int * Importance int * Subordinates []int * } */ func getImportance(employees []*Employee, id int) int { m := make(map[int]*Employee) for _, e := range employees { m[e.Id] = e } var res int queue := list.New() queue.PushBack(m[id]) for queue.Len() \u0026gt; 0 { pop := queue.Remove(queue.Front()).(*Employee) res += pop.Importance for _, e := range pop.Subordinates { queue.PushBack(m[e]) } } return res } 执行用时: 12 ms 内存消耗: 6.7 MB ","date":"2021年05月02日","permalink":"/posts/leetcode-690-yuan-gong-de-chong-yao-xing/","summary":"给定一个保存员工信息的数据结构，它包含了员工 唯一的 id ，重要度 和 直系下属的 id","title":"LeetCode 690. 员工的重要性"},{"contents":"Parallels Desktop 16 在最新的macOS Big Sur 11.0系统上无法联网，并且无法连接USB设备。之前解决联网的办法是在终端通过命令启动parallels desktop的方法解决联网的问题，但是相对比较麻烦，而且还是无法解决Parallels Desktop 16 不能连接USB设备的问题。\n今天小编为大家提供一个更好的方法解决Parallels Desktop 16 不能联网与连接USB设备的问题。详细操作步骤如下：\n1、打开访达，按下shift+command+G 三个键，前往文件夹：/Library/Preferences/Parallels ；\n2、下载 Sublime Text 打开文件 network.desktop.xml ，找到第5行的 -1 （也可能是 1 ），修改为 0 保存并退出，保存时会提示输入密码，输入系统密码确定即可；\n3、用 Sublime Text 打开文件 dispatcher.desktop.xml ，按 command + F 查找 0 ，修改为 1 保存并退出，保存时会提示输入密码，输入系统密码确定即可；\n4、然后打开Parallels Desktop 16，可能会提示 Parallels需要系统扩展 ，打开 系统偏好设置 ，进入 安全性与隐私 ，点击左下角的锁图标解锁，在点击下方【 来自开发者“Parallels International GmbH”的系统软件已被阻止载入。 】右侧的 允许 按钮，提示需要重启，点击确定重启电脑，再打开Parallels Desktop 16进入Windows系统即可正常上网和连接USB设备了。\n———————————————— 版权声明：本文为CSDN博主「blank_t」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/blank_t/article/details/111567618\n","date":"2021年05月01日","permalink":"/posts/parallels-desktop-16-wang-luo-chu-shi-hua-shi-bai-he-bu-neng-lian-jie-usb-she-bei-jie-jue-fang-fa/","summary":"Parallels Desktop 16 在最新的macOS Big Sur 11.","title":"Parallels Desktop 16 网络初始化失败和不能连接USB设备解决方法"},{"contents":"错误1： func test() error { if err := func1(); err != nil { // func1() 会产生错误 return err } defer func() { fmt.Println(\u0026quot;defer func\u0026quot;) } } 上面的函数不会输出 ”defer func“，因为压根就没有执行到 defer 处，导致 defer 没有注册（或者说是压入栈中？还不了解 defer 的原理）\n正确写法：\nfunc test() error { defer func() { fmt.Println(\u0026quot;defer func\u0026quot;) } if err := func1(); err != nil { // func1() 会产生错误 return err } } 把 defer 定义在前面就好了，这样在一开始就会将 defer 函数注册。 看来还是得多写写项目才能发现自己的问题。\n","date":"2021年04月23日","permalink":"/posts/goji-yi-ci-defer-cuo-wu/","summary":"错误1： func test() error { if err := func1(); err !","title":"go：记一次 defer 错误"},{"contents":"有如下 c++ 代码，它的功能是删除容器中所有小于 10 的元素：\nlist\u0026lt;int\u0026gt; l{1, 2, 3, 4, 12, 13, 15}; int xx = 10; for (auto f = l.front(); f \u0026lt; xx \u0026amp;\u0026amp; !l.empty(); ) { l.pop_front(); } for_each(l.begin(), l.end(), [](int v) -\u0026gt; void { cout \u0026lt;\u0026lt; v \u0026lt;\u0026lt; \u0026quot; \u0026quot;; }); cout \u0026lt;\u0026lt; endl; 这段代码的逻辑是：首先获取 list 的第一个元素的值，用变量 f 保存，再用 f 和 变量 xx 比较，如果 f \u0026lt; xx 且 list 不为空，则移除 list 的第一个元素。\n正确的结果应该是 [12, 13, 15]，但是运行上面的代码，结果却为空，容器里的所有元素都被删掉了。\n将上面的 for 改成下面的代码后，结果正确\nfor (; l.front() \u0026lt; xx \u0026amp;\u0026amp; !l.empty(); ) { l.pop_front(); } 原因出在 auto f = l.front() 这里了，打印一下该值：\nlist\u0026lt;int\u0026gt; l{1, 2, 3, 4, 12, 13, 15}; int xx = 10; for (auto f = l.front(); f \u0026lt; xx \u0026amp;\u0026amp; !l.empty(); ) { l.pop_front(); } 输出：1 1 1 1 1 1 1\n我希望的是每次 for 都会更新 f 的值，因为执行了 pop_front() 后，l.front() 也随之改变，所以用来保存 front() 的变量 f 也需要改变，但这恰好说明了我的基础不牢固，对 for 循环的理解不到位。\n参见菜鸟教程中对 for 循环的讲解：（https://www.runoob.com/cplusplus/cpp-for-loop.html）\nfor ( init; condition; increment ) { statement(s); } init 会首先被执行，且只会执行一次。这一步允许您声明并初始化任何循环控制变量。您也可以不在这里 写任何语句，只要有一个分号出现即可。\nfor 循环的第一个语句只会执行一次，上面的运行结果可以证明这一点，有这一句话就可以解释上面的所有问题了。\n","date":"2021年04月23日","permalink":"/posts/wei-shi-me-bu-neng-yong-bian-liang-bao-cun-die-dai-qi/","summary":"有如下 c++ 代码，它的功能是删除容器中所有小于 10 的元素：","title":"再探 for 循环"},{"contents":" 写一个函数 StrToInt，实现把字符串转换成整数这个功能。不能使用 atoi 或者其他类似的库函数。\n首先，该函数会根据需要丢弃无用的开头空格字符，直到寻找到第一个非空格的字符为止。\n当我们寻找到的第一个非空字符为正或者负号时，则将该符号与之后面尽可能多的连续数字组合起来，作为 该整数的正负号；假如第一个非空字符是数字，则直接将其与之后连续的数字字符组合起来，形成整数。\n该字符串除了有效的整数部分之后也可能会存在多余的字符，这些字符可以被忽略，它们对于函数不应该造 成影响。\n注意：假如该字符串中的第一个非空格字符不是一个有效整数字符、字符串为空或字符串仅包含空白字符 时，则你的函数不需要进行转换。\n在任何情况下，若函数不能进行有效的转换时，请返回 0。\n说明：\n假设我们的环境只能存储 32 位大小的有符号整数，那么其数值范围为 [−231, 231 − 1]。如果数值超 过这个范围，请返回 INT_MAX (231 − 1) 或 INT_MIN (−231) 。\n示例 1:\n输入: \u0026ldquo;42\u0026rdquo; 输出: 42\n示例 2:\n输入: \u0026quot; -42\u0026quot; 输出: -42 解释: 第一个非空白字符为 \u0026lsquo;-\u0026rsquo;, 它是一个负号。 我们尽可能将负号与后面所有连续出现的数字组合起来，最后得到 -42 。\n示例 3:\n输入: \u0026ldquo;4193 with words\u0026rdquo; 输出: 4193 解释: 转换截止于数字 \u0026lsquo;3\u0026rsquo; ，因为它的下一个字符不为数字。\n示例 4:\n输入: \u0026ldquo;words and 987\u0026rdquo; 输出: 0 解释: 第一个非空字符是 \u0026lsquo;w\u0026rsquo;, 但它不是数字或正、负号。 因此无法执行有效的转换。\n示例 5:\n输入: \u0026ldquo;-91283472332\u0026rdquo; 输出: -2147483648 解释: 数字 \u0026ldquo;-91283472332\u0026rdquo; 超过 32 位有符号整数范围。 因此返回 INT_MIN (−231) 。\n分析 这道题乍一看好像没什么特殊之处，不就是一个 atoi 嘛，只是这个 28% 的通过率有些不对劲，等到自己写的时候才发现，这道题太 tm 恶心了，有各种各样的特殊情况，比如：\n” +1“ 空格后面紧跟一个符号 ” +-1“ 两个符号在一起 ” +0 123“ 两个数字之间有空格 ”0-1“ 两个数字之间右符号 还有溢出的问题，如果小于 minInt32，则返回 minInt32，maxInt32 同理 这些条件会导致写出来的代码非常繁琐，而且非常容易出错，往往是解决了一种情况，又导致另一种情况错误，有点像边修 bug 边造 bug。\n方法1 自动机 自动机表示如下：\nstart signed in_number end start start signed in_number end signed end end in_number end in_number end end in_number end end end end end end 解释： start：代表空格 signed：代表 “+” 或者 “-” in_number：代表数字 end：代表其他字符\n当 start 遇到 start 时：代表空格遇到空格，状态依然为 start，例如 “ ” 当 start 遇到 signed 时：代表空格遇到 + 或 -，状态改变为 signed，例如 “ +” 当 start 遇到 in_number 时：代表空格遇到数字，状态改变为 in_number，例如 “ 1” 当 start 遇到 end 时：代表空格遇到其他字符，状态改变为 end，例如 “ a”\n当 signed 遇到 start 时：代表符号遇到空格，例如 “+ ”，此时为非法，状态改为 end 当 signed 遇到 signed 时：代表符号遇到符号：例如 “++”，此时为非法，状态改为 end 当 signed 遇到 in_number 时，代表符号遇到数字，例如 “+1”，合法且为一个数字，状态改为 in_number 当 signed 遇到 end 时，代表符号遇到其他字符，例如 “+a”，此时为非法，状态改为 end\n下面为简写，x -\u0026gt; y 表示为 x 遇到 y，\u0026quot; \u0026quot; 为示例，stat 为更新后的状态\nin_number -\u0026gt; start：\u0026ldquo;1 \u0026ldquo;，stat = end in_number -\u0026gt; signed：\u0026ldquo;1+\u0026quot;，stat = end in_number -\u0026gt; in_number：\u0026ldquo;12\u0026rdquo;，stat = in_number in_number -\u0026gt; end：\u0026ldquo;1a\u0026rdquo;，stat = end\nend -\u0026gt; start：\u0026ldquo;a \u0026ldquo;，stat = end end -\u0026gt; signed：\u0026ldquo;a+\u0026quot;，stat = end end -\u0026gt; in_number：\u0026ldquo;a1\u0026rdquo;，stat = end end -\u0026gt; end：\u0026ldquo;ab\u0026rdquo;，stat = end\n理解了自动机以后，也就有了一些写代码的思路了，不得不说这种方法真的十分巧妙，能够轻松化解各种繁杂的 if else 条件和边界判断。\n代码如下：\n// 对应之前的自动机图表 var stat = map[string][]string{ \u0026quot;start\u0026quot;: {\u0026quot;start\u0026quot;, \u0026quot;sign\u0026quot;, \u0026quot;num\u0026quot;, \u0026quot;end\u0026quot;}, \u0026quot;sign\u0026quot;: {\u0026quot;end\u0026quot;, \u0026quot;end\u0026quot;, \u0026quot;num\u0026quot;, \u0026quot;end\u0026quot;}, \u0026quot;num\u0026quot;: {\u0026quot;end\u0026quot;, \u0026quot;end\u0026quot;, \u0026quot;num\u0026quot;, \u0026quot;end\u0026quot;}, \u0026quot;end\u0026quot;: {\u0026quot;end\u0026quot;, \u0026quot;end\u0026quot;, \u0026quot;end\u0026quot;, \u0026quot;end\u0026quot;}, } // 对应自动机图表的表头 func getStat(c rune) int { if unicode.IsSpace(c) { return 0 } else if c == '+' || c == '-' { return 1 } else if unicode.IsDigit(c) { return 2 } else { return 3 } } func myAtoi(s string) int { is := \u0026quot;start\u0026quot; sign := 1 var res int for _, c := range s { is = stat[is][getStat(c)] if is == \u0026quot;num\u0026quot; { res = res*10 + int(c-'0') // math.MinInt32: -2147483648 // math.MaxInt32: 2147483647 switch sign { case 1: res = min(res, math.MaxInt32) case -1: // min(res, math.MaxInt32) // 输入：\u0026quot;-91283472332\u0026quot; // 输出：-2147483647 // 预期：-2147483648 // -math.MinInt32 ???? res = min(res, -math.MinInt32) } } if is == \u0026quot;sign\u0026quot; { if c == '+' { sign = 1 } else { sign = -1 } } } return res * sign } func min(x, y int) int { if x \u0026gt; y { return y } return x } 这里要说明一下 sign = -1 时的溢出判断 res = min(res, -math.MinInt32) ，为什么要加负号呢？以测试用例 -91283472332 为例，正确的结果为 -2147483648：\n(注：math.MinInt32: -2147483648 math.MaxInt32: 2147483647)\n因为有负号，所以 sign 的值更新为 -1，而最后返回的的结果为 res * sign，即 - res，所以需要为 math.MinInt32 添加一个符号，将其变为 2147483648，这样再与 sign 相乘，得到的就是一个负数了。\n此外，如果条件为 min(res, math.MinInt32)，那么当 res 为 -42 时，并没有溢出，但是会错误的返回 MinInt32，如果改为 -math.MinInt32 则会正确的返回 -42 了。\n","date":"2021年04月21日","permalink":"/posts/jian-zhi-offer-67-ba-zi-fu-chuan-zhuan-huan-cheng-zheng-shu/","summary":"写一个函数 StrToInt，实现把字符串转换成整数这个功能。不能使用 atoi 或者其他类似的库函数。","title":"剑指 Offer 67. 把字符串转换成整数"},{"contents":"题目描述 给定一个经过编码的字符串，返回它解码后的字符串。\n编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。\n你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。\n此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。\n示例 1：\n输入：s = \u0026ldquo;3[a]2[bc]\u0026rdquo; 输出：\u0026ldquo;aaabcbc\u0026rdquo;\n示例 2：\n输入：s = \u0026ldquo;3[a2[c]]\u0026rdquo; 输出：\u0026ldquo;accaccacc\u0026rdquo;\n示例 3：\n输入：s = \u0026ldquo;2[abc]3[cd]ef\u0026rdquo; 输出：\u0026ldquo;abcabccdcdcdef\u0026rdquo;\n示例 4：\n输入：s = \u0026ldquo;abc3[cd]xyz\u0026rdquo; 输出：\u0026ldquo;abccdcdcdxyz\u0026rdquo;\n方法1 两个辅助栈 代码如下：\nfunc decodeString(s string) string { resStack := list.New() culStack := list.New() mul, res := 0, \u0026quot;\u0026quot; for _, char := range s { c := byte(char) if c == '[' { resStack.PushBack(res) culStack.PushBack(mul) mul = 0 res = \u0026quot;\u0026quot; } else if c == ']' { var temp string cpop := culStack.Remove(culStack.Back()).(int) for i := 0; i \u0026lt; cpop; i++ { temp += res } rpop := resStack.Remove(resStack.Back()).(string) res = rpop + temp } else if c \u0026gt;= '0' \u0026amp;\u0026amp; c \u0026lt;= '9' { mul = mul*10 + int(c-'0') } else { res += string(c) } } return res } ","date":"2021年04月19日","permalink":"/posts/leetcode-394-zi-fu-chuan-jie-ma/","summary":"题目描述 给定一个经过编码的字符串，返回它解码后的字符串。\n编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。","title":"leetcode 394. 字符串解码"},{"contents":"有如下代码，先使用 io.Copy 将 request.Body copy 到 stdout 标准输出，再使用 io.ReadAll 再次读取 request.Body：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;io\u0026quot; \u0026quot;os\u0026quot; ) func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { io.WriteString(os.Stdout, \u0026quot;read request body from io.Copy: \u0026quot;) io.Copy(os.Stdout, r.Body) io.WriteString(os.Stdout, \u0026quot;\\n\u0026quot;) body, err := io.ReadAll(r.Body) if err != nil { fmt.Println(\u0026quot;read request body error: \u0026quot;, err) return } fmt.Println(\u0026quot;read request body from io.ReadAll: \u0026quot;, body) }) if err := http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil); err != nil { panic(err) } } 使用 curl 访问接口：\n$ curl localhost:8080 -d \u0026quot;user=admin\u0026amp;passwd=12345678\u0026quot; # http server 这边的输出： $ go run http_readcloser.go read request body from io.Copy: user=admin\u0026amp;passwd=12345678 read request body from io.ReadAll: [] 执行时发现 io.ReadAll() 读出来的内容始终为空。\n原因： 【猜测】copy() 会将 src 清空而不是 copy，这和它的函数名不符，\ncopy 函数没有问题，对于这种标准库的东西也不可能犯这种低级错误，真正的原因在于 http.Request.Body。\nhttp.request.Body 是 readCloser 类型的，我们使用 ReadAll 读取 http.request.Body 后将无法再次读取 http.request.Body 里面的信息\n解决方法 var bodyB []byte bodyB, _ = io.ReadAll(r.Body) fmt.Println(bodyB) // 把刚刚读出来的再写进去 r.Body = io.NopCloser(bytes.NewReader(bodyB)) all, _ := io.ReadAll(r.Body) fmt.Println(all) 为什么 Request.Body 只能读取一次（未解决） NopCloser() 该函数将一个 reader 类型包装成 readCloser 类型，而相应的 Close 方法啥也不做，只是返回 nil。\n源码：\n// NopCloser returns a ReadCloser with a no-op Close method wrapping // the provided Reader r. func NopCloser(r Reader) ReadCloser { return nopCloser{r} } type nopCloser struct { Reader } ","date":"2021年04月17日","permalink":"/posts/goiocopy-de-keng/","summary":"有如下代码，先使用 io.Copy 将 request.Body copy 到 stdout 标准输出，再使用 io.","title":"go：http.Request.Body 第二次读取为空 "},{"contents":"情景 某天突发奇想，写了一个在线阅读的 http 小程序，当访问某一路由时会读取电子书文件，并写入到 response 中，其中封装了一个读取文件的函数：\nfunc ReadFile(src string) ([]byte, error) { file, err := os.Open(src) if err != nil { return nil, err } b, err := io.ReadAll(file) if err != nil { return nil, err } defer file.Close() return b, nil } 很简单的一个函数，使用 io.ReadAll() （在 go 1.16 中废弃了 ioutil包，将其中的函数全部移到了 io 包下）读取文件为字节切片并返回，这里读取的文件是电子书。\n但是当部署到服务器上（1核 2g 的学生服务器）并访问路由时，产生了 out of memory 异常，致使程序中指，这是一个我还未曾见过的错误，同时也比较诧异，这样一个简单的小程序为什么会内存溢出呢？\n从错误的提示信息中，定位到了问题的所在为：io.ReadAll()，早就听说过这个函数是一次性将文件全部读取的，在文件较大时不建议使用（go 语言圣经中有提到），但也没太当回事，毕竟这个所谓的“文件较大”过于宽泛，但经过这次的错误后，我对文件较大这个概念有了新的认识，在我这台土豆服务器上，读取一个 300m 的文件会直接导致内存溢出，服务器的内存使用情况：KiB Mem : 1882892 total, 70112 free, 1347204 used ，也就是说使用 ReadAll() 读取一个 300m 的文件，会占用超过 700m 的内存。\n解决方法 将 ReadAll() 替换为 io.Copy() 即可\n","date":"2021年04月16日","permalink":"/posts/goioutilreadall-yin-fa-de-out-of-memory/","summary":"情景 某天突发奇想，写了一个在线阅读的 http 小程序，当访问某一路由时会读取电子书文件，并写入到 response 中，其中封装了一个读取文件的函数：","title":"go：ioutil.ReadAll() 引发的 out of memory "},{"contents":"在做 压缩字符串 这道题时，提交的输出结果十分诡异，如下：\n输入：\n[\u0026ldquo;a\u0026rdquo;,\u0026ldquo;a\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;c\u0026rdquo;,\u0026ldquo;c\u0026rdquo;,\u0026ldquo;c\u0026rdquo;]\n输出：\n[\u0026ldquo;a\u0026rdquo;,\u0026quot;\\u0002\u0026quot;,\u0026ldquo;b\u0026rdquo;,\u0026quot;\\u0002\u0026quot;,\u0026ldquo;c\u0026rdquo;,\u0026quot;\\u0003\u0026quot;]\n预期：\n[\u0026ldquo;a\u0026rdquo;,\u0026ldquo;2\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;2\u0026rdquo;,\u0026ldquo;c\u0026rdquo;,\u0026ldquo;3\u0026rdquo;]\n可以看到结果中的数字部分是对的，但是多了 \\u000 这部分，这个结果令我百思不得其解，在 ide 中运行时正常的，但在力扣上运行一直都会有 \\u000 ，起初还以为是力扣的问题，在查阅资料以后终于找到了原因所在，是一个比较低级的错误。\n大概是力扣会将运行的字节数组转换为字符串，而字符数组转字符串会根据 ascii 来映射转换，问题来了，我的字节数组里存储的全部都是 1 - 9 的数字，而不是数字对应的 ascii 码，所以在转换后无法输出有效字符，解决的方法也很简单，只要在存储的值上加 \u0026lsquo;0\u0026rsquo; 即可（ascii 为48）\n比如，原先的字节数组是 [2, 2, 3]，再加 48 以后，变成了 [50, 50, 51]，ascii 50 对应十进制 2，ascii 51 对应十进制 3，此时转换后的结果就是正确的了。\n","date":"2021年04月16日","permalink":"/posts/2021-3-13-byte-array/","summary":"在做 压缩字符串 这道题时，提交的输出结果十分诡异，如下：\n输入：\n[\u0026ldquo;a\u0026rdquo;,\u0026ldquo;a\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;b\u0026rdquo;,\u0026ldquo;c\u0026rdquo;,\u0026ldquo;c\u0026rdquo;,\u0026ldquo;c\u0026rdquo;]","title":"go 字节数组错误：\\u0001"},{"contents":"背景：\n本地 MacOS 上通过 GDB 调试 golang 程序，结果提示 No symbol table is loaded. Use the \u0026ldquo;file\u0026rdquo; command.\n解决方法：\n打包时加上 -ldflags=-compressdwarf=false 参数即可 比如在作者本地就是 go build -gcflags \u0026ldquo;-N -l\u0026rdquo; -ldflags=-compressdwarf=false gdb/main.go 然后通过命令 gdb main 即可调试\n参考：https://kaijuan.co/topics/25/no-symbol-table-is-loaded-use-the-file-command\n","date":"2021年04月16日","permalink":"/posts/2021-4-10-golang-gdb/","summary":"背景：\n本地 MacOS 上通过 GDB 调试 golang 程序，结果提示 No symbol table is loaded.","title":"Golang GDB 调试提示 No symbol table is loaded. Use the \"file\" command"},{"contents":"情景描述 有如下 c++ 代码：\nauto b = vec.begin(); *b = 99999; for (int \u0026amp;i : vec) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; // Output: 99999 2 3 4 5 } cout \u0026lt;\u0026lt; endl; auto bb = *vec.begin(); cout \u0026lt;\u0026lt; \u0026quot;bb: \u0026quot; \u0026lt;\u0026lt; bb \u0026lt;\u0026lt; endl; bb = 5; for (int \u0026amp; i : vec) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; // Output: 99999 2 3 4 5 } cout \u0026lt;\u0026lt; endl; 通过 *b 的方式可以更改元素的值，而 bb 则不行。这里我比较疑惑的是，*b 和 *vec.begin() 难道不一样吗？\n通过编译器查看得知，b 的类型是 __wrap_iter\u0026lt;vector\u0026lt;int, allocator\u0026lt;_Tp\u0026gt;\u0026gt;::pointer\u0026gt;，bb 的类型是 int，这让我有了一些思路。\n举一个指针的例子：\nint i = 5; int *p = \u0026amp;i; // 类似上面的 b int *b = p; *b = 555; cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; // Output: 555 // 修改成功 // 类似上面的 bb int bb = *p; bb = 555; cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; // Output: 5 // 未修改 b 保存的是 p 指向的地址，也就是 i，所以通过解引用 *b 的方式可以更改掉 i 的值，而 bb 保存的是 p 解引用的值，仅仅是一个 int 变量，而不是地址，自然也不会对 i 产生任何影响。\n说明 鉴于还处于初学 c++ 的阶段，对迭代器以及语言本身都不熟悉，所以以上仅仅是猜想，本文仅做保存记录，方便以后的回看。\n","date":"2021年04月16日","permalink":"/posts/2021-4-14-whats-different-in-cpp-iterator-var/","summary":"情景描述 有如下 c++ 代码：\nauto b = vec.","title":"c++：iterator.begin() 和 *iterator.begin() 的区别 【未完】"},{"contents":"etcd 远程操作失败：Error: context deadline exceeded 在服务器启动 etcd，并在本地机器通过 etcdctl \u0026ndash;endpoints=http://ip:2379 put name \u0026ldquo;123\u0026rdquo; 命令，在服务器的 etcd 中创建一对键值对， 如下图所示：\n服务端 etcd 的启动命令如下：\n但是在执行 etcdctl 命令后发生了错误：\n经过网上查阅资料，终于找到了解决方法，在启动 etcd 时，需要添加一些参数，否则远程连接无法执行，\n完整命令如下：\n$ ./etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://0.0.0.0:2379 --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://0.0.0.0:2380 --initial-cluster my-etcd-1=http://0.0.0.0:2380 将ip为0.0.0.0可以理解为不限制连接机器（真正的生产不推荐这样设置）。\n使用上面的命令启动 etcd 后，本地机就可以正常的对其执行操作了\n参考：https://github.com/yuedun/micro-service/blob/master/README.md\n","date":"2021年04月16日","permalink":"/posts/2020-10-17-etcd-error/","summary":"etcd 远程操作失败：Error: context deadline exceeded 在服务器启动 etcd，并在本地机器通过 etcdctl \u0026ndash;endpoints=http://ip:2379 put name \u0026ldquo;123\u0026rdquo; 命令，在服务器的 etcd 中创建一对键值对， 如下图所示：","title":"etcd 错误：context deadline exceeded"},{"contents":"题目描述 字符串压缩。利用字符重复出现的次数，编写一种方法，实现基本的字符串压缩功能。比如，字符串aabcccccaaa会变为a2b1c5a3。若“压缩”后的字符串没有变短，则返回原先的字符串。你可以假设字符串中只包含大小写英文字母（a至z）。\n示例1:\n输入：\u0026ldquo;aabcccccaaa\u0026rdquo; 输出：\u0026ldquo;a2b1c5a3\u0026rdquo; 示例2:\n输入：\u0026ldquo;abbccd\u0026rdquo; 输出：\u0026ldquo;abbccd\u0026rdquo; 解释：\u0026ldquo;abbccd\u0026quot;压缩后为\u0026quot;a1b2c2d1\u0026rdquo;，比原字符串长度更长。 提示：\n字符串长度在[0, 50000]范围内。\n双指针 代码\nfunc compressString(S string) string { var sb strings.Builder p, l := 0, 1 // l 记录长度，初始值为 1 var cur byte // 指针 p 从 0 开始，i 从 1 开始 for i := 1; i \u0026lt; len(S); i++ { cur = S[p] if S[i] == cur { l++ } else { sb.WriteByte(cur) sb.WriteString(strconv.Itoa(l)) l = 1 // 重置 l } if i == len(S) - 1 { // 到最后一个 char 了 sb.WriteByte(S[i]) sb.WriteString(strconv.Itoa(l)) } p++ } // 压缩后没有变短则返回原先的字符串 // 长度为 1 的也直接返回 if sb.Len() \u0026gt;= len(S) || len(S) \u0026lt;= 1 { return S } return sb.String() } ","date":"2021年04月16日","permalink":"/posts/2021-3-13-ctcl-0106-compress-string/","summary":"题目描述 字符串压缩。利用字符重复出现的次数，编写一种方法，实现基本的字符串压缩功能。比如，字符串aabcccccaaa会变为a2b1c5a3。若“压缩”后的字符串没有变短，则返回原先的字符串。你可以假设字符串中只包含大小写英文字母（a至z）。\n示例1:\n输入：\u0026ldquo;aabcccccaaa\u0026rdquo; 输出：\u0026ldquo;a2b1c5a3\u0026rdquo; 示例2:","title":"面试题 01.06. 字符串压缩"},{"contents":"题目描述 统计一个数字在排序数组中出现的次数。\n示例 1:\n输入: nums = [5,7,7,8,8,10], target = 8 输出: 2 示例 2:\n输入: nums = [5,7,7,8,8,10], target = 6 输出: 0\n限制：\n0 \u0026lt;= 数组长度 \u0026lt;= 50000\n方法1：二分查找 这道题最简单粗暴的方法就是从头遍历，当获取到值等于 target 的元素时开始累加次数，直到当前值不等于 target，但是这样做的话时间复杂度为 O(n)，此外像这种暴力法也显然不是一个合格的解决方案。\n解决这道题的另一种方式是 二分查找，这道题属于二分查找的一个变种，常规的二分查找，在 nums[mid] == target 时返回，但是这道题要求的是寻找出 target 出现的次数，所以区别就在于 nums[mid] == target 时的处理。\n整道题的思路可以是这样：先查找出 target 最后出现的位置，记为 leftIndex，再查找出 target 第一次出现的位置，记为 rightIndex，然后用 leftIndex - rightIndex + 1 即可求出出现的次数。\n至于二分查找的特殊处理，可以将 [mid] == target 时处理为 left = mid + 1。\n具体的流程如下图：\n代码如下：\nfunc search(nums []int, target int) int { leftIndex := binarySearch(nums, target) rightIndex := binarySearch(nums, target-1) return leftIndex - rightIndex } func binarySearch(nums []int, target int) int { l, r := 0, len(nums)-1 for l \u0026lt;= r { m := (l + r) \u0026gt;\u0026gt; 1 if nums[m] \u0026lt;= target { l = m + 1 } else { r = m - 1 } } return l } ","date":"2021年04月16日","permalink":"/posts/2021-3-17-offer-53-search-sort-array1/","summary":"题目描述 统计一个数字在排序数组中出现的次数。\n示例 1:\n输入: nums = [5,7,7,8,8,10], target = 8 输出: 2 示例 2:","title":"剑指 Offer 53 - I. 在排序数组中查找数字 I"},{"contents":"题目描述 输入两个链表，找出它们的第一个公共节点。\t编写一个程序，找到两个单链表相交的起始节点。\n如下面的两个链表：\na1 -\u0026gt; a2 ↘ c1 -\u0026gt; c2 -\u0026gt; c3 b1 -\u0026gt; b2 -\u0026gt; b3 ↗ 在节点 c1 开始相交。\n示例 1：\n4 -\u0026gt; 1 ↘ 8 -\u0026gt; 4 -\u0026gt; 5 5 -\u0026gt; 0 -\u0026gt; 1 ↗ 输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3\n输出：Reference of the node with value = 8\n输入解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。 从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。 在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。\n示例 2：\n2 -\u0026gt; 6 -\u0026gt; 4 1 -\u0026gt; 5\n输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2 输出：null\n输入解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。 由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。 解释：这两个链表不相交，因此返回 null。\n如果两个链表没有交点，返回 null. 在返回结果后，两个链表仍须保持原有的结构。 可假定整个链表结构中没有循环。 程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。 本题与主站 160 题相同：https://leetcode-cn.com/problems/intersection-of-two-linked-lists/\n方法1 双指针法 具体的方法如下图所示：\n对应代码如下：\n/** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func getIntersectionNode(headA, headB *ListNode) *ListNode { // 不需要边界检查 // if headA == nil || headB == nil { // return nil // } hha, hhb := headA, headB //pa, pb := headA, headB for headA != headB { if headA == nil { headA = hhb } else { headA = headA.Next } if headB == nil { headB = hha } else { headB = headB.Next } } return headA } 错误记录 1.超出时间限制 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func getIntersectionNode(headA, headB *ListNode) *ListNode { hha, hhb := headA, headB pa, pb := headA, headB for pa != pb { pa = pa.Next if pa == nil { pa = hhb } pb = pb.Next if pb == nil { pb = hha } } return pa } 测试用例：\n0 [2,6,4] [1,5] 3 2 分析：两个链表分别为 2 -\u0026gt; 6 -\u0026gt; 4 和 1 -\u0026gt; 5，且没有交点，而上面代码的问题在于，当某个指针为 null 时，会继续跳转到另一个链表的头部，而不会停留。对应代码为：\npa = pa.Next if pa == nil { pa = hhb } 对于两个不相交的链表而言，退出 while 的条件是两个指针都为 null，而上面的代码会导致指针永远不为 null， while 条件 pa != pb 永远不会成立，所以会进入死循环。\n2.其中一个链表为空 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func getIntersectionNode(headA, headB *ListNode) *ListNode { hha, hhb := headA, headB pa, pb := headA, headB for pa != pb { if pa == nil { pa = hhb } pa = pa.Next if pb == nil { pb = hha } pb = pb.Next } return pa } 测试用例：\n0 [1,3] [] 2 0 分析：解决之前的 超出时间 问题很简单，只要调换一下 pa == nil 和 pa = pa.Next 的位置即可，现在指针会停留在 null 上了。\n但又有了新的问题：如果两个链表有一个为空，如这里的测试用例 1 -\u0026gt; 3 和 null，此时第一个指针移动到 3，第二个链表的指针会直接移动到第一个链表的 1 处，接着又会执行 pa = pa.Next 移动到 3，两个指针的节点相同，返回结果 3\n这个结果明显是错误的，有一个空链表，怎么可能会有交点呢，错误的原因在于：pa = pa.Next 这句话是一定会执行的，解决这个问题只需要为 pa = pa.Next 加一个 else，让 pa = hhb 和 pa = pa.Next 只有一个能执行即可，更改后如下。\nif pa == nil { pa = hhb } else { pa = pa.Next } 起初还以为是边界判断的问题，为此加上了判断语句\nif headA == nil || headB == nil { return nil } 后来分析了以后才发现，这道题并不需要边界检查。\n一个特殊的测试用例 3 [3] [2,3] 0 1 两个链表分别为 3 和 2 -\u0026gt; 3，在 3 处相交。\n按照上面的步骤： 第 1 次 p1 = 3.Next = null （链表1） p2 = 2.Next = 3 （链表2）\n第 2 次 p1 = 2 （链表2） p2 = 3.Next = null （链表2）\n第 3 次 p1 = 2.Next = 3 （链表2） p2 = 3 （链表1）\n此时虽然两个指针都为 3，但是并不是同一个节点，一个在链表1，一个在链表2，按照逻辑，如果继续往下执行，两个指针则都为 null，会返回错误结果 无交点。\n但返回的却是正确结果 3，造成这一结果的原因，只可能是两个链表的 3 节点是相同的地址，为了验证这一猜想，打印一下两个结构体：\nfmt.Printf(\u0026quot;h1:%p %v h2:%p %v\\n\u0026quot;, headA, headA, headB, headB) // Output: // h1:0xc00008a320 \u0026amp;{3 \u0026lt;nil\u0026gt;} h2:0xc00008a340 \u0026amp;{2 0xc00008a320} 果真如此，两个链表的 3 节点地址都为 0xc00008a320\n","date":"2021年04月16日","permalink":"/posts/2021-3-23-offer-52-get-intersection-node/","summary":"题目描述 输入两个链表，找出它们的第一个公共节点。\t编写一个程序，找到两个单链表相交的起始节点。\n如下面的两个链表：\na1 -\u0026gt; a2 ↘ c1 -\u0026gt; c2 -\u0026gt; c3 b1 -\u0026gt; b2 -\u0026gt; b3 ↗ 在节点 c1 开始相交。","title":"剑指 Offer 52. 两个链表的第一个公共节点"},{"contents":"题目描述 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三 个元素 a，b，c ，使得 a + b + c = 0 请你找出所有和为 0 且不重复的三元组。\n注意：答案中不可以包含重复的三元组。\n示例 1：\n输入：nums = [-1,0,1,2,-1,-4] 输出：[[-1,-1,2],[-1,0,1]] 示例 2：\n输入：nums = [] 输出：[] 示例 3：\n输入：nums = [0] 输出：[] 提示：\n0 \u0026lt;= nums.length \u0026lt;= 3000 -105 \u0026lt;= nums[i] \u0026lt;= 105\n方法1：排序 + 双指针 算法流程 大致流程：\n首先将数组按 升序 排序 准备三个指针 k、i、j 初始化，将 k 放置在第一个元素，i 放置在 k+1 处，j 放置在最后一个元素处 开始循环，条件为 k \u0026lt; len(nums)-2 如果当前 nums[k] \u0026gt; 0，则 break 如果 nums[k\u0026ndash;] 和 nums[k] 相同，则 continue 去重 固定 k ，先移动 i 和 j，当 i == j 时，再移动 k 在移动 i 和 j 时计算 nums[k] + nums[j] + nums[i] 的值，记为 s 如果 s \u0026lt; 0，将 i++，如果 nums[i++] == nums[i]，则继续 + 1，这样可以保证不取到重复元素 如果 s \u0026gt; 0，将 j\u0026ndash;，如果 nums[j\u0026ndash;] == nums[j]，则继续 - 1，原因同上 如果 s == 0，记录当前的三个数字，之后将 i++，j\u0026ndash;，移动同时和上面一样，进行去重处理 分析：\n因为数组是排好序的，所以 k 指向的是最小元素，i 指向次小元素，j 指向最大元素（排除一些特殊情况比如 [0, 0, 0, 0]）\n这里的 k 起的是固定作用，目的是枚举全部情况，当三个值的和小于 0 时，可以将 i++ 使和增大，同理当和大于 0 时，可以将 j\u0026ndash; 将和减小，当和等于 0 时，说明此时已经搜集到了结果，可以将 k++ 进行下个数的枚举\n第 5 步的原因是：因为 nums[j] \u0026gt;= nums[i] \u0026gt;= nums[k] \u0026gt; 0，即 3 个数字都大于 0 ，在此固定指针 k 之后不可能再找到结果了。\n第 4 步的条件为 k \u0026lt; len(nums)-2，原因是 i 和 j 都在 k 之后，当 k 在 len(nums)-2-1 处时，j 在最后一个，i 在倒数第二个，k，i，j 刚好对应最后三个数，所以可以作为最后一次判断。这只是一个优化，可以减少 2 次不必要的循环，k \u0026lt; len(nums) 依然是正确的。\n关于去重的条件 i \u0026lt; j \u0026amp;\u0026amp; nums[i] == nums[i+1] 中需要首先判断 i \u0026lt; j 的原因是防止越界，例如：\n测试用例 [0, 0, 0]，k 在 [0]，i 在 [1]，j 在 [2]，sum = 0，此时因为 nums[i] == nums[i+1]，所以执行 i++ 到 [1]，还是重复，继续 i++ 到 [2]，如果没有 i \u0026lt; j 的条件，这里会执行判断语句 nums[i] == nums[i+1] 即 nums[2] == nums[3]，而这个 nums[3] 就会导致越界致使程序错误，但加了 i \u0026lt; j 会导致短路，使得后面的 nums[i] == nums[i+1] 不执行。\n对于 i \u0026lt; j \u0026amp;\u0026amp; nums[j] == nums[j-1] 也是一样的道理，还是测试用例 [0, 0, 0]，如果没有 i \u0026lt; j 的约束，则 j 会一直执行 j\u0026ndash;，当 j 到达 0 时，[j-1] 就会发生数组越界问题。\n流程如下图：\n代码如下：\n未优化版，是我根据上面思路写的：\nfunc threeSum(nums []int) [][]int { res := make([][]int, 0) if len(nums) == 0 { return res } sort.Slice(nums, func(i, j int) bool { return nums[i] \u0026lt; nums[j] }) k := 0 i, j := k+1, len(nums)-1 for k \u0026lt; len(nums)-2 { count++ s := 0 // [-1, 0, 0] if i \u0026gt;= j { k++ continue } if nums[k] \u0026gt; 0 { break } //fmt.Println(k, i, j) s = nums[k] + nums[i] + nums[j] if s \u0026gt; 0 { for i \u0026lt; j \u0026amp;\u0026amp; nums[j] == nums[j-1] { j-- } j-- } else if s \u0026lt; 0 { for i \u0026lt; j \u0026amp;\u0026amp; nums[i] == nums[i+1] { i++ } i++ } else { r := make([]int, 0) r = append(r, nums[k], nums[i], nums[j]) res = append(res, r) for i \u0026lt; j \u0026amp;\u0026amp; nums[i] == nums[i+1] { i++ } for i \u0026lt; j \u0026amp;\u0026amp; nums[j] == nums[j-1] { j-- } i++ j-- } if i \u0026gt;= j { for k \u0026lt; len(nums)-1 \u0026amp;\u0026amp; nums[k] == nums[k+1] { k++ } k++ i, j = k+1, len(nums)-1 } } return res }\t优化版，来自题解：\nfunc threeSum1(nums []int) [][]int { sort.Slice(nums, func(i, j int) bool { return nums[i] \u0026lt; nums[j] }) res := make([][]int, 0) for k := 0; k \u0026lt; len(nums)-2; k++ { count++ if nums[k] \u0026gt; 0 { break } if k \u0026gt; 0 \u0026amp;\u0026amp; nums[k] == nums[k-1] { continue } i, j := k+1, len(nums)-1 for i \u0026lt; j { count++ sum := nums[i] + nums[j] + nums[k] if sum \u0026lt; 0 { for i \u0026lt; j \u0026amp;\u0026amp; nums[i] == nums[i+1] { i++ } i++ } else if sum \u0026gt; 0 { for i \u0026lt; j \u0026amp;\u0026amp; nums[j] == nums[j-1] { j-- } j-- } else { r := make([]int, 0) r = append(r, nums[k], nums[i], nums[j]) res = append(res, r) for i \u0026lt; j \u0026amp;\u0026amp; nums[i] == nums[i+1] { i++ } i++ for i \u0026lt; j \u0026amp;\u0026amp; nums[j] == nums[j-1] { j-- } j-- } } } return res } 经过测试，这两个函数的执行时间相差了三十倍！不知道为何性能差距会如此之大，可能是我写的冗余判断条件太多了，暂时也没有去仔细分析的想法，只能说明自己的逻辑能力还是菜的抠脚，还需要多多去练习才行。\n","date":"2021年04月16日","permalink":"/posts/2021-3-24-leet-15-three-sum/","summary":"题目描述 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三 个元素 a，b，c ，使得 a + b + c = 0 请你找出所有和为 0 且不重复的三元组。","title":"leetcode 15. 三数之和"},{"contents":"题目描述 以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。 请你合并所有重叠的区间，并返回一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间。\n示例 1：\n输入：intervals = [[1,3],[2,6],[8,10],[15,18]] 输出：[[1,6],[8,10],[15,18]] 解释：区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6].\n示例 2：\n输入：intervals = [[1,4],[4,5]] 输出：[[1,5]] 解释：区间 [1,4] 和 [4,5] 可被视为重叠区间。\n分析 如何判断区间是否重叠，需要比较第二个数组的首元素是否小于前一个数组的尾元素，例如 [1,5],[2,6]，2 是小于 6 的，所以存在重叠， 而 [2,6],[8,10] 中，8 大于 6，所以区间不重叠。判断是否重叠并不难，难的在于合并。\n考虑如下几种情况：\n重叠，两个数组的尾元素相同，例如：[[1,4],[0,4]] =\u0026gt; [0, 4] 重叠，第二个数组的尾元素大于第一个数组的尾元素：[[1,5],[2,6]] =\u0026gt; [1, 6] 重叠，第二个数组的尾元素小于第一个数组的尾元素：[[1,5],[2,4]] =\u0026gt; [1, 5] 重叠，第一个数组的首元素大于第二个数组的首元素：[[1,4],[0,1]] =\u0026gt; [1, 4] 重叠，第一个数组的首元素小于第二个数组的首元素：[[0,4],[1,2]] =\u0026gt; [1, 4] 比较特殊的，[[1,4],[0,0]] =\u0026gt; [[0,0],[1,4]] 等等其他情况 可以看到情况有很多种，而我的第一想法也是粗暴的 if else，于是写出了下面的丑陋代码：\nfunc merge(intervals [][]int) [][]int { res := make([][]int, 0) res = append(res, intervals[0]) // 上一个数组的下标 prev := 0 //start, end := intervals[0][0], intervals[0][1] for i := 1; i \u0026lt; len(intervals); i++ { // 上一个数组的 [0] 和 [1] prevs, preve := intervals[prev][0], intervals[prev][1] // 当前数组的 [0] 和 [1] curs, cure := intervals[i][0], intervals[i][1] if curs \u0026lt;= preve \u0026amp;\u0026amp; cure \u0026gt; preve { // [[1,5],[2,6]] if curs \u0026lt; prevs { // [[1,4],[0,5]] r := []int{curs, cure} res[i-1] = r prev = i - 1 } else { r := []int{prevs, cure} res[i-1] = r prev = i - 1 } } else if curs \u0026lt;= preve \u0026amp;\u0026amp; cure \u0026lt;= preve { // [[1,5],[2,4]] if curs \u0026lt; prevs { // [[1,4],[0,4]] [[1,4],[0,1]] r := []int{curs, preve} res[i-1] = r prev = i - 1 } else if cure \u0026lt; prevs { // [[1,4],[0,0]] res = append(res, intervals[i]) swap(res, i, prev) } else { continue } } else { res = append(res, intervals[i]) } } return res } func swap(nums [][]int, i, j int) { nums[i], nums[j] = nums[j], nums[i] } 这是在提交失败了 n 次的基础上不断修改的，每个 if 分支都标明了对应的情况，但是，还是遇到了一种特殊的测试用例：[[1,4],[0,0]]，我的代码运行的结果是 [0,4]，但是正确答案是 [[0,0],[1,4]]，为了对付这种情况，我又写了一个 swap 函数来应对。考虑的情况越来越多，代码也越来越丑，最关键的是，提交还始终无法通过。无奈只能看题解了，\n方法 排序 + 双指针 这个排序我是万万没想到的，根据数组的左元素进行排序，这样一来，就可以完美处理 [[1,4],[0,0]] 这种情况了。\n这里直接贴上来自题解中的代码，非常直观易懂且简洁，巧妙的使用了 max 函数来合并区间，比我傻傻的写的一堆 if else 优雅多了。 （来自 https://leetcode-cn.com/problems/merge-intervals/solution/shou-hua-tu-jie-56he-bing-qu-jian-by-xiao_ben_zhu/）\nfunc merge(intervals [][]int) [][]int { sort.Slice(intervals, func(i, j int) bool { return intervals[i][0] \u0026lt; intervals[j][0] }) res := [][]int{} prev := intervals[0] for i := 1; i \u0026lt; len(intervals); i++ { cur := intervals[i] if prev[1] \u0026lt; cur[0] { // 没有一点重合 res = append(res, prev) prev = cur } else { // 有重合 prev[1] = max(prev[1], cur[1]) } } res = append(res, prev) return res } func max(a, b int) int { if a \u0026gt; b { return a } return b } 这里用 prev 来保存上一个数组，初始保存的是第 0 个，然后 for 从 1 开始，即用第 1 个和第 0 个进行比较，如果没有重合，则 append 并更新 prev 为当前数组，有重合，则对 prev 进行合并，因为已经排好序了，所以前一个的 [0] 是一定小于后一个的 [0]，只需要比较 前一个的[1] 和后一个的 [1] 即可，这里用了 max 函数来比较。\n特别注意一下最后的 res = append(res, prev) ，没有这句话会导致错误。\n原因：for 是从第二个数组开始判断的，并不断与前一个数组比较，如果不在 for 外面再定义一次 res = append(res, prev)，则会导致结果缺失的情况。\n例如：[[1,3]]，此时都不会进入 for ，如果没有末尾的这句话，会直接导致结果为空。\n","date":"2021年04月16日","permalink":"/posts/2021-3-26-leet-56-merge-interval/","summary":"题目描述 以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。 请你合并所有重叠的区间，并返回一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间。","title":"LeetCode 56. 合并区间"},{"contents":"算法描述 这不是一道力扣或者剑指上的原题，但却是二叉树问题的一个基础核心算法，适用于许多二叉树类型的题目，比如 剑指 Offer 68 - II. 二叉树的最近公共祖先，因为这里有必要记录一下，大致的说明如下：\n例如如下一颗二叉树：\n当指定节点为 4 时，输出 [3,5,2,4]\n方法1 回溯，有返回值 代码如下，参考\n// 找到根节点到某一节点的路径 func findPath(node, need *TreeNode, // node: 当前节点，need: 指定节点 path, res *[]*TreeNode, // path: 记录当前路径 res: 保存结果路径 flag *bool) []*TreeNode {\t// flag: 用来标识是否已经找到结果，作为递归终止的条件 if node == nil || *flag { return *res } *path = append(*path, node) // 找到指定的节点了，将 flag 更改为 true，同时将当前路径添加到 res 作为结果 if node == need { *flag = true *res = *path } // 先找左子树，再找右子树 findPath(node.Left, need, path, res, flag) findPath(node.Right, need, path, res, flag) // 回溯，在 path 中移除当前节点 *path = (*path)[:len(*path)-1] rr := make([]*TreeNode, len(*res)) // 因为 slice 共用一个底层数组，第二次会更改第一次的结果， // 所以需要 copy 一个新 slice copy(rr, *res) return rr } path 和 res 都需要为指针切片，因为 append 会导致地址变更，flag 为了保证递归参数传递的状态一致，也需要为指针类型，其他的都写在注释中了，特别需要说明的是最后的 copy，这也是我踩的一个坑，特此记录一下：\n引用 算法 中的二叉树，求 1 和 4 两个节点的路径，正确结果应该为 [3,1] 和 [3,5,2,4]但是最后运行的结果却是 [3,5] 和 [3,5,2,4]，测试代码如下：\n// p = 1 pr := findPath(root, p, \u0026amp;path, \u0026amp;res, \u0026amp;flag) flag = false // p = 4 qr := findPath(root, q, \u0026amp;path, \u0026amp;res, \u0026amp;flag) for i := 0; i \u0026lt; len(pr); i++ { fmt.Printf(\u0026quot;%v \u0026quot;, pr[i]) // Output: [3, 5] } fmt.Println() for i := 0; i \u0026lt; len(qr); i++ { fmt.Printf(\u0026quot;%v \u0026quot;, qr[i]) // Output: [3,5,2,4] } 我又尝试将 pr 的输出语句移动至 qr 前，结果是正确的：\n// p = 1 pr := findPath(root, p, \u0026amp;path, \u0026amp;res, \u0026amp;flag) for i := 0; i \u0026lt; len(pr); i++ { fmt.Printf(\u0026quot;%v \u0026quot;, pr[i]) // Output: [3, 1] } fmt.Println() flag = false // p = 4 qr := findPath(root, q, \u0026amp;path, \u0026amp;res, \u0026amp;flag) for i := 0; i \u0026lt; len(qr); i++ { fmt.Printf(\u0026quot;%v \u0026quot;, qr[i]) // Output: [3,5,2,4] } 原因是因为我刚开始在函数中返回的是 *res，而 *res = *path ，即让 res 的底层数组指针指向了 path 的底层数组， 传入的 path 都是同一个，这样第二次寻找路径时，path 会改变，这会使得指向 path 的 res 也跟着改变，所以第二次运行时会将 res 的 [1] 改为 [5]，解决方法就是新定义一个 slice，并 copy，再返回这个新 slice 即可。\n此外，flag 参数也不需要传递指针类型，只要 bool 即可。\n方法2 回溯，无返回值 方法 1 中的返回值其实完全是多余的，参数 res 已经保存了结果，完全可以不需要返回值，如下：\nfunc findPath(root, need *TreeNode, path, res *[]*TreeNode, flag *bool) { if root == nil || *flag { return } *path = append(*path, root) if root == need { *flag = true // copy 一个新切片，防止多个 res 指向同一个 path news := make([]*TreeNode, len(*path)) copy(news, *path) *res = news return } findPath(root.Left, need, path, res, flag) findPath(root.Right, need, path, res, flag) *path = (*path)[:len(*path)-1] return } func main() { var path, pp, qq []*TreeNode var flag bool findPath(root, p, \u0026amp;path, \u0026amp;pp, \u0026amp;flag) // 重置参数 flag = false path = path[0:0] findPath(root, q, \u0026amp;path, \u0026amp;qq, \u0026amp;flag) // 此时 pp 和 qq 已经保存了结果 } 有无返回值的性能差异 在 剑指 Offer 68 - II. 二叉树的最近公共祖先 这道题中，有一个非常长的测试用例（可能有上万个节点），这里使用有返回值的方法会超时，即使返回指针也一样超时，而无返回值则正常，可能是因为返回值需要拷贝的缘故，而且方法 1 的返回值本身就是一个冗余的东西，既然都已经提供了 res 参数用来保存结果，就不需要再返回了。\n","date":"2021年04月16日","permalink":"/posts/2021-3-27-find-binarytree-root-to-node-path/","summary":"算法描述 这不是一道力扣或者剑指上的原题，但却是二叉树问题的一个基础核心算法，适用于许多二叉树类型的题目，比如 剑指 Offer 68 - II.","title":"求二叉树根节点到指定节点的路径"},{"contents":"题目描述 给你两个版本号 version1 和 version2 ，请你比较它们。\n版本号由一个或多个修订号组成，各修订号由一个 \u0026lsquo;.\u0026rsquo; 连接。每个修订号由 多位数字 组成，可能包含 前导零 。每个版本号至少包含一个字符。修订号从左到右编号，下标从 0 开始，最左边的修订号下标为 0 ，下一个修订号下标为 1 ，以此类推。例如，2.5.33 和 0.1 都是有效的版本号。\n比较版本号时，请按从左到右的顺序依次比较它们的修订号。比较修订号时，只需比较 忽略任何前导零后的整数值 。也就是说，修订号 1 和修订号 001 相等 。如果版本号没有指定某个下标处的修订号，则该修订号视为 0 。例如，版本 1.0 小于版本 1.1 ，因为它们下标为 0 的修订号相同，而下标为 1 的修订号分别为 0 和 1 ，0 \u0026lt; 1 。\n返回规则如下：\n如果 version1 \u0026gt; version2 返回 1， 如果 version1 \u0026lt; version2 返回 -1， 除此之外返回 0。\n示例 1：\n输入：version1 = \u0026ldquo;1.01\u0026rdquo;, version2 = \u0026ldquo;1.001\u0026rdquo; 输出：0 解释：忽略前导零，\u0026ldquo;01\u0026rdquo; 和 \u0026ldquo;001\u0026rdquo; 都表示相同的整数 \u0026ldquo;1\u0026rdquo; 示例 2：\n输入：version1 = \u0026ldquo;1.0\u0026rdquo;, version2 = \u0026ldquo;1.0.0\u0026rdquo; 输出：0 解释：version1 没有指定下标为 2 的修订号，即视为 \u0026ldquo;0\u0026rdquo; 示例 3：\n输入：version1 = \u0026ldquo;0.1\u0026rdquo;, version2 = \u0026ldquo;1.1\u0026rdquo; 输出：-1 解释：version1 中下标为 0 的修订号是 \u0026ldquo;0\u0026rdquo;，version2 中下标为 0 的修订号是 \u0026ldquo;1\u0026rdquo; 。0 \u0026lt; 1，所以 version1 \u0026lt; version2 示例 4：\n输入：version1 = \u0026ldquo;1.0.1\u0026rdquo;, version2 = \u0026ldquo;1\u0026rdquo; 输出：1 示例 5：\n输入：version1 = \u0026ldquo;7.5.2.4\u0026rdquo;, version2 = \u0026ldquo;7.5.3\u0026rdquo; 输出：-1\n提示：\n1 \u0026lt;= version1.length, version2.length \u0026lt;= 500 version1 和 version2 仅包含数字和 \u0026lsquo;.\u0026rsquo; version1 和 version2 都是 有效版本号 version1 和 version2 的所有修订号都可以存储在 32 位整数 中\n算法设计 1.切割比较 思路：使用 strings.split() 函数，按 . 切割，再使用 atoi() 将切割好的字符转换为整形，并一一比较。\n上面的问题：如果两个版本号长度不同，例如 1.0 和 1.0.0，切割出的数组分别为 [1, 0] 和 [1, 0, 0]，按下标逐一比较，会有一个出现越界异常\n解决方法：对较短的数组进行扩充，使二者长度相同，扩充元素为 0，因为 0 不会对结果产生影响，例如上面的例子，可以将 1 扩充为 [1, 0, 0]，这样就可以安全比较了。\n代码如下：\nfunc compareVersion(version1 string, version2 string) int { v1arr := strings.Split(version1, \u0026quot;.\u0026quot;) v2arr := strings.Split(version2, \u0026quot;.\u0026quot;) // 为较短数组填充 0，使两个数组的长度相同 if len(v1arr) \u0026gt; len(v2arr) { sub := len(v1arr) - len(v2arr) for sub \u0026gt; 0 { v2arr = append(v2arr, \u0026quot;0\u0026quot;) sub-- } } else if len(v1arr) \u0026lt; len(v2arr) { sub := len(v2arr) - len(v1arr) for sub \u0026gt; 0 { v1arr = append(v1arr, \u0026quot;0\u0026quot;) sub-- } } for i := 0; i \u0026lt; len(v1arr); i++ { // 转换为整形比较，无需担心 001 这样的字符串，atoi() 会智能的将其转换为 1 v1i, _ := strconv.Atoi(v1arr[i]) v2i, _ := strconv.Atoi(v2arr[i]) if v1i \u0026gt; v2i { return 1 } else if v1i \u0026lt; v2i { return -1 } else { continue } } return 0 } 2.双指针 第一种方法比较直观易懂，但是需要额外的空间，还需要多次遍历，效率不是很高，而双指针法可以解决上述的问题。\n思路：准备两个指针，分别置于两个版本号的开头，之后不断移动两个指针，直到都到达末尾。在移动的过程中，初始化两个变量 v1，v2 用于保存当前小版本号的值，如何获取小版本号的值呢？只需要使用 while 循环不断移动指针，当指针值为 . 时停止，每次循环都将 v1 = v1 + 指针值 - \u0026lsquo;0\u0026rsquo;，\nfunc compareVersion(version1 string, version2 string) int { l1, l2 := len(version1), len(version2) p1, p2 := 0, 0 // 获取最大值 var maxFunc = func(x, y int) int { if x \u0026gt; y { return x } else { return y } } max := maxFunc(l1, l2) for p1 \u0026lt; max || p2 \u0026lt; max { var v1, v2 int // 使用 while 来循环读取一个小版本号（例如 1.111 中的 1 和 111 就是小版本号）， // 遇到 . 停止，此时 vv1, vv2 的值即是小版本号，vv1 和 vv2 定义在外层 for 内， // 每次比较后都会清零 for p1 \u0026lt; l1 \u0026amp;\u0026amp; version1[p1] != '.' { v1 = v1*10 + int(version1[p1]) - '0' p1++ } for p2 \u0026lt; l2 \u0026amp;\u0026amp; version2[p2] != '.' { v2 = v2*10 + int(version2[p2]) - '0' p2++ } if v1 \u0026gt; v2 { return 1 } else if v1 \u0026lt; v2 { return -1 } p1++ p2++ } return 0 } 错误记录 1. 超出时间限制 func compareVersion(version1 string, version2 string) int { p1, p2 := 0, 0 for p1 \u0026lt; len(version1) || p2 \u0026lt; len(version2) { v1, v2 := 0, 0 for p1 \u0026lt; len(version1) \u0026amp;\u0026amp; version1[p1] != '.' { v, _ := strconv.Atoi(string(version1[p1])) v1 += v p1++ } for p2 \u0026lt; len(version2) \u0026amp;\u0026amp; version1[p2] != '.' { v, _ := strconv.Atoi(string(version1[p2])) v2 += v p2++ } if v1 \u0026gt; v2 { return 1 } else if v1 \u0026lt; v2 { return -1 } } return 0 } 这里超时的原因是末尾没有 p1++ 和 p2++，当 p1 和 p2 都为 \u0026lsquo;.\u0026rsquo; 时，p1 和 p2 将不会有任何改变，这会导致 while 条件一直满足，从而陷入死循环。\n所以末尾的 p1++ 和 p2++ 就是用来保证当 p1，p2 都为 \u0026lsquo;.\u0026rsquo; 时仍然会移动，防止死循环的发生。\n2. 解答错误 func compareVersion(version1 string, version2 string) int { p1, p2 := 0, 0 for p1 \u0026lt; len(version1) || p2 \u0026lt; len(version2) { v1, v2 := 0, 0 for p1 \u0026lt; len(version1) \u0026amp;\u0026amp; version1[p1] != '.' { v, _ := strconv.Atoi(string(version1[p1])) v1 += v p1++ } for p2 \u0026lt; len(version2) \u0026amp;\u0026amp; version2[p2] != '.' { v, _ := strconv.Atoi(string(version2[p2])) v2 += v p2++ } if v1 \u0026gt; v2 { return 1 } else if v1 \u0026lt; v2 { return -1 } p1++ p2++ } return 0 } 测试用例：\n\u0026quot;1.1\u0026quot; \u0026quot;1.10\u0026quot; 这里错误的原因是因为对于小版本号只是单纯的相加操作，对于 1.10 的第二个小版本而言，结果为 1 + 0 = 1，从而导致错误结果 1.1 = 1.10，解决方法是将对应小版本号从 string 转为 int，具体方法是 v = v*10 + s[i] - \u0026lsquo;0\u0026rsquo;。\n","date":"2021年04月16日","permalink":"/posts/2021-3-5-leet-165-compare-version-numbers/","summary":"题目描述 给你两个版本号 version1 和 version2 ，请你比较它们。","title":"LeetCode165.比较版本号"},{"contents":"题目描述 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转，输出旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一个旋转，该数组的最小值为1。\n示例 1：\n输入：[3,4,5,1,2] 输出：1 示例 2：\n输入：[2,2,2,0,1] 输出：0\n图片来源：https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/solution/mian-shi-ti-11-xuan-zhuan-shu-zu-de-zui-xiao-shu-3/\n从上图可以看出来，所谓的旋转数组，实质上是变成了两个有序数组，且左边的有序数组要大于右边的有序数组，所以右边的有序数组的第一个值，就是整个数组中的最小值。\n二分查找，是通过每次缩短查找区间来减少查找的次数，但前提是数组必须是有序的，而旋转数组恰好破坏了数组的有序性，但这并不代表旋转数组就不能使用二分查找来解决问题了。\n思路 还是和常规的二分查找一样，先创建两个指针（left 和 right），分别置于数组的第一个数（left）和最后一个数（right），并相加除以二求出中位 接下来判断中位属于哪个有序数组，是左边的还是右边的，具体的判断方法后面会提到 在确定了属于哪个有序数组后，就可以像普通的二分查找一样，去掉一部分的查找区间了，如何去除，后面会提到 不断更新指针，直到两个指针在同一位置 1.如何判断中位属于左边还是右边？\n如果中位值 大于 right 的值，说明在左边数组 如果中位值 小于 right 的值，说明在右边数组\n例如： [4, 5, 1, 2, 3]，中位值为 1，小于 right 3，所以在右边数组 [3, 4, 5, 1, 2]，中位值为 5，大于 right 2，所以在左边数组\n2.在确定了中位所属数组后，如何缩短查找空间？\n因为左数组要大于右数组，所以：\n如果中位在右数组，那么可以直接去掉左数组，例如 [4, 5, 1, 2, 3]，左数组 [4, 5] 是要大于右数组 [1, 2, 3] 的，所以可以直接去掉 4, 5 这两个元素，但其本身有可能是最小元素，所以不能去除。\n如果中位在左数组，那么可以去掉其左边的所有元素以及它自己，因为左数组是要大于右数组的，例如 [3, 4, 5, 1, 2]，可以直接将 3，4，5全部去掉\n如果中位在右数组，那么可以去掉其右边的所有元素，因为这些元素是一定比它大的，例如 [5, 1, 2, 3, 4]，中位所处数组为 [1, 2, 3, 4]，它右边的 3 和 4 可以直接去除掉。\n一种特殊情况： [1, 0, 1, 1, 1]，中位的值和 right 值相等，这时该如何处理呢？参照力扣的题解说法：遇到 nums[mid] == nums[right] 的时候，不能草率地下定结论最小数字在哪一边，但是可以确定的是，把 right 舍弃掉，并不影响结果。\n图解 图片来源：https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/solution/mian-shi-ti-11-xuan-zhuan-shu-zu-de-zui-xiao-shu-3/\n代码 func minArray(numbers []int) int { left, right := 0, len(numbers)-1 for left \u0026lt; right { mid := (left + right) \u0026gt;\u0026gt; 1 // 在第一个数组 if numbers[mid] \u0026gt; numbers[right] { left = mid + 1 } else if numbers[mid] \u0026lt; numbers[right] { // 在第二个 right = mid } else { right-- } } return numbers[left] } 错误记录/思考 为什么判断是否在第一个数组，必须用 mid 和 right 比较，而不能和 left 比较？比如 [3, 4, 5, 1, 2]，可以通过 mid \u0026gt; right 得到在第一个数组，但同样也可以用 mid \u0026gt; left 判断出同样的结果。\n首先可以确定的是，mid 和 left 比较会出错，比如这个测试用例 [1, 1, 0, 1, 1]，两种比较方式结果如下图：\n对于这样一种特殊旋转数组 [1, 2, 3, 4, 5] （即旋转了 0 个数），此时 mid 无法确定属于哪个数组，且也不符合之前的左数组大于右数组的条件，这种情况下，与 right 比较是安全的，这会将其分配到右数组 [3, 4, 5]，并将搜索区间转移到左部分 [1, 2, 3]。\n如果与 left 比较，会错误的分配到右数组 [1, 2, 3]，并将这部分从搜索区间中去除，留下 [4, 5]，结果也显然意见是错误的。\n综上，要避免与 left 比较。\n","date":"2021年04月16日","permalink":"/posts/2021-3-6-offer-11-retate-array-min-num/","summary":"题目描述 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转，输出旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一个旋转，该数组的最小值为1。","title":"剑指 Offer 11. 旋转数组的最小数字"},{"contents":"题目描述 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖 先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n例如，给定如下二叉树:\nroot = [3,5,1,6,2,0,8,null,null,7,4] 示例 1:\n输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出: 3 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。\n示例 2:\n输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 输出: 5 解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定 义最近公共祖先节点可以为节点本身。\n算法设计 方法1. 递归 这次不多 bb 直接上代码：\n代码： /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root == nil || root == p || root == q { return root } // 记录左子树部分是否有 p 或 q，如果有，则 left 的值为 p 或 q 的某 // 一父节点（也有可能是 p 或 q本身），如果没有则为 null left := lowestCommonAncestor(root.Left, p, q) // 记录右子树部分是否有 p 或 q，如果有，则 left 的值为 p 或 q 的某 // 一父节点（也有可能是 p 或 q本身），如果没有则为 null right := lowestCommonAncestor(root.Right, p, q) // left 为空，说明左子树部分没有 p，q，返回 right，即在右子树部分 // 找到的结果 if left == nil { return right // 同理如上 } else if right == nil { return left } // left 和 right 都不为空，说明 p，q 在异侧，此时当前节点 root // 就是它们的公共父节点 return root } 递归分析 递归流程图 递归过程 （勘误：p=7，q=4 这一步的图片流程的第 2 步，应该是：说明该节点在其右子树，图片中错误的写成了：说明该节点在其左子树，这里懒得再修改图片了，直接文字说明）\n文字分析 递归解析：\n终止条件： 当越过叶节点，则直接返回 null； 当 root 等于 p, q，则直接返回 root； 递推工作： 开启递归左子节点，返回值记为 left； 开启递归右子节点，返回值记为 right； 返回值： 根据 left 和 right ，可展开为四种情况； 当 left 和 right 同时为空 ：说明 root 的左 / 右子树中都不包含 p,q ，返回 null； 当 left 和 right 同时不为空 ：说明 p, q 分列在 root 的 异侧 （分别在 左 / 右子树），因此 root 为最近公共祖先，返回 root； 当 left 为空 ，right 不为空 ：p,q 都不在 root 的左子树中，直接返回 right 。具体可分为两种情况： p,q 其中一个在 root 的 右子树 中，此时 right 指向 pp（假设为 pp ）； p,q 两节点都在 root 的 右子树 中，此时的 right 指向 最近公共祖先节点 ； 当 left 不为空 ， right 为空 ：与情况 3. 同理； 参考\n递归在向上归时，会不断根据当前条件，更新返回的信息（参考递归过程图）\n方法2. 搜索根节点到 p，q 的路径，再求两条路径的最后一个相同节点 代码： /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { var path, pp, qq []*TreeNode var flag bool findPath(root, p, \u0026amp;path, \u0026amp;pp, \u0026amp;flag) flag = false path = path[0:0] findPath(root, q, \u0026amp;path, \u0026amp;qq, \u0026amp;flag) size := min(len(pp), len(qq)) i := 0 for ; i \u0026lt; size; i++ { if pp[i] == qq[i] { continue } else { break } } return pp[i-1] } func findPath(root, need *TreeNode, path, res *[]*TreeNode, flag *bool) { if root == nil || *flag { return } *path = append(*path, root) if root == need { *flag = true //*res = *path //fmt.Println(res) news := make([]*TreeNode, len(*path)) copy(news, *path) //fmt.Println(news) *res = news return } findPath(root.Left, need, path, res, flag) findPath(root.Right, need, path, res, flag) *path = (*path)[:len(*path)-1] return } func min(x, y int) int { if x \u0026lt; y { return x } return y } 算法的思路已经写在标题里了，分析可以参考文章 求二叉树根节点到指定节点的路径\n","date":"2021年04月16日","permalink":"/posts/2021-4-3-offer-68-lowest-common-ancestor/","summary":"题目描述 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖 先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”","title":"剑指 Offer 68 - II. 二叉树的最近公共祖先"},{"contents":"题目描述 算法设计 方法1 回溯法，使用 bool 数组记录每个数是否被访问 cpp 版本：\nclass Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; permute(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; res; vector\u0026lt;bool\u0026gt; visit; // error: load of null pointer of type 'std::_Bit_type' // need init bool vector visit.resize(nums.size(), false); vector\u0026lt;int\u0026gt; temp; backtrack(nums, temp, visit, res); return res; } void backtrack(vector\u0026lt;int\u0026gt;\u0026amp; nums, vector\u0026lt;int\u0026gt;\u0026amp; temp, vector\u0026lt;bool\u0026gt;\u0026amp; visit, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; res) { if (temp.size() == nums.size()){ res.emplace_back(temp); return; } for (int i = 0; i \u0026lt; nums.size(); i++) { if (visit[i]) { continue; } temp.emplace_back(nums[i]); visit[i] = true; backtrack(nums, temp, visit, res); visit[i] = false; temp.pop_back(); } } }; go 版本：\nfunc permute(nums []int) [][]int { res := make([][]int, 0) if len(nums) == 0 { return res } temp := make([]int, 0) isvisited := make([]bool, len(nums)) BackTrack(isvisited, temp, nums, \u0026amp;res) return res } func BackTrack(isvisited []bool, temp []int, nums []int, res *[][]int) { if len(temp) == len(nums) { newTemp := make([]int, len(temp)) copy(newTemp, temp) *res = append(*res, newTemp) return // 这里可以直接 return 了，没必要再走下面的循环了 } for i := 0; i \u0026lt; len(nums); i++ { if isvisited[i] { continue } temp = append(temp, nums[i]) isvisited[i] = true BackTrack(isvisited, temp, nums, res) temp = temp[:len(temp)-1] isvisited[i] = false } } 方法2 回溯法，使用 swap swap 的方式相比方法 1 要简洁一些，也比较好理解，效率也高一些。\n代码如下：\ncpp 版本：\nclass Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; permute(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; res; backtrack(nums, 0, res); return res; } void backtrack(vector\u0026lt;int\u0026gt;\u0026amp; vec, int first, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; res) { if (first == vec.size()) { res.emplace_back(vec); return; } for (int i = first; i \u0026lt; vec.size(); i++) { swap(vec.at(i), vec.at(first)); backtrack(vec, first+1, res); swap(vec.at(i), vec.at(first)); } } }; go 版本：\nfunc permute(nums []int) [][]int { var res [][]int backtrack(nums, 0, \u0026amp;res) return res } func backtrack(nums []int, first int, res *[][]int) { if first == len(nums) { news := make([]int, len(nums)) copy(news, nums) *res = append(*res, news) } for i := first; i \u0026lt; len(nums); i++ { swap(nums, first, i) backtrack(nums, first+1, res) swap(nums, first, i) } } func swap(nums []int, i, j int) { nums[i], nums[j] = nums[j], nums[i] } 性能测试 采用 go test benchmark，结果如下：\n// BenchmarkBool-8 2581156\t437.1 ns/op // BenchmarkSwap-8 1999869\t573.2 ns/op // BenchmarkSwap-8 2313684\t440.1 ns/op func BenchmarkSwap(b *testing.B) { n := []int{1, 2, 3} for i := 0; i \u0026lt; b.N; i++ { permute(n) } } // BenchmarkBool-8 999951\t1029 ns/op // BenchmarkBool-8 1098356\t963.6 ns/op // BenchmarkBool-8 1248417\t927.0 ns/op func BenchmarkBool(b *testing.B) { n := []int{1, 2, 3} for i := 0; i \u0026lt; b.N; i++ { permute1(n) } } ","date":"2021年04月16日","permalink":"/posts/2021-4-6-leet-46-quan-pai-lie/","summary":"题目描述 算法设计 方法1 回溯法，使用 bool 数组记录每个数是否被访问 cpp 版本：","title":"leetcode 46.全排列【未完】"},{"contents":"题目描述 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通队列的全部四种操作（push、top、pop 和 empty）。\n实现 MyStack 类：\nvoid push(int x) 将元素 x 压入栈顶。 int pop() 移除并返回栈顶元素。 int top() 返回栈顶元素。 boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。 注意：\n你只能使用队列的基本操作 —— 也就是 push to back、peek/pop from front、size 和 is empty 这些操作。 你所使用的语言也许不支持队列。 你可以使用 list （列表）或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 示例：\n输入： [\u0026ldquo;MyStack\u0026rdquo;, \u0026ldquo;push\u0026rdquo;, \u0026ldquo;push\u0026rdquo;, \u0026ldquo;top\u0026rdquo;, \u0026ldquo;pop\u0026rdquo;, \u0026ldquo;empty\u0026rdquo;] [[], [1], [2], [], [], []] 输出： [null, null, null, 2, 2, false]\n解释： MyStack myStack = new MyStack(); myStack.push(1); myStack.push(2); myStack.top(); // 返回 2 myStack.pop(); // 返回 2 myStack.empty(); // 返回 False 提示：\n1 \u0026lt;= x \u0026lt;= 9 最多调用100 次 push、pop、top 和 empty 每次调用 pop 和 top 都保证栈不为空 进阶：你能否实现每种操作的均摊时间复杂度为 O(1) 的栈？换句话说，执行 n 个操作的总时间复杂度 O(n) ，尽管其中某个操作可能需要比其他操作更长的时间。你可以使用两个以上的队列。\n方法1 一个队列实现 具体思路： push 时，先将元素正常 push 到队尾，再将之前的所有元素执行 1.出队、2.入队 操作\n代码如下：\ntype MyStack struct { Queue *list.List } /** Initialize your data structure here. */ func Constructor() MyStack { return MyStack{ Queue: list.New(), } } /** Push element x onto stack. */ func (this *MyStack) Push(x int) { q := this.Queue _len := q.Len() q.PushBack(x) for i := 0; i \u0026lt; _len; i++ { q.PushBack(q.Remove(q.Front()) ) } } /** Removes the element on top of the stack and returns that element. */ func (this *MyStack) Pop() int { return this.Queue.Remove(this.Queue.Front()).(int) } /** Get the top element. */ func (this *MyStack) Top() int { return this.Queue.Front().Value.(int) } /** Returns whether the stack is empty. */ func (this *MyStack) Empty() bool { return this.Queue.Len() == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ ","date":"2021年04月16日","permalink":"/posts/leetcode-225-yong-dui-lie-shi-xian-zhan-wei-wan/","summary":"题目描述 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通队列的全部四种操作（push、top、pop 和 empty）。\n实现 MyStack 类：","title":"leetcode 225. 用队列实现栈【未完】"},{"contents":"题目描述 在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n示例：\n现有矩阵 matrix 如下：\n[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ] 给定 target = 5，返回 true。\n给定 target = 20，返回 false。\n限制：\n0 \u0026lt;= n \u0026lt;= 1000 0 \u0026lt;= m \u0026lt;= 1000 分析 因为矩阵是从上往下、从左往右递增的，所以可以选择左下角或者右上角为起始点：\n算法流程：\n从起始点 m[a] [b] 开始判断，如果是 右上角，若当前值 大于 target，则可以 b\u0026ndash; 去掉 当前列 ，若当前值 小于 target，可以 a++ 去掉 当前行 ，左下角也类似\n图解，这里借用一下别人的：\n作者：jyd 链接：https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/solution/mian-shi-ti-04-er-wei-shu-zu-zhong-de-cha-zhao-zuo/\n结合上面的图示，这道题的思路还是比较好理解的，但是代码并不好写，主要是 while 条件比较难把握，很容易出错，比如对于以下测试用例：\n// 越界 [[1,4,7,11,15],[2,5,8,12,19],[3,6,9,16,22],[10,13,14,17,24],[18,21,23,26,30]] 20 // 不执行 while，直接返回 false [[-5]] -5 具体分析如下：\n代码 起始点在左下角：\nfunc findNumberIn2DArray(matrix [][]int, target int) bool { if len(matrix) == 0 || len(matrix[0]) == 0 { return false } i, j := len(matrix)-1, 0 var flag int for i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; len(matrix[0]) { flag = matrix[i][j] if flag \u0026gt; target { i-- } else if flag \u0026lt; target { j++ } else { return true } } return false } 起始点在右上角：\nfunc findNumberIn2DArray(matrix [][]int, target int) bool { if len(matrix) == 0 { return false } if len(matrix[0]) == 0 { return false } i, j := 0, len(matrix[0])-1 var flag int for i \u0026lt; len(matrix) \u0026amp;\u0026amp; j \u0026gt;= 0 { flag = matrix[i][j] if flag \u0026gt; target { j-- } else if flag \u0026lt; target { i++ } else { return true } //fmt.Printf(\u0026quot;i -\u0026gt; %d j -\u0026gt; %d\\n\u0026quot;, i, j) //fmt.Println(\u0026quot;flag: \u0026quot;, flag) //flag = matrix[i][j] } return false } ","date":"2021年04月16日","permalink":"/posts/jian-zhi-offer-04-er-wei-shu-zu-zhong-de-cha-zhao/","summary":"题目描述 在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。","title":"剑指 Offer 04. 二维数组中的查找"},{"contents":"题目描述 请从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。\n示例 1:\n输入: \u0026quot;abcabcbb\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;abc\u0026quot;，所以其长度为 3。 示例 2:\n输入: \u0026quot;bbbbb\u0026quot; 输出: 1 解释: 因为无重复字符的最长子串是 \u0026quot;b\u0026quot;，所以其长度为 1。 示例 3:\n输入: \u0026quot;pwwkew\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;wke\u0026quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，\u0026quot;pwke\u0026quot; 是一个子序列，不是子串 提示：\ns.length \u0026lt;= 40000\n方法1：滑动窗口 通过双指针 + 哈希表实现，算法思路如下：\n初始化两个指针 i 和 j，同时指向第一个字符 i 指针不动，j 指针不断前进，每次前进都更新其在 map 中的值（值是下标），同时计算最大窗口值 如果 j 在移动过程中遇到重复元素（在 map 中已存在），则将 i 移动到 max(i, map[重复元素]+1) 处，这里为什么要比较大小，后面会解释 当 j 大于字符串长度时，结束循环，返回最大窗口值 对应代码如下：\nfunc lengthOfLongestSubstring(s string) int { i, j, maxl := 0, 0, 0 m := make(map[byte]int) for j \u0026lt; len(s) { cur := s[j] // 当前字符重复 if _, ok := m[cur]; ok { // 移动 i 到重复元素下标+1 处，如果当前位置下标大于 m[cur]+1，则不移动 i = max(i, m[cur]+1) } // 如果当前长度更大，则更新最大长度 maxl = max(maxl, j-i+1) m[cur] = j j++ } return maxl } func max(a, b int) int { if a \u0026gt; b { return a } else { return b } } // 执行用时: 8 ms // 内存消耗: 3.1 MB 疑问 当元素重复时，为什么 i 要移动到 max(i, m[cur]+1) 处，而不是直接移动到 m[cur]+1 处？\n刚开始写的时候，我就犯了这个错误，写下了如下代码：\nfunc lengthOfLongestSubstring(s string) int { i, j := 0, 0 m := make(map[byte]int) maxl := 0 for j \u0026lt; len(s) { char := s[j] if _, ok := m[char]; ok { i = m[char] + 1 } maxl = max(maxl, j-i) m[char] = j j++ } return maxl } func max(a, b int) int { if a \u0026gt; b { return a } else { return b } } 测试用例：abba，正确结果应该是 2 ，但是上面代码执行却诡异的返回了 3，通过画图分析后才知道了原因所在：\n错误记录 func lengthOfLongestSubstring(s string) int { i, j := 0, 0 m := make(map[byte]int) maxl := 0 for j \u0026lt; len(s) { char := s[j] if _, ok := m[char]; ok { maxl = max(maxl, j-i) i = m[char] + 1 } m[char] = j j++ } return maxl } func max(a, b int) int { if a \u0026gt; b { return a } else { return b } } 测试用例：\nau\t预期：2\t输出：0\n原因：更新最大长度语句写在了 if 内，只有字符重复时才会执行，对于 au 这个没有重复字符的 string，该语言始终不会执行，所以最后会返回 0\n\u0026quot; \u0026quot;\t预期：1\t输出：0\n原因：还是和上面一样的情况。\n总结 第一次做这道题的时候是毫无头绪的，因为当时并不知道滑动窗口是什么东西，在看了题解后，照猫画虎，大致摸清了算法思路，但对于一些特殊语句，比如 if 中的 max(i, m[s[j]]+1)，却没有很好的理解，只是知道这句话不这样写，答案就不会正确。\n这次二刷，则是先大致回想一下之前记忆里的思路，并尝试在不看原来正确代码的情况下，先自己写写，结果就发现了很多问题，一些测试用例无法通过，这时再结合之前的正确代码，比较自己写的代码的缺陷并改正，对于特殊语句也理解清楚了。\n所以这道题给我的感悟就是，做题还是得自己动手实践才能真正理解，只是看别人的代码，很有可能会处于半知半解状态。算法题就是这样，往往你以为自己做对了，实际并没有，就是有几个刁钻的测试用例过不去，只要有 1 个测试过不去，就说明这个代码写的是有问题的，这个时候再结合错误仔细分析，查找发现有问题的地方，就能更好的理解代码。如果只是看别人的，少了发现并改正错误的过程，对代码的理解也会不够透彻，。\n总的来说，在不会做的情况下可以看别人的题解，学习方法和思路，但不能只看不写，要试着将别人的思路转换成自己的。\n","date":"2021年04月16日","permalink":"/posts/jian-zhi-offer-48-zui-chang-bu-han-chong-fu-zi-fu-de-zi-zi-fu-chuan/","summary":"题目描述 请从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。\n示例 1:\n输入: \u0026quot;abcabcbb\u0026quot; 输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;abc\u0026quot;，所以其长度为 3。 示例 2:","title":"剑指 Offer 48. 最长不含重复字符的子字符串"},{"contents":"题目描述 在字符串 s 中找出第一个只出现一次的字符。如果没有，返回一个单空格。 s 只包含小写字母。\n示例:\ns = \u0026quot;abaccdeff\u0026quot; 返回 \u0026quot;b\u0026quot; s = \u0026quot;\u0026quot; 返回 \u0026quot; \u0026quot; 限制：\n0 \u0026lt;= s 的长度 \u0026lt;= 50000 方法1：哈希表 可以将哈希表的 key 定义为 char，用来存储 string 的每个字符；value 定义为 int ，用来记录字符出现的次数，或者定义为 bool，用来标识字符是否重复出现，因为 bool 的方式相对简单明了，也并不需要频繁对变量进行自增操作，所以这里选择 bool，false 代表未重复出现。\n由于 go 的 map 是无序的，遍历出的第一个 value 为 false 的字符，在字符串中未必是第一个，所以只能通过二次遍历字符串，判断每个字符在 map 中对应的值，并返回第一个值为 false 的字符。\n代码如下：\nfunc firstUniqChar(s string) byte { m := make(map[rune]bool) for _, char := range s { if _, ok := m[char]; ok { m[char] = true continue } m[char] = false } for _, char := range s { if !m[char] { return byte(char) } } return 32 } // 执行用时: 48 ms // 内存消耗: 5.3 MB 方法2：使用数组代替哈希表 这是 《剑指offer》 中提供的方法，虽然哈希表可以很好地解决问题，但是对于一个只有小写字母的字符串来说（题目中说明了）来说，杀鸡焉用宰牛刀，可以用一个长度为 26 的数组，建立一个简单的哈希表即可，用字符的 ascii 减去 97 作为下标，值为出现的次数，代码如下：\nfunc firstUniqChar(s string) byte { n := [26]int{} for _, char := range s { n[char-97]++ } for _, char := range s { if n[char-97] == 1 { return byte(char) } } return 32 } // 执行用时: 8 ms // 内存消耗: 5.3 MB 执行用时只有原来的 1/6，这个结果可能不是很准确，但是相对哈希表而言，少了哈希计算、处理哈希冲突等等操作，效率肯定要好一些的。通过这道题也使我明白了，不要无脑使用 map，在一些特殊条件下，往往会有更简单高效的解决方法。\n方法 3： 有序哈希表 如果哈希表有序，则可以遍历哈希表，找到第一个值为 fasle 的字符，但 go 中没有提供有序 map，所以这里使用一个 slice 来按顺序保存 key ，之后遍历这个 slice 即可，代码如下：\nfunc firstUniqChar(s string) byte { sli := make([]rune, 0) m := make(map[rune]bool) for _, char := range s { // map 中没有该 char，则 append 到 slice 中 if _, ok := m[char]; !ok { sli = append(sli, char) } if _, ok := m[char]; ok { m[char] = true continue } m[char] = false } // 遍历 slice for _, char := range sli { if m[char] == false { return byte(char) } } return 32 } // 执行用时: 48 ms // 内存消耗: 5.3 MB 不过该方法看起来并没有显著提高效率。\n错误记录 func firstUniqChar(s string) byte { // 这里错误的将 value 设置为 byte 类型，最大值只有 255，255 + 1 会被重置为 0 // 也就是说如果一个数出现了 257 次，则它的值会变为 1 n := [26]byte{} for _, char := range s { n[char-97]++ } for _, char := range s { if n[char-97] == 1 { return byte(char) } } return 32 } 例如下面这个刁钻的测试用例，它的正确结果应该是 \u0026ldquo;y\u0026rdquo;，但是上面的代码却返回了 \u0026ldquo;n\u0026rdquo;，原因就是 n 出现了 257 次，值变为了 1，而字符串中 n 又比 y 先出现，导致错误地返回了 n。\n\u0026quot;uindrseqbljlhqvlwvgdebeihttirikuahlikgnahvgnptmqwbovmuwesxkvcitcwrwrucsbbfqvldridfviduqvmfcmeiphoqupbitnwdbvevouoaetisdmgvvvwoglwtgjrpcbghxkrkjthetxeexbphbjiehaicuicgnirslhdstgmqcdnlulpdpadjdltfouwhfqicfcqntnpeqaohslwkhbvflxaudembsrsindluthxapnmrqinsivbxmkohubvmmmpmklbfrmeuvdrhptdmelmjjefgbsqqlbqhvsmswwxrlkutadqbbeisbgfrcivvoxmxxptrscxnjjvtajfhqiucdihihcutxhlonlomfdnbwanrcnbarojsajseqrgkuqgkcnvrghxnmbclfomktwfaakodeecsglufvobkgoqsbrhdiuhxqbcndkmxuertupngvnkgwrfwgdbiurbooxariklwarjgavsuddoveipltessrssxwxgvrrtkisnbavkbpaphicxhapjjpkccakepuafjdaswfwfmbdmuuaveukxvnpkgmhcjcpqntssthjlvrngugbhrivtwmbrvrprrlfmvwjdkonfmgnepqoxwfcvefjihisarwskjfmqrjlkbbugfawmkqgfxgpokkxxivqbqimwsccdekjegwcxhmmuhpstxfpofwmrmhrxeptxwbvxsesheijjfsshgrrwckjkmbslpbngnwhokjnujtiepfrdhiwwgbffixaidabacaibummwxgxowsewlqfxrkarjrkqxmxwobqbcoowmbggtpoqadhqdxlhvxrkfuwpxxgnchudreoeuefkqhlrmwwfvjexvbxdhtdngwvoohpjtdbdbesceiafrhenfljleegsencfmbauxlltfueudnjxsiiggsfwiuidgktsbtcvxtdllviocfdbonjciosucbjidwxnogmnveqkcrxbpamfxwxiugjgrfpstromuxxdiodqeoqdlfulttraquskhwfholbrcbchijlgqvuwxvejedikvgetlmrcmeampdgjvmbdovkcjilbralhmniwvbeandldnudkpjvhoqtfdwllridwljfvflfdrbqadvabggwsiexrthrngexpebuhtefkqgkkjoopsmunesfotsprxuaswwenhkdvsspkppulecahkkvccqngeaoijjgvsfqfpvphvhdnkkcqsebhijkvfpqjmarbkpejagtbtisnflrrvawmvfxeccxtrmorihgslxlqrqcmouojjfhcieuwlrcqhevmveookgonxqdbtgqjimsidnaaiuchwkfkpxfptuvhfqemojixqvhgokdekdwbomptqqlfiaiptxvgfmovdqxupjxjpoxuplsagxpgpmvtcpawkrrthvclhdbpqeucchxptdceswhnqocmeocpgthkgxmxggwlnantwwuoqpmnpvgateitxlocmhnihmfgjnvrggenbhnjfubtoteojojjkjpcnwqthxhlfukingjletwnxdnjwrgjopqoxtqcvwsakqxxumbtcblufdmdvxfkshitenmenkxjvblsoiyhjpakinimwxhcebabgsvqftfvwjnstltjawhwipkubadtoxqrutkwxjnmfoowtnvqplvqokcuwlkmxxhboampcdwokjfxggtnojebagxlwaeowtomubtbfsrufkttugfpnxmipkcsphqafjuxovwpcgonhiqmomsweunoeqkpkxxsdksmufowqpmkontccckcdbwrfwamikananakgjkahndrepemcgxecgpltvdbpoexemnrejdephuuxhfcubxlbdrmhvmeqmtdhnbkwnidigxdantmkckijiecavkpumegrbveffclcltmibjcstrrauphfxxssgxkkirapiihnlbrodvfostahqdkajqtrrfwdsemwxlucbbjjspnnqjnpnpmimhulgoskwpsactexmkfdhaihnlggqeunqevxfrpiwskhrhgfluelshqejavomjshfomvgpugqesbtakvxqwrtguuebcgqphocglfrircfvuikfbviomfrsnpvvlftwrkbmjpvdpgvohtmxcxwnuhwojnfvsfwvdlaxxlmafihpussffcawjpaxdwerwfbvsbipljualcnhseealvqiqfiaiskaafufaubvhjglktfhbdsaskmedroxkrxshnggumcbtdcablawkodmnafkjekuiecqlvbxocfcwipaicgndfafjhtjcelakrecmntjxeqjqgxkxapuobbcaxfrpsktjswxdfvugmmgjwiphnsclqwmranthpiueffaxvhplajqsrtoxbixbdpcfkbpkjkelemubwachcpxcniaqmkmasmvlfcubcdkxfkupgcgbrvmbmgnjgfbawmmdritdrkppswatwtdjemhifhmshunkvaivhteqnwkdcrpfxmrafupfhbgligbvrqjkvcgbiqudvtblhfgetffduvsfhsmjimgruxrvbqniluapaoniwhqhltbxvrphmlisfaomqoecmdbbrgujgsbdelkbddcgmpggogfonssxisphbwljlvhwhewmpqugxgbubqfeilobxxhtcxmxvjtbuavxlaghhjiemihvrsjxbleutpuumgndtnocwkpkdaupbmcsahcbwoelmgnwqummmekwhpahtdvehoprfcciwqphfwppscruimikiximhbkdomovbdalsihioncpaclevxawqcqthtmuogolkrvlipropmnmecttmlecpapdlrcqxopfjobgsvhcadqiudqjoscpvjguddnorldkpqtoocrtkcutkvdpbkkekenpqjmxmuxccamkwwxrdrbdmameptvcgsltqdasicoouvdtbranexokmtkioptqblrruaihvfvvebjigkuwwgearxvmimlgmxgpvbnotepprknjxcunheclsvxxmmoufvdhrwxqlrkmvcaxtbiqrjrnrsfrkvttnpxbspnetcofhpcbcphrchivnrskkalsvdllhtisnaerxbcxutcpmnquwrutmmvrtchhheplhdmbfkrrrxalosjbebtdqtsjbdcuuatgldnubigijbkehxsmhsadwtcbwinlnilthiqslvsduspkjuunpetcofujbqhtkmalnjmllhulckqodbgffxdiqjajecbmlekvepkirxnguqnpnfjdptinuphcoxkoqhqrmkkawmngqwvfpcuxgpekkprupplsbwshmsduaitlhcjargacduewtqgnxbtpwrkawpncarxlsnardkftmsdjflmhoplghandjisupacixxthwokctbxekqwbmhtoutgrogxsdnmcaqrsqrpigbigqhsmrksxgkdbdbxkwimpehltcffuhrphbaqgvabjduudbrxgwiljkwijvaugnleamskjijhnvcspagsfnhjielrvxddxqfgaipfflffksmvgaioaclfsjhovcotoaiwhhnmrudsqmsepsgdofnxjjcgdejutmdtgauuvuepajlikebtpgfimudswplfwgaodtwqjuaujlbqqartevsqesmrpknoanfprqcwochqdrcvquovtsogptduvfiaislfvpxwsqitodxwckprpfxtqhbdixklhtsajqgbinfpdqaehrwhnwggmggnbstolrcbmlhpfjurbknpwbrfhvdanrjcwnvsxudagbkhdlwujqvwpbbgmsfrkgutkqwxqfemaaokrfdocsbbktwaugoadgvqcdhmqkworbvxqwdpmbjkqghmapevrsnaaparwnbvcfnhhgfqplfwiswrxxistamulcbxcmvkbcxsgftbmbvqlxrhbmcujkktdvcgnjkfaclxndtjkdbgqvbqxicgmpowbjsfvspvxtrovjwbbkaapfgcqbifdosnmutvfbgdmhiqvflasedhafqnmrlmwbuhmptnkvvojnjngiepmdktewwsuxernpfmktsgwwnxfvmuotpfxfntcrsjpldmringfpkpievvbgbghevlmuticukrfvahrmtkvckjbxxrmkilbeojsxgbvbhsgvkhgtjddoreipbkwiqbkecobfexswiwttlohnfwokixfmfrpadtudrssiobbvhctpbeesiogwhdqojoalxpxikwuheilvsrxbfbnckjeswhkfviihvwrfxkxcomhvsjojhltospooumkstqqnvxmrowlrtufhxkipxnfecchklrqljfwnglxeharemawsbvnuentnapewjhnzbieisjtomdqtjkigisgopmgcfqxcxninrisupchhmfrosxgdtakhrrnbbnxovvbfjgwdgnlxoxswhsfjeijvivqemoekkmrttscthlpglaorarhibrucuhikfkphqmhowloobeumlikqmmlatesupiekwjhdvntnjbakohrbobimmujkbatfxpmgfmmqsgwsffcbenwgdrhlqrsmfghevnjfdlxupihmkptsefubmixhacnxngpjrckxhvhrelolqwevoqklhxbplakmsxdetvpuddmvaodpglmsknbvfftstckjurbntwqenuhcurxqvotrxfpootqkobeatoduihwhpjxxrvboggkcxagnrrfwfuaqplirlnafprumnkcjxlpvanfmqwuoaupqptqeulpwvnhbahbkihustenkbdushjakemufdhjllnowmvgpdxbrhxfonpgcjslvwgdmajicqqmdxubrofvdodfsedjghpbjncgkckeudxwascljlraoodwvgpvojcqgbalntslsfrwnfcdvidsdrvwhscskpalubxeobapkgpdsqcjpwkednraijmbcreplwijofniggavpdlwfmwnvsaridhbppeolakquhamneffnfmbruivassdaaikxiaxupgdpgkfojvkkagcsmqnweofwikvevrsottkbtcldoruakajinnlgxmpddcrmohaktsdrxjelrbmfdthspikxeocqqdordrqwjtaxihswcubtfomksaxddcvtadnrtqkmdnacgduudtdbhsgpfomrdiaotcfwqlxccqeelgtnkflgixpjcriroruhbbgpidwkggdevpfratqebewaxcudseaikqjuuhpsguvtwxniofskkslfnrphatmxngneuituefcxufisrsjitqxwufrgidbwlmkrqojpwpljasaiukwwthrudhcocoguerakhajdncxbrnuavoqeuwsamwdqgutbaoixcgeibpoajhedooqcewiqvddedmanxljjjcbojgbmwabkfbvamgnfpncdcxoaqhmgurifpwpgrpctlqpwmqraknjltneknsphtwbnbiahknxipsovljkivlpggvldeveeopvoqlvfjbratadttlcecomllpkdgiloeedquivsnmxkfcvrkwaohgbrbvjklkktgwtlfgafqgbigheajrvffvvkkmibcedfmnwasopoqgxjohjoqnijeaifuwiwmogkwqlfmibuwecvnulvhkbsukscqlqlsxjulillmlgjkrbmllcunhbqmbujftqgkpwkvemthigednfohxpsduotwfnknfjbexflcfieaosnlhndsorbkcdlahovwbccshmrlcofowrmqajluiqaarnqtxokbaddswftiexiahmstpbonwmhvqgcpmfrocvtwfhjrtsupscfmvwfvaoolanrlgdsvgoseltdnoxrdglockhwlvffaakgrxjfnfbxnbprfvpwmexnhfekjnkenbohhmwlqoteiwgtjsrnceptbgwkfcmtkliwwqskmsoihmnbjsvnmfkwbwemijdtpnajgrousbrdaagenqlgeaiixfgcbfhceaxkwxbpksfouuqcvcerqecpdtvtsubbadmaatdnmnqhladeiapejkxffoagbwqbvppssvvvhlvhntatxlvqgjxbejpvxemqbdjknuogumrwbognklmvjsldrktpeowiuaapbjgktxwfjiferxgmafbcerteqlvpqvxbgdwiufdctkwptadtujmifppudubpmdtiedmqhnihquansnjufpbuumhhmidphkwusjdsdaocavtauhsvtgcoqhufmacwcbxvjmagkounkoqpcnoanhgwsjvgtlwgvbpdbufekosgagfsmadmvbonkrtcbspoabugkcjeebqhqwfjcqlqjvabaqecofgwskqplgup\u0026quot; 总的来说，这是一个愚蠢的错误，但对于我这样的菜鸡来说，也是一个容易忽视的错误，刚开始发现时也确实是比较困惑的，所以特此记录一下。\n","date":"2021年04月16日","permalink":"/posts/jian-zhi-offer-50-di-yi-ge-zhi-chu-xian-yi-ci-de-zi-fu-shu-zhong-wei-35-ti/","summary":"题目描述 在字符串 s 中找出第一个只出现一次的字符。如果没有，返回一个单空格。 s 只包含小写字母。","title":"剑指 Offer 50. 第一个只出现一次的字符（书中为35题）"},{"contents":"题目描述 输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。例如，一个链表有6个节点，从头节点开始，它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个节点是值为4的节点。\n示例：\n给定一个链表: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5, 和 k = 2.\n返回链表 4-\u0026gt;5.\n双指针 算法流程：\n定量两个变量 i 和 j，这两个变量初始都指向链表的头结点 先让 j 向前移动 k 步 i 和 j 同时向前移动 当 j 的值为 null 时，i 就是倒数第 k 个节点 代码如下：\nfunc getKthFromEnd(head *ListNode, k int) *ListNode { h := head i, j := h, h // j 先走 k 步 for i := 0; i \u0026lt; k; i++ { if j != nil { j = j.Next } } // 之后 i 和 j 同时前进 1 步 for j != nil { i = i.Next j = j.Next } return i } ","date":"2021年04月16日","permalink":"/posts/jian-zhi-offer-22-lian-biao-zhong-dao-shu-di-k-ge-jie-dian/","summary":"题目描述 输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。例如，一个链表有6个节点，从头节点开始，它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个节点是值为4的节点。\n示例：\n给定一个链表: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5, 和 k = 2.","title":"剑指 Offer 22. 链表中倒数第k个节点"},{"contents":"题目描述 给定单向链表的头指针和一个要删除的节点的值，定义一个函数删除该节点。\n返回删除后的链表的头节点。\n注意：此题对比原题有改动\n示例 1:\n输入: head = [4,5,1,9], val = 5 输出: [4,1,9] 解释: 给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -\u0026gt; 1 -\u0026gt; 9. 示例 2:\n输入: head = [4,5,1,9], val = 1 输出: [4,5,9] 解释: 给定你链表中值为 1 的第三个节点，那么在调用了你的函数之后，该链表应变为 4 -\u0026gt; 5 -\u0026gt; 9.\n说明：\n题目保证链表中节点的值互不相同 若使用 C 或 C++ 语言，你不需要 free 或 delete 被删除的节点。\n双指针 算法流程：\n定量两个变量 i 和 j，这两个变量初始都指向链表的头结点 先让 j 向前移动 k 步 i 和 j 同时向前移动 当 j 的值为 null 时，i 就是倒数第 k 个节点 代码如下：\nfunc getKthFromEnd(head *ListNode, k int) *ListNode { h := head i, j := h, h // j 先走 k 步 for i := 0; i \u0026lt; k; i++ { if j != nil { j = j.Next } } // 之后 i 和 j 同时前进 1 步 for j != nil { i = i.Next j = j.Next } return i } ","date":"2021年04月16日","permalink":"/posts/jian-zhi-offer-18-shan-chu-lian-biao-de-jie-dian/","summary":"题目描述 给定单向链表的头指针和一个要删除的节点的值，定义一个函数删除该节点。\n返回删除后的链表的头节点。\n注意：此题对比原题有改动\n示例 1:","title":"剑指 Offer 18. 删除链表的节点"},{"contents":" 题目描述 找出数组中重复的数字。\n在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。\n示例 1：\n输入： [2, 3, 1, 0, 2, 5, 3] 输出：2 或 3\n限制：\n2 \u0026lt;= n \u0026lt;= 100000\n方法1：哈希表 看到这种寻找重复元素的题目，首先想到的就是哈希表，通过 map 的 key 可以很容易的找到重复元素：\nfunc findRepeatNumber(nums []int) int { m := make(map[int]struct{}) for i := 0; i \u0026lt; len(nums); i++ { // 如果该 key 已存在，说明重复 if _, ok := m[nums[i]]; ok { return nums[i] } m[nums[i]] = struct{}{} } return -1 } // 执行用时: 40 ms // 内存消耗: 9.6 MB 复杂度分析： 时间复杂度 O(N) ： 遍历数组使用 O(N) ，map 添加与查找元素皆为 O(1) 。 空间复杂度 O(N) ： map 占用 O(N) 大小的额外空间。\n方法2：排序 对数组排序后，看相邻元素是否有相同的：\nfunc findRepeatNumber(nums []int) int { // 先排序 sort.Slice(nums, func(i, j int) bool { return nums[i] \u0026lt; nums[j] }) for i := 0; i \u0026lt; len(nums); i++ { if nums[i] == nums[i+1] { return nums[i] } } return -1 } // 执行用时: 48 ms // 内存消耗: 8.7 MB 复杂度分析： 时间复杂度 O(NlogN) ： 排序的时间复杂度为 O(NlogN) 空间复杂度 O(1) ： 无需额外空间\n方法3：原地置换、原地哈希 题干中有一句很重要的话：在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内，这说明了每个元素都可以存放在其值下标处（例如 2 可以存在 nums[2]，5 可以存在 nums[5]）,我们可以使用某种方法，将每个元素放到其对应位置，实现一个哈希表，哈希表的 key 是数组下标，value 是与下标相同的值，例如：\n当 key 发生冲突时，说明该元素是我们要找的重复数，但是用什么方法使每个元素移动到其对应位置呢?\n这里可以使用原地置换法，首先判断 i 是否等于 nums[i]，不是的话则继续判断 nums[i] 是否等于 nums[nums[i]]，如果相等，则说明发生了冲突，该数为重复数，直接返回，否则交换两个位置的值（swap(nums[i], nums[nums[i]]) ），如果 i 等于 nums[i]，则说明 nums[i] 已经放在了对应的位置，此时可以i++，重复上述步骤，直到 i \u0026gt; len(nums) 或者 return。\n算法流程：\n遍历数组 nums ，设索引初始值为 i = 0 : 若 nums[i] = i ： 说明此数字已在对应索引位置，无需交换，因此跳过； 若 nums[nums[i]] = nums[i] ： 代表索引 nums[i] 处和索引 i 处的元素值都为 nums[i] ，即找 到一组重复值，返回此值 nums[i] ； 否则： 交换索引为 i 和 nums[i] 的元素值，将此数字交换至对应索引位置。 若遍历完毕尚未返回，则返回 -1 。 文字可能不太好理解，结合下面的图帮助理解：\nfunc findRepeatNumber(nums []int) int { i := 0 for i \u0026lt; len(nums) { if i == nums[nums[i]] { i++ } else if nums[i] == nums[nums[i]] { return nums[i] } nums[i], nums[nums[i]] = nums[nums[i]], nums[i] } return -1 } // 执行用时: 36 ms // 内存消耗: 8.7 MB 复杂度分析： 时间复杂度 O(N) ： 遍历数组使用 O(N)，每轮遍历的判断和交换操作使用 O(1)。 空间复杂度 O(1) ： 使用常数复杂度的额外空间。\n","date":"2021年04月16日","permalink":"/posts/jian-zhi-offer-03-shu-zu-zhong-chong-fu-de-shu-zi/","summary":"题目描述 找出数组中重复的数字。\n在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。","title":"剑指 Offer 03. 数组中重复的数字"},{"contents":"题目描述 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。\n获取数据 get(key) - 如果关键字 (key) 存在于缓存中，则获取关键字的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字/值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。\n示例:\nLRUCache cache = new LRUCache( 2 /* 缓存容量 */ );\ncache.put(1, 1); cache.put(2, 2); cache.get(1); // 返回 1 cache.put(3, 3); // 该操作会使得关键字 2 作废 cache.get(2); // 返回 -1 (未找到) cache.put(4, 4); // 该操作会使得关键字 1 作废 cache.get(1); // 返回 -1 (未找到) cache.get(3); // 返回 3 cache.get(4); // 返回 4\n进阶:\n你是否可以在 O(1) 时间复杂度内完成这两种操作？\n上面的操作如下图所示：\n通过上图应该可以大致了解 lru 的工作过程了，要想进一步了解 lru，可以自行网上查阅。\n算法设计 低效率版 如果忽视题目描述中的进阶要求：在 O(1) 时间复杂度内完成这两种操作，可以很容易的写出一个 lru 算法，只要通过一个链表，在链表中存储 map 即可实现，\n// ⚠️ 低效率，get 和 put 时间复杂度非 O(1) type LRUCache struct { Cap int Element *list.List } func Constructor(capacity int) LRUCache { l := LRUCache{ Cap: capacity, Element: list.New(), } return l } func (c *LRUCache) Get(key int) int { for i := c.Element.Front(); i != nil; i = i.Next() { data := i.Value.(map[int]int) if val, ok := data[key]; ok { // 如果找到，返回的同时将其置于头部 c.Element.MoveToFront(i) return val } } return -1 } func (c *LRUCache) Put(key int, value int) { e := c.Element // 如果 key 已存在，则更新值，同时置于头部 for i := e.Front(); i != nil; i = i.Next() { data := i.Value.(map[int]int) if _, ok := data[key]; ok { data[key] = value // 置于头部 c.Element.MoveToFront(i) return } } // 如果已满，则移除最后一个元素 if e.Len() == c.Cap { e.Remove(e.Back()) } data := make(map[int]int) data[key] = value e.PushFront(data) } 虽然通过了测试，但是效率非常低下，因为每次 get 和 put 都需要遍历链表，并且判断当前链表存储的 map 的 key，时间复杂度会非常高。\n更好的方法：\n要让 put 和 get 方法的时间复杂度为 O(1)，我们可以总结出这个数据结构必要的条件：查找快，插入快，删除快，有顺序之分。\n因为显然 cache 必须有顺序之分，以区分最近使用的和久未使用的数据；而且我们要在 cache 中查找键是否已存在；如果容量满了要删除最后一个数据；每次访问还要把数据插入到队头。\n那么，什么数据结构同时符合上述条件呢？哈希表查找快，但是数据无固定顺序；链表有顺序之分，插入删除快，但是查找慢。所以结合一下，形成一种新的数据结构：哈希链表。\n双链表中存储了一对键值对，并且 键 和 哈希表的 键 保持一致，来达到 哈希表映射双链表的目的。\n哈希表可以使查询相关操作达到 O(1) 时间复杂度，例如 get 操作，以及 put 中 判断该 key 是否以及存在；通过哈希表可以快速找到对应的链表节点，所以可以保证链表的删除操作也为 O(1) （链表删除虽然只需要改变指针，但是还需要找到该节点）。\n具体代码如下：\ntype LRUCache struct { // 最大容量 cap int // map 映射到 双向链表，实现 O(1) 的时间复杂度 cache map[int]*list.Element // 双向链表实现 lru list *list.List } // 该结构体作为链表的节点值，并且 key 与 cache 的 key 保持同步， // 以此实现 map 映射到 list type kv struct { key int value int } func Constructor(capacity int) LRUCache { return LRUCache{ cap: capacity, cache: make(map[int]*list.Element), list: list.New(), } } func (c *LRUCache) Get(key int) int { // 通过 cache (map 类型) 查询 key，因为 list 节点 value 的 key 与 cache key 相同， // 所以可以通过 cache 以 O(1) 查询到对应的 list node if v, ok := c.cache[key]; ok { // 将查询到的 node 移动到 list 头部 c.list.MoveToFront(v) // cache 的 value 是一个 list node，并且 node value 是 struct kv， // 返回 struct kv 的 value return v.Value.(*kv).value } return -1 } func (c *LRUCache) Put(key int, value int) { // 如果该 key 已经存在，则更新 value，并置于 list 头部 if v, ok := c.cache[key]; ok { // 更新位置 c.list.MoveToFront(v) // 更新 value v.Value = \u0026amp;kv{ key: key, value: value, } // 该 key 不存在 } else { // 如果已经达到最大容量 if c.list.Len() == c.cap { // 删除 cache 中处于末尾的 key delete(c.cache, c.list.Back().Value.(*kv).key) // 删除 list 中最后一个 node c.list.Remove(c.list.Back()) } // 还有剩余容量 e := \u0026amp;kv{ key: key, value: value, } // 置于 list 头部 c.list.PushFront(e) // value 取 list 首元素 c.cache[key] = c.list.Front() } } 可以看到执行耗时缩短了近 7 倍。\n注：go 官方有一个分布式 kv缓存库，里面有 lru 的实现方式，地址如下：\nhttps://github.com/golang/groupcache/blob/master/lru/lru.go\n","date":"2021年04月16日","permalink":"/posts/leetcode146lru-huan-cun-ji-zhi/","summary":"题目描述 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。","title":"LeetCode146.LRU 缓存机制"},{"contents":"docker 安装 etcd 创建一个 sh 文件\n粘贴下面内容\nrm -rf /tmp/etcd-data.tmp \u0026amp;\u0026amp; mkdir -p /tmp/etcd-data.tmp \u0026amp;\u0026amp; \\ # docker rmi quay.io/coreos/etcd:v3.3.13 || true \u0026amp;\u0026amp; \\ docker run -d \\ -p 2379:2379 \\ -p 2380:2380 \\ --mount type=bind,source=/tmp/etcd-data.tmp,destination=/etcd-data \\ --name etcd-gcr-v3.3.13 \\ quay.io/coreos/etcd:v3.3.13 \\ /usr/local/bin/etcd \\ --name s1 \\ --data-dir /etcd-data \\ --listen-client-urls http://0.0.0.0:2379 \\ --advertise-client-urls http://0.0.0.0:2379 \\ --listen-peer-urls http://0.0.0.0:2380 \\ --initial-advertise-peer-urls http://0.0.0.0:2380 \\ --initial-cluster s1=http://0.0.0.0:2380 \\ --initial-cluster-token tkn \\ --initial-cluster-state new 执行\n附：进入容器的命令\ndocker exec -it 容器id /usr/local/bin/etcdctl ","date":"2020年11月06日","permalink":"/posts/2020-11-6-docker-etcd/","summary":"docker 安装 etcd 创建一个 sh 文件","title":"docker 安装 etcd"},{"contents":"使用docker安装redis，挂载外部配置和数据 mkdir /docker\nmkdir /docker/redis\nmkdir /docker/redis/conf\nmkdir /docker/redis/data\n创建redis.conf配置文件\ntouch /docker/redis/conf/redis.conf\ncd 到conf下，vi redis.conf\ni进入插入模式，粘贴配置文件，配置文件中需将daemonize设置为no，#requirepass xx 去掉注释，并设置密码（xx处即为要设置的密码），protected-mode改为no，bind注释掉。\nesc :wq保存\ndocker run -d \u0026ndash;privileged=true -p 6379:6379 -v /docker/redis/conf/redis.conf:/etc/redis/redis.conf -v /docker/redis/data:/data \u0026ndash;name redis redis redis-server /etc/redis/redis.conf \u0026ndash;appendonly yes\nmac下：\ndocker run -d \u0026ndash;privileged=true -p 6379:6379 -v /usr/local/redis/conf/redis.conf:/etc/redis/redis.conf -v /usr/local/redis/data:/data \u0026ndash;name redis redis redis-server /etc/redis/redis.conf \u0026ndash;appendonly yes\n创建挂载外部配置的容器\n参数说明：\n\u0026ndash;privileged=true：容器内的root拥有真正root权限，否则容器内root只是外部普通用户权限\n-v /docker/redis/conf/redis.conf:/etc/redis/redis.conf：映射配置文件\n-v /docker/redis/data:/data：映射数据目录\nredis-server /etc/redis/redis.conf：指定配置文件启动redis-server进程\n\u0026ndash;appendonly yes：开启数据持久化\n附：已经修改好的配置文件\n# Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k =\u0026gt; 1000 bytes # 1kb =\u0026gt; 1024 bytes # 1m =\u0026gt; 1000000 bytes # 1mb =\u0026gt; 1024*1024 bytes # 1g =\u0026gt; 1000000000 bytes # 1gb =\u0026gt; 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \u0026quot;include\u0026quot; won't be rewritten by command \u0026quot;CONFIG REWRITE\u0026quot; # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so ################################## NETWORK ##################################### # By default, if no \u0026quot;bind\u0026quot; configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \u0026quot;bind\u0026quot; configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 loopback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # bind 127.0.0.1 ::1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \u0026quot;bind\u0026quot; directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \u0026quot;bind\u0026quot; directive. protected-mode no # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 ################################# TLS/SSL ##################################### # By default, TLS/SSL is disabled. To enable it, the \u0026quot;tls-port\u0026quot; configuration # directive can be used to define TLS-listening ports. To enable TLS on the # default port, use: # # port 0 # tls-port 6379 # Configure a X.509 certificate and private key to use for authenticating the # server to connected clients, masters or cluster peers. These files should be # PEM formatted. # # tls-cert-file redis.crt # tls-key-file redis.key # Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange: # # tls-dh-params-file redis.dh # Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL # clients and peers. Redis requires an explicit configuration of at least one # of these, and will not implicitly use the system wide configuration. # # tls-ca-cert-file ca.crt # tls-ca-cert-dir /etc/ssl/certs # By default, clients (including replica servers) on a TLS port are required # to authenticate using valid client side certificates. # # If \u0026quot;no\u0026quot; is specified, client certificates are not required and not accepted. # If \u0026quot;optional\u0026quot; is specified, client certificates are accepted and must be # valid if provided, but are not required. # # tls-auth-clients no # tls-auth-clients optional # By default, a Redis replica does not attempt to establish a TLS connection # with its master. # # Use the following directive to enable TLS on replication links. # # tls-replication yes # By default, the Redis Cluster bus uses a plain TCP connection. To enable # TLS for the bus protocol, use the following directive: # # tls-cluster yes # Explicitly specify TLS versions to support. Allowed values are case insensitive # and include \u0026quot;TLSv1\u0026quot;, \u0026quot;TLSv1.1\u0026quot;, \u0026quot;TLSv1.2\u0026quot;, \u0026quot;TLSv1.3\u0026quot; (OpenSSL \u0026gt;= 1.1.1) or # any combination. To enable only TLSv1.2 and TLSv1.3, use: # # tls-protocols \u0026quot;TLSv1.2 TLSv1.3\u0026quot; # Configure allowed ciphers. See the ciphers(1ssl) manpage for more information # about the syntax of this string. # # Note: this configuration applies only to \u0026lt;= TLSv1.2. # # tls-ciphers DEFAULT:!MEDIUM # Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more # information about the syntax of this string, and specifically for TLSv1.3 # ciphersuites. # # tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256 # When choosing a cipher, use the server's preference instead of the client # preference. By default, the server follows the client's preference. # # tls-prefer-server-ciphers yes # By default, TLS session caching is enabled to allow faster and less expensive # reconnections by clients that support it. Use the following directive to disable # caching. # # tls-session-caching no # Change the default number of TLS sessions cached. A zero value sets the cache # to unlimited size. The default size is 20480. # # tls-session-cache-size 5000 # Change the default timeout of cached TLS sessions. The default timeout is 300 # seconds. # # tls-session-cache-timeout 60 ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /usr/local/var/run/redis.pid when daemonized. daemonize no # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \u0026quot;process is ready.\u0026quot; # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \u0026quot;/usr/local/var/run/redis.pid\u0026quot;. # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \u0026quot;\u0026quot; # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT \u0026lt;dbid\u0026gt; where # dbid is a number between 0 and 'databases'-1 databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \u0026quot;save\u0026quot; lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \u0026quot;\u0026quot; save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that's set to 'yes' as it's almost always a win. # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # Remove RDB files used by replication in instances without persistence # enabled. By default this option is disabled, however there are environments # where for regulations or other security concerns, RDB files persisted on # disk by masters in order to feed replicas, or stored on disk by replicas # in order to load them for the initial synchronization, should be deleted # ASAP. Note that this option ONLY WORKS in instances that have both AOF # and RDB persistence disabled, otherwise is completely ignored. # # An alternative (and sometimes better) way to obtain the same effect is # to use diskless replication on both master and replicas instances. However # in the case of replicas, diskless is not always an option. rdb-del-sync-files no # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir /usr/local/var/db/redis/ ################################# REPLICATION ################################# # Master-Replica replication. Use replicaof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # +------------------+ +---------------+ # | Master | ---\u0026gt; | Replica | # | (receive writes) | | (exact copy) | # +------------------+ +---------------+ # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of replicas. # 2) Redis replicas are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition replicas automatically try to reconnect to masters # and resynchronize with them. # # replicaof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt; # If the master is password protected (using the \u0026quot;requirepass\u0026quot; configuration # directive below) it is possible to tell the replica to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the replica request. # # masterauth \u0026lt;master-password\u0026gt; # # However this is not enough if you are using Redis ACLs (for Redis version # 6 or greater), and the default user is not capable of running the PSYNC # command and/or other commands needed for replication. In this case it's # better to configure a special user to use with replication, and specify the # masteruser configuration as such: # # masteruser \u0026lt;username\u0026gt; # # When masteruser is specified, the replica will authenticate against its # master using the new AUTH form: AUTH \u0026lt;username\u0026gt; \u0026lt;password\u0026gt;. # When a replica loses its connection with the master, or when the replication # is still in progress, the replica can act in two different ways: # # 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if replica-serve-stale-data is set to 'no' the replica will reply with # an error \u0026quot;SYNC with master in progress\u0026quot; to all the kind of commands # but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, # SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, # COMMAND, POST, HOST: and LATENCY. # replica-serve-stale-data yes # You can configure a replica instance to accept writes or not. Writing against # a replica instance may be useful to store some ephemeral data (because data # written on a replica will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default replicas are read-only. # # Note: read only replicas are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only replica exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only replicas using 'rename-command' to shadow all the # administrative / dangerous commands. replica-read-only yes # Replication SYNC strategy: disk or socket. # # New replicas and reconnecting replicas that are not able to continue the # replication process just receiving differences, need to do what is called a # \u0026quot;full synchronization\u0026quot;. An RDB file is transmitted from the master to the # replicas. # # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the replicas incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to replica sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more replicas # can be queued and served with the RDB file as soon as the current child # producing the RDB file finishes its work. With diskless replication instead # once the transfer starts, new replicas arriving will be queued and a new # transfer will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple # replicas will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the replicas. # # This is important since once the transfer starts, it is not possible to serve # new replicas arriving, that will be queued for the next RDB transfer, so the # server waits a delay in order to let more replicas arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # ----------------------------------------------------------------------------- # WARNING: RDB diskless load is experimental. Since in this setup the replica # does not immediately store an RDB on disk, it may cause data loss during # failovers. RDB diskless load + Redis modules not handling I/O reads may also # cause Redis to abort in case of I/O errors during the initial synchronization # stage with the master. Use only if your do what you are doing. # ----------------------------------------------------------------------------- # # Replica can load the RDB it reads from the replication link directly from the # socket, or store the RDB to a file and read that file after it was completely # recived from the master. # # In many cases the disk is slower than the network, and storing and loading # the RDB file may increase replication time (and even increase the master's # Copy on Write memory and salve buffers). # However, parsing the RDB file directly from the socket may mean that we have # to flush the contents of the current database before the full rdb was # received. For this reason we have the following options: # # \u0026quot;disabled\u0026quot; - Don't use diskless load (store the rdb file to the disk first) # \u0026quot;on-empty-db\u0026quot; - Use diskless load only when it is completely safe. # \u0026quot;swapdb\u0026quot; - Keep a copy of the current db contents in RAM while parsing # the data directly from the socket. note that this requires # sufficient memory, if you don't have it, you risk an OOM kill. repl-diskless-load disabled # Replicas send PINGs to server in a predefined interval. It's possible to # change this interval with the repl_ping_replica_period option. The default # value is 10 seconds. # # repl-ping-replica-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of replica. # 2) Master timeout from the point of view of replicas (data, pings). # 3) Replica timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-replica-period otherwise a timeout will be detected # every time there is low traffic between the master and the replica. # # repl-timeout 60 # Disable TCP_NODELAY on the replica socket after SYNC? # # If you select \u0026quot;yes\u0026quot; Redis will use a smaller number of TCP packets and # less bandwidth to send data to replicas. But this can add a delay for # the data to appear on the replica side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \u0026quot;no\u0026quot; the delay for data to appear on the replica side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and replicas are many hops away, turning this to \u0026quot;yes\u0026quot; may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # replica data when replicas are disconnected for some time, so that when a # replica wants to reconnect again, often a full resync is not needed, but a # partial resync is enough, just passing the portion of data the replica # missed while disconnected. # # The bigger the replication backlog, the longer the time the replica can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a replica connected. # # repl-backlog-size 1mb # After a master has no longer connected replicas for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last replica disconnected, for # the backlog buffer to be freed. # # Note that replicas never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \u0026quot;partially # resynchronize\u0026quot; with the replicas: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The replica priority is an integer number published by Redis in the INFO # output. It is used by Redis Sentinel in order to select a replica to promote # into a master if the master is no longer working correctly. # # A replica with a low priority number is considered better for promotion, so # for instance if there are three replicas with priority 10, 100, 25 Sentinel # will pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the replica as not able to perform the # role of master, so a replica with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. replica-priority 100 # It is possible for a master to stop accepting writes if there are less than # N replicas connected, having a lag less or equal than M seconds. # # The N replicas need to be in \u0026quot;online\u0026quot; state. # # The lag in seconds, that must be \u0026lt;= the specified value, is calculated from # the last ping received from the replica, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough replicas # are available, to the specified number of seconds. # # For example to require at least 3 replicas with a lag \u0026lt;= 10 seconds use: # # min-replicas-to-write 3 # min-replicas-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-replicas-to-write is set to 0 (feature disabled) and # min-replicas-max-lag is set to 10. # A Redis master is able to list the address and port of the attached # replicas in different ways. For example the \u0026quot;INFO replication\u0026quot; section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover replica instances. # Another place where this info is available is in the output of the # \u0026quot;ROLE\u0026quot; command of a master. # # The listed IP and address normally reported by a replica is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the replica to connect with the master. # # Port: The port is communicated by the replica during the replication # handshake, and is normally the port that the replica is using to # listen for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the replica may be actually reachable via different IP and port # pairs. The following two options can be used by a replica in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # replica-announce-ip 5.5.5.5 # replica-announce-port 1234 ############################### KEYS TRACKING ################################# # Redis implements server assisted support for client side caching of values. # This is implemented using an invalidation table that remembers, using # 16 millions of slots, what clients may have certain subsets of keys. In turn # this is used in order to send invalidation messages to clients. Please # to understand more about the feature check this page: # # https://redis.io/topics/client-side-caching # # When tracking is enabled for a client, all the read only queries are assumed # to be cached: this will force Redis to store information in the invalidation # table. When keys are modified, such information is flushed away, and # invalidation messages are sent to the clients. However if the workload is # heavily dominated by reads, Redis could use more and more memory in order # to track the keys fetched by many clients. # # For this reason it is possible to configure a maximum fill value for the # invalidation table. By default it is set to 1M of keys, and once this limit # is reached, Redis will start to evict keys in the invalidation table # even if they were not modified, just to reclaim memory: this will in turn # force the clients to invalidate the cached values. Basically the table # maximum size is a trade off between the memory you want to spend server # side to track information about who cached what, and the ability of clients # to retain cached objects in memory. # # If you set the value to 0, it means there are no limits, and Redis will # retain as many keys as needed in the invalidation table. # In the \u0026quot;stats\u0026quot; INFO section, you can find information about the number of # keys in the invalidation table at every given moment. # # Note: when key tracking is used in broadcasting mode, no memory is used # in the server side so this setting is useless. # # tracking-table-max-keys 1000000 ################################## SECURITY ################################### # Warning: since Redis is pretty fast an outside user can try up to # 1 million passwords per second against a modern box. This means that you # should use very strong passwords, otherwise they will be very easy to break. # Note that because the password is really a shared secret between the client # and the server, and should not be memorized by any human, the password # can be easily a long string from /dev/urandom or whatever, so by using a # long and unguessable password no brute force attack will be possible. # Redis ACL users are defined in the following format: # # user \u0026lt;username\u0026gt; ... acl rules ... # # For example: # # user worker +@list +@connection ~jobs:* on \u0026gt;ffa9203c493aa99 # # The special username \u0026quot;default\u0026quot; is used for new connections. If this user # has the \u0026quot;nopass\u0026quot; rule, then new connections will be immediately authenticated # as the \u0026quot;default\u0026quot; user without the need of any password provided via the # AUTH command. Otherwise if the \u0026quot;default\u0026quot; user is not flagged with \u0026quot;nopass\u0026quot; # the connections will start in not authenticated state, and will require # AUTH (or the HELLO command AUTH option) in order to be authenticated and # start to work. # # The ACL rules that describe what an user can do are the following: # # on Enable the user: it is possible to authenticate as this user. # off Disable the user: it's no longer possible to authenticate # with this user, however the already authenticated connections # will still work. # +\u0026lt;command\u0026gt; Allow the execution of that command # -\u0026lt;command\u0026gt; Disallow the execution of that command # +@\u0026lt;category\u0026gt; Allow the execution of all the commands in such category # with valid categories are like @admin, @set, @sortedset, ... # and so forth, see the full list in the server.c file where # the Redis command table is described and defined. # The special category @all means all the commands, but currently # present in the server, and that will be loaded in the future # via modules. # +\u0026lt;command\u0026gt;|subcommand Allow a specific subcommand of an otherwise # disabled command. Note that this form is not # allowed as negative like -DEBUG|SEGFAULT, but # only additive starting with \u0026quot;+\u0026quot;. # allcommands Alias for +@all. Note that it implies the ability to execute # all the future commands loaded via the modules system. # nocommands Alias for -@all. # ~\u0026lt;pattern\u0026gt; Add a pattern of keys that can be mentioned as part of # commands. For instance ~* allows all the keys. The pattern # is a glob-style pattern like the one of KEYS. # It is possible to specify multiple patterns. # allkeys Alias for ~* # resetkeys Flush the list of allowed keys patterns. # \u0026gt;\u0026lt;password\u0026gt; Add this passowrd to the list of valid password for the user. # For example \u0026gt;mypass will add \u0026quot;mypass\u0026quot; to the list. # This directive clears the \u0026quot;nopass\u0026quot; flag (see later). # \u0026lt;\u0026lt;password\u0026gt; Remove this password from the list of valid passwords. # nopass All the set passwords of the user are removed, and the user # is flagged as requiring no password: it means that every # password will work against this user. If this directive is # used for the default user, every new connection will be # immediately authenticated with the default user without # any explicit AUTH command required. Note that the \u0026quot;resetpass\u0026quot; # directive will clear this condition. # resetpass Flush the list of allowed passwords. Moreover removes the # \u0026quot;nopass\u0026quot; status. After \u0026quot;resetpass\u0026quot; the user has no associated # passwords and there is no way to authenticate without adding # some password (or setting it as \u0026quot;nopass\u0026quot; later). # reset Performs the following actions: resetpass, resetkeys, off, # -@all. The user returns to the same state it has immediately # after its creation. # # ACL rules can be specified in any order: for instance you can start with # passwords, then flags, or key patterns. However note that the additive # and subtractive rules will CHANGE MEANING depending on the ordering. # For instance see the following example: # # user alice on +@all -DEBUG ~* \u0026gt;somepassword # # This will allow \u0026quot;alice\u0026quot; to use all the commands with the exception of the # DEBUG command, since +@all added all the commands to the set of the commands # alice can use, and later DEBUG was removed. However if we invert the order # of two ACL rules the result will be different: # # user alice on -DEBUG +@all ~* \u0026gt;somepassword # # Now DEBUG was removed when alice had yet no commands in the set of allowed # commands, later all the commands are added, so the user will be able to # execute everything. # # Basically ACL rules are processed left-to-right. # # For more information about ACL configuration please refer to # the Redis web site at https://redis.io/topics/acl # ACL LOG # # The ACL Log tracks failed commands and authentication events associated # with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below. acllog-max-len 128 # Using an external ACL file # # Instead of configuring users here in this file, it is possible to use # a stand-alone file just listing users. The two methods cannot be mixed: # if you configure users here and at the same time you activate the exteranl # ACL file, the server will refuse to start. # # The format of the external ACL user file is exactly the same as the # format that is used inside redis.conf to describe users. # # aclfile /etc/redis/users.acl # IMPORTANT NOTE: starting with Redis 6 \u0026quot;requirepass\u0026quot; is just a compatiblity # layer on top of the new ACL system. The option effect will be just setting # the password for the default user. Clients will still authenticate using # AUTH \u0026lt;password\u0026gt; as usually, or more explicitly with AUTH default \u0026lt;password\u0026gt; # if they follow the new protocol: both will work. # requirepass zxvf666 # Command renaming (DEPRECATED). # # ------------------------------------------------------------------------ # WARNING: avoid using this option if possible. Instead use ACLs to remove # commands from the default user, and put them only in some admin user you # create for administrative purposes. # ------------------------------------------------------------------------ # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \u0026quot;\u0026quot; # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to replicas may cause problems. ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # # IMPORTANT: When Redis Cluster is used, the max number of connections is also # shared with the cluster bus: every node in the cluster will use two # connections, one incoming and another outgoing. It is important to size the # limit accordingly in case of very large clusters. # # maxclients 10000 ############################## MEMORY MANAGEMENT ################################ # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have replicas attached to an instance with maxmemory on, # the size of the output buffers needed to feed the replicas are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of replicas is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have replicas attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for replica # output buffers (but this is not needed if the policy is 'noeviction'). # # maxmemory \u0026lt;bytes\u0026gt; # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select one from the following behaviors: # # volatile-lru -\u0026gt; Evict using approximated LRU, only keys with an expire set. # allkeys-lru -\u0026gt; Evict any key using approximated LRU. # volatile-lfu -\u0026gt; Evict using approximated LFU, only keys with an expire set. # allkeys-lfu -\u0026gt; Evict any key using approximated LFU. # volatile-random -\u0026gt; Remove a random key having an expire set. # allkeys-random -\u0026gt; Remove a random key, any key. # volatile-ttl -\u0026gt; Remove the key with the nearest expire time (minor TTL) # noeviction -\u0026gt; Don't evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5 # Starting from Redis 5, by default a replica will ignore its maxmemory setting # (unless it is promoted to master after a failover or manually). It means # that the eviction of keys will be just handled by the master, sending the # DEL commands to the replica as keys evict in the master side. # # This behavior ensures that masters and replicas stay consistent, and is usually # what you want, however if your replica is writable, or you want the replica # to have a different memory setting, and you are sure all the writes performed # to the replica are idempotent, then you may change this default (but be sure # to understand what you are doing). # # Note that since the replica by default does not evict, it may end using more # memory than the one set via maxmemory (there are certain buffers that may # be larger on the replica, or data structures may sometimes take more memory # and so forth). So make sure you monitor your replicas and make sure they # have enough memory to never hit a real out-of-memory condition before the # master hits the configured maxmemory setting. # # replica-ignore-maxmemory yes # Redis reclaims expired keys in two ways: upon access when those keys are # found to be expired, and also in background, in what is called the # \u0026quot;active expire key\u0026quot;. The key space is slowly and interactively scanned # looking for expired keys to reclaim, so that it is possible to free memory # of keys that are expired and will never be accessed again in a short time. # # The default effort of the expire cycle will try to avoid having more than # ten percent of expired keys still in memory, and will try to avoid consuming # more than 25% of total memory and to add latency to the system. However # it is possible to increase the expire \u0026quot;effort\u0026quot; that is normally set to # \u0026quot;1\u0026quot;, to a greater value, up to the value \u0026quot;10\u0026quot;. At its maximum value the # system will use more CPU, longer cycles (and technically may introduce # more latency), and will tollerate less already expired keys still present # in the system. It's a tradeoff betweeen memory, CPU and latecy. # # active-expire-effort 1 ############################# LAZY FREEING #################################### # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 4) During replication, when a replica performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transferred. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives. lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no # It is also possible, for the case when to replace the user code DEL calls # with UNLINK calls is not easy, to modify the default behavior of the DEL # command to act exactly like UNLINK, using the following configuration # directive: lazyfree-lazy-user-del no ################################ THREADED I/O ################################# # Redis is mostly single threaded, however there are certain threaded # operations such as UNLINK, slow I/O accesses and other things that are # performed on side threads. # # Now it is also possible to handle Redis clients socket reads and writes # in different I/O threads. Since especially writing is so slow, normally # Redis users use pipelining in order to speedup the Redis performances per # core, and spawn multiple instances in order to scale more. Using I/O # threads it is possible to easily speedup two times Redis without resorting # to pipelining nor sharding of the instance. # # By default threading is disabled, we suggest enabling it only in machines # that have at least 4 or more cores, leaving at least one spare core. # Using more than 8 threads is unlikely to help much. We also recommend using # threaded I/O only if you actually have performance problems, with Redis # instances being able to use a quite big percentage of CPU time, otherwise # there is no point in using this feature. # # So for instance if you have a four cores boxes, try to use 2 or 3 I/O # threads, if you have a 8 cores, try to use 6 threads. In order to # enable I/O threads use the following configuration directive: # # io-threads 4 # # Setting io-threads to 1 will just use the main thread as usually. # When I/O threads are enabled, we only use threads for writes, that is # to thread the write(2) syscall and transfer the client buffers to the # socket. However it is also possible to enable threading of reads and # protocol parsing using the following configuration directive, by setting # it to yes: # # io-threads-do-reads no # # Usually threading reads doesn't help much. # # NOTE 1: This configuration directive cannot be changed at runtime via # CONFIG SET. Aso this feature currently does not work when SSL is # enabled. # # NOTE 2: If you want to test the Redis speedup using redis-benchmark, make # sure you also run the benchmark itself in threaded mode, using the # --threads option to match the number of Redis theads, otherwise you'll not # be able to notice the improvements. ############################ KERNEL OOM CONTROL ############################## # On Linux, it is possible to hint the kernel OOM killer on what processes # should be killed first when out of memory. # # Enabling this feature makes Redis actively control the oom_score_adj value # for all its processes, depending on their role. The default scores will # attempt to have background child processes killed before all others, and # replicas killed before masters. oom-score-adj no # When oom-score-adj is used, this directive controls the specific values used # for master, replica and background child processes. Values range -1000 to # 1000 (higher means more likely to be killed). # # Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities) # can freely increase their value, but not decrease it below its initial # settings. # # Values are used relative to the initial value of oom_score_adj when the server # starts. Because typically the initial value is 0, they will often match the # absolute values. oom-score-adj-values 0 200 800 ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \u0026quot;appendonly.aof\u0026quot;) appendfilename \u0026quot;appendonly.aof\u0026quot; # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don't fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \u0026quot;everysec\u0026quot;, as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # \u0026quot;no\u0026quot; that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use \u0026quot;always\u0026quot; that's very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \u0026quot;everysec\u0026quot;. # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \u0026quot;appendfsync none\u0026quot;. In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \u0026quot;yes\u0026quot;. Otherwise leave it as # \u0026quot;no\u0026quot; that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \u0026quot;redis-check-aof\u0026quot; utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \u0026quot;REDIS\u0026quot; # string and loads the prefixed RDB file, and continues loading the AOF # tail. aof-use-rdb-preamble yes ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 ################################ REDIS CLUSTER ############################### # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A replica of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a replica to actually have an exact measure of # its \u0026quot;data age\u0026quot;, so the following two checks are performed: # # 1) If there are multiple replicas able to failover, they exchange messages # in order to try to give an advantage to the replica with the best # replication offset (more data from the master processed). # Replicas will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single replica computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \u0026quot;connected\u0026quot; state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the replica will not try to failover # at all. # # The point \u0026quot;2\u0026quot; can be tuned by user. Specifically a replica will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * replica-validity-factor) + repl-ping-replica-period # # So for example if node-timeout is 30 seconds, and the replica-validity-factor # is 10, and assuming a default repl-ping-replica-period of 10 seconds, the # replica will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large replica-validity-factor may allow replicas with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a replica at all. # # For maximum availability, it is possible to set the replica-validity-factor # to a value of 0, which means, that replicas will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-replica-validity-factor 10 # Cluster replicas are able to migrate to orphaned masters, that are masters # that are left without working replicas. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working replicas. # # Replicas migrate to orphaned masters only if there are still at least a # given number of other working replicas for their old master. This number # is the \u0026quot;migration barrier\u0026quot;. A migration barrier of 1 means that a replica # will migrate only if there is at least 1 other working replica for its master # and so forth. It usually reflects the number of replicas you want for every # master in your cluster. # # Default is 1 (replicas migrate only if their masters remain with at least # one replica). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # This option, when set to yes, prevents replicas from trying to failover its # master during master failures. However the master can still perform a # manual failover, if forced to do so. # # This is useful in different scenarios, especially in the case of multiple # data center operations, where we want one side to never be promoted if not # in the case of a total DC failure. # # cluster-replica-no-failover no # This option, when set to yes, allows nodes to serve read traffic while the # the cluster is in a down state, as long as it believes it owns the slots. # # This is useful for two cases. The first case is for when an application # doesn't require consistency of data during node failures or network partitions. # One example of this is a cache, where as long as the node has the data it # should be able to serve it. # # The second use case is for configurations that don't meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the # entire cluster without this option set, with it set there is only a write outage. # Without a quorum of masters, slot ownership will not change automatically. # # cluster-allow-reads-when-down no # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \u0026quot;CONFIG SET latency-monitor-threshold \u0026lt;milliseconds\u0026gt;\u0026quot; if needed. latency-monitor-threshold 0 ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \u0026quot;foo\u0026quot; stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@\u0026lt;db\u0026gt;__ prefix. # E Keyevent events, published with __keyevent@\u0026lt;db\u0026gt;__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # t Stream commands # m Key-miss events (Note: It is not included in the 'A' class) # A Alias for g$lshzxet, so that the \u0026quot;AKE\u0026quot; string means all the events # (Except key-miss events which are excluded from 'A' due to their # unique nature). # # The \u0026quot;notify-keyspace-events\u0026quot; takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don't need # this feature and the feature has some overhead. Note that if you don't # specify at least one of K or E, no events will be delivered. notify-keyspace-events \u0026quot;\u0026quot; ############################### GOPHER SERVER ################################# # Redis contains an implementation of the Gopher protocol, as specified in # the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt). # # The Gopher protocol was very popular in the late '90s. It is an alternative # to the web, and the implementation both server and client side is so simple # that the Redis server has just 100 lines of code in order to implement this # support. # # What do you do with Gopher nowadays? Well Gopher never *really* died, and # lately there is a movement in order for the Gopher more hierarchical content # composed of just plain text documents to be resurrected. Some want a simpler # internet, others believe that the mainstream internet became too much # controlled, and it's cool to create an alternative space for people that # want a bit of fresh air. # # Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol # as a gift. # # --- HOW IT WORKS? --- # # The Redis Gopher support uses the inline protocol of Redis, and specifically # two kind of inline requests that were anyway illegal: an empty request # or any request that starts with \u0026quot;/\u0026quot; (there are no Redis commands starting # with such a slash). Normal RESP2/RESP3 requests are completely out of the # path of the Gopher protocol implementation and are served as usually as well. # # If you open a connection to Redis when Gopher is enabled and send it # a string like \u0026quot;/foo\u0026quot;, if there is a key named \u0026quot;/foo\u0026quot; it is served via the # Gopher protocol. # # In order to create a real Gopher \u0026quot;hole\u0026quot; (the name of a Gopher site in Gopher # talking), you likely need a script like the following: # # https://github.com/antirez/gopher2redis # # --- SECURITY WARNING --- # # If you plan to put Redis on the internet in a publicly accessible address # to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance. # Once a password is set: # # 1. The Gopher server (when enabled, not by default) will still serve # content via Gopher. # 2. However other commands cannot be called before the client will # authenticate. # # So use the 'requirepass' option to protect your instance. # # To enable Gopher support uncomment the following line and set # the option from no (the default) to yes. # # gopher-enabled no ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb \u0026lt;-- not recommended for normal workloads # -4: max size: 32 Kb \u0026lt;-- not recommended # -3: max size: 16 Kb \u0026lt;-- probably not recommended # -2: max size: 8 Kb \u0026lt;-- good # -1: max size: 4 Kb \u0026lt;-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \u0026quot;don't start compressing until after 1 node into the list, # going from either the head or tail\u0026quot; # So: [head]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]-\u0026gt;[next]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[prev]-\u0026gt;[tail] # 2 here means: don't compress head or head-\u0026gt;next or tail-\u0026gt;prev or tail, # but compress all nodes between them. # 3: [head]-\u0026gt;[next]-\u0026gt;[next]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[prev]-\u0026gt;[prev]-\u0026gt;[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Streams macro node max size / items. The stream data structure is a radix # tree of big nodes that encode multiple items inside. Using this configuration # it is possible to configure how big a single node can be in bytes, and the # maximum number of items it may contain before switching to a new node when # appending new stream entries. If any of the following settings are set to # zero, the limit is ignored, so for instance it is possible to set just a # max entires limit by setting max-bytes to 0 and max-entries to the desired # value. stream-node-max-bytes 4096 stream-node-max-entries 100 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \u0026quot;steps\u0026quot; are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \u0026quot;activerehashing no\u0026quot; if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \u0026quot;activerehashing yes\u0026quot; if you don't have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -\u0026gt; normal clients including MONITOR clients # replica -\u0026gt; replica clients # pubsub -\u0026gt; clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit \u0026lt;class\u0026gt; \u0026lt;hard limit\u0026gt; \u0026lt;soft limit\u0026gt; \u0026lt;soft seconds\u0026gt; # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and replica clients, since # subscribers and replicas receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here, but must be 1mb or greater # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \u0026quot;hz\u0026quot; value. # # By default \u0026quot;hz\u0026quot; is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # Normally it is useful to have an HZ value which is proportional to the # number of clients connected. This is useful in order, for instance, to # avoid too many clients are processed for each background task invocation # in order to avoid latency spikes. # # Since the default HZ value by default is conservatively set to 10, Redis # offers, and enables by default, the ability to use an adaptive HZ value # which will temporary raise when there are many connected clients. # # When dynamic HZ is enabled, the actual configured HZ will be used # as a baseline, but multiples of the configured HZ value will be actually # used as needed once more clients are connected. In this way an idle # instance will use very little CPU time while a busy instance will be # more responsive. dynamic-hz yes # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # When redis saves RDB file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. rdb-save-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R \u0026lt; P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits | # +--------+------------+------------+------------+------------+------------+ # | 0 | 104 | 255 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 1 | 18 | 49 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 10 | 10 | 18 | 142 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 100 | 8 | 11 | 49 | 143 | 255 | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # # redis-benchmark -n 1000000 incr foo # redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less \u0026lt;= 10). # # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # # lfu-log-factor 10 # lfu-decay-time 1 ########################### ACTIVE DEFRAGMENTATION ####################### # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \u0026quot;hot\u0026quot; way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \u0026quot;CONFIG SET activedefrag yes\u0026quot;. # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag no # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage, to be used when the lower # threshold is reached # active-defrag-cycle-min 1 # Maximal effort for defrag in CPU percentage, to be used when the upper # threshold is reached # active-defrag-cycle-max 25 # Maximum number of set/hash/zset/list fields that will be processed from # the main dictionary scan # active-defrag-max-scan-fields 1000 # Jemalloc background thread for purging will be enabled by default jemalloc-bg-thread yes # It is possible to pin different threads and processes of Redis to specific # CPUs in your system, in order to maximize the performances of the server. # This is useful both in order to pin different Redis threads in different # CPUs, but also in order to make sure that multiple Redis instances running # in the same host will be pinned to different CPUs. # # Normally you can do this using the \u0026quot;taskset\u0026quot; command, however it is also # possible to this via Redis configuration directly, both in Linux and FreeBSD. # # You can pin the server/IO threads, bio threads, aof rewrite child process, and # the bgsave child process. The syntax to specify the cpu list is the same as # the taskset command: # # Set redis server/io threads to cpu affinity 0,2,4,6: # server_cpulist 0-7:2 # # Set bio threads to cpu affinity 1,3: # bio_cpulist 1,3 # # Set aof rewrite child process to cpu affinity 8,9,10,11: # aof_rewrite_cpulist 8-11 # # Set bgsave child process to cpu affinity 1,10,11 # bgsave_cpulist 1,10-11 ","date":"2020年11月06日","permalink":"/posts/2020-11-7-docker-redis-config/","summary":"使用docker安装redis，挂载外部配置和数据 mkdir /docker\nmkdir /docker/redis","title":"docker 下的 redis 如何挂载外部配置和数据"},{"contents":"This is chapter 1 of nested chapter.\nUt aliquam dui convallis purus interdum In nec aliquet nisi. Vestibulum varius dui eu ante tempor maximus. Pellentesque at ipsum libero. Sed non augue enim. Sed accumsan arcu sed aliquam maximus. Donec fringilla sem at cursus placerat. Nulla commodo enim nunc, at luctus libero facilisis ut. Quisque id leo at velit pharetra imperdiet vel et felis. Nunc eu tincidunt nulla. Nunc consectetur ligula ut hendrerit malesuada. Sed commodo aliquam elit, at dapibus tellus pellentesque non. Sed vel massa pulvinar, sollicitudin justo sit amet, porta lacus. Quisque eleifend placerat placerat. Phasellus nisl eros, consectetur sit amet mi eget, fringilla luctus purus.\nMorbi eu mi convallis Sed sed est nulla. Etiam fermentum sed lectus ac molestie. Vestibulum elementum lectus quis interdum semper. Nam id tempus nisl. Nullam in tempor sapien. Integer volutpat nibh vel libero ornare hendrerit. Vestibulum non condimentum nulla. Nulla vel finibus mi. Cras consequat ligula augue, ac vulputate est tincidunt eu. Maecenas nec nibh consequat, egestas nisl non, iaculis nunc. Maecenas eu dolor tortor. Cras in bibendum diam. Vivamus mattis ultricies diam, in dignissim ante scelerisque vel. In euismod dictum ligula, sed scelerisque ipsum volutpat at. Nam lacus elit, porttitor et orci quis, malesuada sagittis risus.\nAenean sit amet lorem sit amet Sed eget erat faucibus, malesuada risus at, mollis est. Suspendisse et fringilla nisl, vel placerat ipsum. Proin ut est ornare, molestie ligula et, pretium orci. Aliquam id justo in eros vulputate volutpat. Vivamus sed tellus id quam laoreet fermentum nec ut dui. Ut neque nibh, facilisis dapibus tempor at, egestas non urna. Lorem ipsum dolor sit amet, consectetur adipiscing elit. In maximus, mauris id tempor dapibus, ligula velit pharetra justo, vel molestie sem mauris nec nulla. Suspendisse purus magna, pretium sodales pretium id, consectetur et arcu. Fusce commodo vel nibh quis efficitur. Duis feugiat felis ac mi semper, at sollicitudin tellus ullamcorper. Morbi eget efficitur dui. Nam erat diam, ultrices non mollis in, vestibulum ut lectus. Nunc at dui sed metus vestibulum fringilla. Curabitur vitae mauris vitae neque efficitur molestie eget at nisi. Morbi vel vehicula libero, nec tincidunt augue.\nMaecenas faucibus est ac orci fermentum cursus Proin vestibulum pellentesque ipsum, non lobortis arcu volutpat at. Integer neque diam, luctus a turpis non, posuere elementum ex. Donec eget sagittis purus. Maecenas non laoreet quam. Quisque luctus porta pellentesque. Vestibulum auctor commodo tortor non imperdiet. Fusce sed tincidunt nunc. Aenean diam diam, mattis a enim nec, suscipit dignissim neque. Quisque at est quis tortor lobortis vehicula vitae vitae quam. Nam maximus rutrum felis, non feugiat nisl vestibulum quis.\nSuspendisse suscipit tellus at nibh tincidunt Nulla pellentesque, leo eget interdum interdum, sem felis consequat massa, id aliquet eros leo sit amet urna. Phasellus quis odio ut odio porttitor consequat quis a velit. Quisque purus velit, faucibus vel efficitur in, sagittis quis neque. Morbi pretium suscipit odio, vel tincidunt nunc dictum aliquam. Quisque non arcu at urna scelerisque semper ac et tortor. Aliquam a dapibus justo. Vestibulum tempus nunc eu condimentum ultrices. Aenean nec egestas nulla, ut congue libero. Aliquam ut mattis urna, in sollicitudin odio. Aenean ac tristique felis, a consequat erat. Aliquam luctus eros ut porttitor efficitur. Nam cursus elit id arcu semper, at sodales velit vulputate. Suspendisse potenti.\n","date":"2020年10月16日","permalink":"/docs/interview/base/chapter-1/","summary":"\u003cp\u003eThis is chapter 1 of nested chapter.\u003c/p\u003e","title":"Chapter 1"},{"contents":"This is chapter 1 of nested chapter.\nUt aliquam dui convallis purus interdum In nec aliquet nisi. Vestibulum varius dui eu ante tempor maximus. Pellentesque at ipsum libero. Sed non augue enim. Sed accumsan arcu sed aliquam maximus. Donec fringilla sem at cursus placerat. Nulla commodo enim nunc, at luctus libero facilisis ut. Quisque id leo at velit pharetra imperdiet vel et felis. Nunc eu tincidunt nulla. Nunc consectetur ligula ut hendrerit malesuada. Sed commodo aliquam elit, at dapibus tellus pellentesque non. Sed vel massa pulvinar, sollicitudin justo sit amet, porta lacus. Quisque eleifend placerat placerat. Phasellus nisl eros, consectetur sit amet mi eget, fringilla luctus purus.\nMorbi eu mi convallis Sed sed est nulla. Etiam fermentum sed lectus ac molestie. Vestibulum elementum lectus quis interdum semper. Nam id tempus nisl. Nullam in tempor sapien. Integer volutpat nibh vel libero ornare hendrerit. Vestibulum non condimentum nulla. Nulla vel finibus mi. Cras consequat ligula augue, ac vulputate est tincidunt eu. Maecenas nec nibh consequat, egestas nisl non, iaculis nunc. Maecenas eu dolor tortor. Cras in bibendum diam. Vivamus mattis ultricies diam, in dignissim ante scelerisque vel. In euismod dictum ligula, sed scelerisque ipsum volutpat at. Nam lacus elit, porttitor et orci quis, malesuada sagittis risus.\nAenean sit amet lorem sit amet Sed eget erat faucibus, malesuada risus at, mollis est. Suspendisse et fringilla nisl, vel placerat ipsum. Proin ut est ornare, molestie ligula et, pretium orci. Aliquam id justo in eros vulputate volutpat. Vivamus sed tellus id quam laoreet fermentum nec ut dui. Ut neque nibh, facilisis dapibus tempor at, egestas non urna. Lorem ipsum dolor sit amet, consectetur adipiscing elit. In maximus, mauris id tempor dapibus, ligula velit pharetra justo, vel molestie sem mauris nec nulla. Suspendisse purus magna, pretium sodales pretium id, consectetur et arcu. Fusce commodo vel nibh quis efficitur. Duis feugiat felis ac mi semper, at sollicitudin tellus ullamcorper. Morbi eget efficitur dui. Nam erat diam, ultrices non mollis in, vestibulum ut lectus. Nunc at dui sed metus vestibulum fringilla. Curabitur vitae mauris vitae neque efficitur molestie eget at nisi. Morbi vel vehicula libero, nec tincidunt augue.\nMaecenas faucibus est ac orci fermentum cursus Proin vestibulum pellentesque ipsum, non lobortis arcu volutpat at. Integer neque diam, luctus a turpis non, posuere elementum ex. Donec eget sagittis purus. Maecenas non laoreet quam. Quisque luctus porta pellentesque. Vestibulum auctor commodo tortor non imperdiet. Fusce sed tincidunt nunc. Aenean diam diam, mattis a enim nec, suscipit dignissim neque. Quisque at est quis tortor lobortis vehicula vitae vitae quam. Nam maximus rutrum felis, non feugiat nisl vestibulum quis.\nSuspendisse suscipit tellus at nibh tincidunt Nulla pellentesque, leo eget interdum interdum, sem felis consequat massa, id aliquet eros leo sit amet urna. Phasellus quis odio ut odio porttitor consequat quis a velit. Quisque purus velit, faucibus vel efficitur in, sagittis quis neque. Morbi pretium suscipit odio, vel tincidunt nunc dictum aliquam. Quisque non arcu at urna scelerisque semper ac et tortor. Aliquam a dapibus justo. Vestibulum tempus nunc eu condimentum ultrices. Aenean nec egestas nulla, ut congue libero. Aliquam ut mattis urna, in sollicitudin odio. Aenean ac tristique felis, a consequat erat. Aliquam luctus eros ut porttitor efficitur. Nam cursus elit id arcu semper, at sodales velit vulputate. Suspendisse potenti.\n","date":"2020年10月16日","permalink":"/docs/interview/language/chapter-1/","summary":"\u003cp\u003eThis is chapter 1 of nested chapter.\u003c/p\u003e","title":"Chapter 1"},{"contents":"This is chapter 1 of nested chapter.\nUt aliquam dui convallis purus interdum In nec aliquet nisi. Vestibulum varius dui eu ante tempor maximus. Pellentesque at ipsum libero. Sed non augue enim. Sed accumsan arcu sed aliquam maximus. Donec fringilla sem at cursus placerat. Nulla commodo enim nunc, at luctus libero facilisis ut. Quisque id leo at velit pharetra imperdiet vel et felis. Nunc eu tincidunt nulla. Nunc consectetur ligula ut hendrerit malesuada. Sed commodo aliquam elit, at dapibus tellus pellentesque non. Sed vel massa pulvinar, sollicitudin justo sit amet, porta lacus. Quisque eleifend placerat placerat. Phasellus nisl eros, consectetur sit amet mi eget, fringilla luctus purus.\nMorbi eu mi convallis Sed sed est nulla. Etiam fermentum sed lectus ac molestie. Vestibulum elementum lectus quis interdum semper. Nam id tempus nisl. Nullam in tempor sapien. Integer volutpat nibh vel libero ornare hendrerit. Vestibulum non condimentum nulla. Nulla vel finibus mi. Cras consequat ligula augue, ac vulputate est tincidunt eu. Maecenas nec nibh consequat, egestas nisl non, iaculis nunc. Maecenas eu dolor tortor. Cras in bibendum diam. Vivamus mattis ultricies diam, in dignissim ante scelerisque vel. In euismod dictum ligula, sed scelerisque ipsum volutpat at. Nam lacus elit, porttitor et orci quis, malesuada sagittis risus.\nAenean sit amet lorem sit amet Sed eget erat faucibus, malesuada risus at, mollis est. Suspendisse et fringilla nisl, vel placerat ipsum. Proin ut est ornare, molestie ligula et, pretium orci. Aliquam id justo in eros vulputate volutpat. Vivamus sed tellus id quam laoreet fermentum nec ut dui. Ut neque nibh, facilisis dapibus tempor at, egestas non urna. Lorem ipsum dolor sit amet, consectetur adipiscing elit. In maximus, mauris id tempor dapibus, ligula velit pharetra justo, vel molestie sem mauris nec nulla. Suspendisse purus magna, pretium sodales pretium id, consectetur et arcu. Fusce commodo vel nibh quis efficitur. Duis feugiat felis ac mi semper, at sollicitudin tellus ullamcorper. Morbi eget efficitur dui. Nam erat diam, ultrices non mollis in, vestibulum ut lectus. Nunc at dui sed metus vestibulum fringilla. Curabitur vitae mauris vitae neque efficitur molestie eget at nisi. Morbi vel vehicula libero, nec tincidunt augue.\nMaecenas faucibus est ac orci fermentum cursus Proin vestibulum pellentesque ipsum, non lobortis arcu volutpat at. Integer neque diam, luctus a turpis non, posuere elementum ex. Donec eget sagittis purus. Maecenas non laoreet quam. Quisque luctus porta pellentesque. Vestibulum auctor commodo tortor non imperdiet. Fusce sed tincidunt nunc. Aenean diam diam, mattis a enim nec, suscipit dignissim neque. Quisque at est quis tortor lobortis vehicula vitae vitae quam. Nam maximus rutrum felis, non feugiat nisl vestibulum quis.\nSuspendisse suscipit tellus at nibh tincidunt Nulla pellentesque, leo eget interdum interdum, sem felis consequat massa, id aliquet eros leo sit amet urna. Phasellus quis odio ut odio porttitor consequat quis a velit. Quisque purus velit, faucibus vel efficitur in, sagittis quis neque. Morbi pretium suscipit odio, vel tincidunt nunc dictum aliquam. Quisque non arcu at urna scelerisque semper ac et tortor. Aliquam a dapibus justo. Vestibulum tempus nunc eu condimentum ultrices. Aenean nec egestas nulla, ut congue libero. Aliquam ut mattis urna, in sollicitudin odio. Aenean ac tristique felis, a consequat erat. Aliquam luctus eros ut porttitor efficitur. Nam cursus elit id arcu semper, at sodales velit vulputate. Suspendisse potenti.\n","date":"2020年10月16日","permalink":"/docs/interview/middleware/chapter-1/","summary":"\u003cp\u003eThis is chapter 1 of nested chapter.\u003c/p\u003e","title":"Chapter 1"},{"contents":"This is chapter 2 of nested chapter.\nVivamus facilisis risus ac eros porttitor Proin quis sapien porta, dictum magna eget, sodales erat. Nunc eu nisl at elit molestie placerat. Nulla nec massa non lectus malesuada vehicula. Suspendisse quis arcu elit. Praesent est ipsum, pharetra ac malesuada a, ultricies vel sem. Aenean arcu neque, bibendum a tincidunt eget, efficitur ac sapien. Vestibulum eu suscipit massa. Aenean non libero molestie, tristique purus eget, bibendum tellus. Donec id nisl laoreet elit sodales viverra nec et risus. Nullam tortor dolor, auctor convallis vulputate porta, egestas cursus lacus. Sed rhoncus ipsum accumsan magna cursus, sit amet fringilla sapien feugiat. Aliquam facilisis viverra risus, eget varius erat convallis quis.\nInteger feugiat orci a diam cursus tincidunt id a urna Suspendisse tristique sem erat, vitae fringilla purus euismod vel. Nam magna dui, scelerisque at nibh et, faucibus dapibus nibh. Nulla facilisi. Quisque a diam sed mauris pretium volutpat. Nam nec mollis dolor, in ultrices arcu. Suspendisse ut placerat ex, dictum fringilla libero. Quisque mollis aliquet tellus in congue.\nMaecenas eu ipsum at lectus commodo eleifend nec vel ante Nulla sit amet dolor sed elit semper accumsan. Nunc suscipit sapien eget nulla imperdiet gravida. Pellentesque dignissim metus eget felis tempor, at elementum ipsum vulputate. In pretium accumsan neque ut pulvinar. Donec rhoncus at elit id vulputate. Maecenas massa libero, porttitor et lacus eget, gravida auctor tellus. Cras lacus urna, hendrerit et tincidunt efficitur, rutrum id nisi. Nunc quis accumsan ex, eget rutrum sem. Vivamus vel dui eget leo luctus consectetur. Sed tellus elit, aliquet quis commodo in, fermentum sit amet ante. Curabitur nec ornare tortor. Sed et tempus mi. Pellentesque sollicitudin porttitor mi eget hendrerit. Maecenas lobortis turpis ut quam placerat pharetra.\nMaecenas a lorem in sem feugiat ultricies Donec tincidunt interdum magna quis dictum. Integer non sollicitudin justo, id volutpat leo. Donec quis enim porttitor, hendrerit nisi non, accumsan orci. Nulla consectetur porta mauris, et pharetra nunc efficitur eu. Pellentesque eu consectetur tellus. In ac nunc ac turpis fermentum tempor vel vel nisl. Aenean convallis turpis ac ipsum mollis, eget venenatis lacus eleifend. Vestibulum sapien mauris, rutrum sit amet mi ut, volutpat mollis nunc. Praesent lobortis, orci efficitur molestie tempor, metus ante dignissim felis, ut interdum elit leo vitae lectus. Curabitur ex nunc, ornare at mi eu, eleifend molestie lectus. Duis pretium sapien in mauris pulvinar, ut auctor turpis tempus. Maecenas gravida nec dolor vel interdum. Ut posuere aliquam arcu et congue. Morbi scelerisque, leo vel luctus facilisis, est erat ultrices mauris, eget facilisis nisl libero a ligula. Mauris sit amet hendrerit sem. Etiam imperdiet, dolor id fermentum finibus, arcu est malesuada dui, at hendrerit nisi nibh ut mi.\nQuisque convallis sem sit amet magna aliquam interdum Vestibulum a odio vitae sapien tempor sollicitudin. In eu mi ex. Phasellus sollicitudin, augue vitae congue vestibulum, sem mi ullamcorper nibh, sagittis tincidunt ligula turpis vitae arcu. Phasellus orci felis, tristique id scelerisque eu, ornare vel mauris. Curabitur suscipit venenatis facilisis. Phasellus et est et purus posuere accumsan et ut quam. Donec in odio eu enim facilisis scelerisque. Sed eget condimentum elit, quis pretium ipsum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Suspendisse sit amet tortor ligula. Mauris lectus odio, cursus a diam a, ultricies posuere justo. Sed ultrices tempor mi, semper commodo quam lobortis at. Donec consectetur pretium nisl, eget dapibus nibh maximus scelerisque.\n","date":"2020年10月16日","permalink":"/docs/interview/base/chapter-2/","summary":"\u003cp\u003eThis is chapter 2 of nested chapter.\u003c/p\u003e","title":"Chapter 2"},{"contents":"This is chapter 2 of nested chapter.\nVivamus facilisis risus ac eros porttitor Proin quis sapien porta, dictum magna eget, sodales erat. Nunc eu nisl at elit molestie placerat. Nulla nec massa non lectus malesuada vehicula. Suspendisse quis arcu elit. Praesent est ipsum, pharetra ac malesuada a, ultricies vel sem. Aenean arcu neque, bibendum a tincidunt eget, efficitur ac sapien. Vestibulum eu suscipit massa. Aenean non libero molestie, tristique purus eget, bibendum tellus. Donec id nisl laoreet elit sodales viverra nec et risus. Nullam tortor dolor, auctor convallis vulputate porta, egestas cursus lacus. Sed rhoncus ipsum accumsan magna cursus, sit amet fringilla sapien feugiat. Aliquam facilisis viverra risus, eget varius erat convallis quis.\nInteger feugiat orci a diam cursus tincidunt id a urna Suspendisse tristique sem erat, vitae fringilla purus euismod vel. Nam magna dui, scelerisque at nibh et, faucibus dapibus nibh. Nulla facilisi. Quisque a diam sed mauris pretium volutpat. Nam nec mollis dolor, in ultrices arcu. Suspendisse ut placerat ex, dictum fringilla libero. Quisque mollis aliquet tellus in congue.\nMaecenas eu ipsum at lectus commodo eleifend nec vel ante Nulla sit amet dolor sed elit semper accumsan. Nunc suscipit sapien eget nulla imperdiet gravida. Pellentesque dignissim metus eget felis tempor, at elementum ipsum vulputate. In pretium accumsan neque ut pulvinar. Donec rhoncus at elit id vulputate. Maecenas massa libero, porttitor et lacus eget, gravida auctor tellus. Cras lacus urna, hendrerit et tincidunt efficitur, rutrum id nisi. Nunc quis accumsan ex, eget rutrum sem. Vivamus vel dui eget leo luctus consectetur. Sed tellus elit, aliquet quis commodo in, fermentum sit amet ante. Curabitur nec ornare tortor. Sed et tempus mi. Pellentesque sollicitudin porttitor mi eget hendrerit. Maecenas lobortis turpis ut quam placerat pharetra.\nMaecenas a lorem in sem feugiat ultricies Donec tincidunt interdum magna quis dictum. Integer non sollicitudin justo, id volutpat leo. Donec quis enim porttitor, hendrerit nisi non, accumsan orci. Nulla consectetur porta mauris, et pharetra nunc efficitur eu. Pellentesque eu consectetur tellus. In ac nunc ac turpis fermentum tempor vel vel nisl. Aenean convallis turpis ac ipsum mollis, eget venenatis lacus eleifend. Vestibulum sapien mauris, rutrum sit amet mi ut, volutpat mollis nunc. Praesent lobortis, orci efficitur molestie tempor, metus ante dignissim felis, ut interdum elit leo vitae lectus. Curabitur ex nunc, ornare at mi eu, eleifend molestie lectus. Duis pretium sapien in mauris pulvinar, ut auctor turpis tempus. Maecenas gravida nec dolor vel interdum. Ut posuere aliquam arcu et congue. Morbi scelerisque, leo vel luctus facilisis, est erat ultrices mauris, eget facilisis nisl libero a ligula. Mauris sit amet hendrerit sem. Etiam imperdiet, dolor id fermentum finibus, arcu est malesuada dui, at hendrerit nisi nibh ut mi.\nQuisque convallis sem sit amet magna aliquam interdum Vestibulum a odio vitae sapien tempor sollicitudin. In eu mi ex. Phasellus sollicitudin, augue vitae congue vestibulum, sem mi ullamcorper nibh, sagittis tincidunt ligula turpis vitae arcu. Phasellus orci felis, tristique id scelerisque eu, ornare vel mauris. Curabitur suscipit venenatis facilisis. Phasellus et est et purus posuere accumsan et ut quam. Donec in odio eu enim facilisis scelerisque. Sed eget condimentum elit, quis pretium ipsum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Suspendisse sit amet tortor ligula. Mauris lectus odio, cursus a diam a, ultricies posuere justo. Sed ultrices tempor mi, semper commodo quam lobortis at. Donec consectetur pretium nisl, eget dapibus nibh maximus scelerisque.\n","date":"2020年10月16日","permalink":"/docs/interview/language/chapter-2/","summary":"\u003cp\u003eThis is chapter 2 of nested chapter.\u003c/p\u003e","title":"Chapter 2"},{"contents":"This is chapter 2 of nested chapter.\nVivamus facilisis risus ac eros porttitor Proin quis sapien porta, dictum magna eget, sodales erat. Nunc eu nisl at elit molestie placerat. Nulla nec massa non lectus malesuada vehicula. Suspendisse quis arcu elit. Praesent est ipsum, pharetra ac malesuada a, ultricies vel sem. Aenean arcu neque, bibendum a tincidunt eget, efficitur ac sapien. Vestibulum eu suscipit massa. Aenean non libero molestie, tristique purus eget, bibendum tellus. Donec id nisl laoreet elit sodales viverra nec et risus. Nullam tortor dolor, auctor convallis vulputate porta, egestas cursus lacus. Sed rhoncus ipsum accumsan magna cursus, sit amet fringilla sapien feugiat. Aliquam facilisis viverra risus, eget varius erat convallis quis.\nInteger feugiat orci a diam cursus tincidunt id a urna Suspendisse tristique sem erat, vitae fringilla purus euismod vel. Nam magna dui, scelerisque at nibh et, faucibus dapibus nibh. Nulla facilisi. Quisque a diam sed mauris pretium volutpat. Nam nec mollis dolor, in ultrices arcu. Suspendisse ut placerat ex, dictum fringilla libero. Quisque mollis aliquet tellus in congue.\nMaecenas eu ipsum at lectus commodo eleifend nec vel ante Nulla sit amet dolor sed elit semper accumsan. Nunc suscipit sapien eget nulla imperdiet gravida. Pellentesque dignissim metus eget felis tempor, at elementum ipsum vulputate. In pretium accumsan neque ut pulvinar. Donec rhoncus at elit id vulputate. Maecenas massa libero, porttitor et lacus eget, gravida auctor tellus. Cras lacus urna, hendrerit et tincidunt efficitur, rutrum id nisi. Nunc quis accumsan ex, eget rutrum sem. Vivamus vel dui eget leo luctus consectetur. Sed tellus elit, aliquet quis commodo in, fermentum sit amet ante. Curabitur nec ornare tortor. Sed et tempus mi. Pellentesque sollicitudin porttitor mi eget hendrerit. Maecenas lobortis turpis ut quam placerat pharetra.\nMaecenas a lorem in sem feugiat ultricies Donec tincidunt interdum magna quis dictum. Integer non sollicitudin justo, id volutpat leo. Donec quis enim porttitor, hendrerit nisi non, accumsan orci. Nulla consectetur porta mauris, et pharetra nunc efficitur eu. Pellentesque eu consectetur tellus. In ac nunc ac turpis fermentum tempor vel vel nisl. Aenean convallis turpis ac ipsum mollis, eget venenatis lacus eleifend. Vestibulum sapien mauris, rutrum sit amet mi ut, volutpat mollis nunc. Praesent lobortis, orci efficitur molestie tempor, metus ante dignissim felis, ut interdum elit leo vitae lectus. Curabitur ex nunc, ornare at mi eu, eleifend molestie lectus. Duis pretium sapien in mauris pulvinar, ut auctor turpis tempus. Maecenas gravida nec dolor vel interdum. Ut posuere aliquam arcu et congue. Morbi scelerisque, leo vel luctus facilisis, est erat ultrices mauris, eget facilisis nisl libero a ligula. Mauris sit amet hendrerit sem. Etiam imperdiet, dolor id fermentum finibus, arcu est malesuada dui, at hendrerit nisi nibh ut mi.\nQuisque convallis sem sit amet magna aliquam interdum Vestibulum a odio vitae sapien tempor sollicitudin. In eu mi ex. Phasellus sollicitudin, augue vitae congue vestibulum, sem mi ullamcorper nibh, sagittis tincidunt ligula turpis vitae arcu. Phasellus orci felis, tristique id scelerisque eu, ornare vel mauris. Curabitur suscipit venenatis facilisis. Phasellus et est et purus posuere accumsan et ut quam. Donec in odio eu enim facilisis scelerisque. Sed eget condimentum elit, quis pretium ipsum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Suspendisse sit amet tortor ligula. Mauris lectus odio, cursus a diam a, ultricies posuere justo. Sed ultrices tempor mi, semper commodo quam lobortis at. Donec consectetur pretium nisl, eget dapibus nibh maximus scelerisque.\n","date":"2020年10月16日","permalink":"/docs/interview/middleware/chapter-2/","summary":"\u003cp\u003eThis is chapter 2 of nested chapter.\u003c/p\u003e","title":"Chapter 2"},{"contents":" This is official blog theme.\nXshell连接及docker、redis安装 1.连接Xshell ip addr 查看本机ip\nvi /etc/sysconfig/network-scripts/ifcfg-ens33 ifcfg-xxxx，xxx与ip地址2： 相同\na：进入插入模式\n修改ONBOOT=yes（开机启动网卡）\nesc 之后 :wq 保存并退出\nip addr 2:中的inet就是ip地址\n注：Linux 已修改ip地址（ONBOOT=yes）,但是不显示ip地址\n其实原因很简单，是因为你的（VMware DHCP Service）这个服务没有开，进入计算机管理–》服务和应用程序–》服务 找到VMware DHCP Service打开就行了\n2.安装docker sudo yum update 确保 yum 包更新到最新。\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2 安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的。\nsudo yum-config-manager \u0026ndash;add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 设置yum源，使用阿里云。\nsudo yum install docker-ce 安装docker。\nsudo systemctl start docker sudo systemctl enable docker 启动并加入开机启动。\ndocker version 验证安装是否成功(有client和service两部分表示docker安装启动都成功了)。\n3.docker安装redis docker search redis 查看可用版本。\ndocker pull redis:latest 拉取官方的最新版本的镜像。\ndocker image 列出本地镜像。查看是否已安装了 redis。\ndocker ps 显示当前正在运行的容器。\ndocker run -itd \u0026ndash;name redis-test -p 6379:6379 redis 或者：docker run -p 6379:6379 -d redis 创建容器并运行。\n运行已存在容器：sudo docker start \u0026ldquo;redis-test\u0026rdquo; 运行 redis 容器。\ndocker exec -it redis /bin/bash -it xxx xxx与docker ps 的 names 相同\n注意：/bin之前有空格\nroot@fcd32baa79ff:/data# redis-cli 通过 redis-cli 连接测试使用 redis 服务。\n中文不乱码：\u0026ndash;raw root@fcd32baa79ff:/data# redis-cli \u0026ndash;raw\n验证身份 已登录：auth password\n登录的同时验证：redis-cli -a password\n","date":"2020年10月11日","permalink":"/posts/2018-11-7-redis/","summary":"This is official blog theme.","title":"Xshell连接及docker、redis安装"},{"contents":"项目 1/ Django-mdeditor\nDjango-mdeditor 是基于 Editor.md 的一个 django Markdown 文本编辑插件应用。\n2/ liBlog\n一个基于Django1.11TLS版本的博客系统。自己的练手周边项目，会随时尝试新技术。\n教程 Python 入门基础教程\nhttps://pylixm.cc/python_start\nPython入门基础教程，分基础、实战、进阶提高，意在从实战中学习Python基础语法。\n","date":"2020年07月23日","permalink":"/project/","summary":"项目 1/ Django-mdeditor\nDjango-mdeditor 是基于 Editor.","title":"项目和教程"},{"contents":"有如下函数，其返回了一个引用：\nint \u0026amp;ret_val(int \u0026amp;a) { return a; } 另一个函数用来接收返回的引用：\n// 接收一个引用返回 void receive_refer() { int i = 10; int a = ret_val(i); // int 接收 cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; endl; // Output: 10 a = 50; cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; // Output: 10 int \u0026amp;a1 = ret_val(i); // int\u0026amp; 接收 a1 = 50; cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; // Output: 50 ret_val(i) = 555; cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; endl; // Output: 555 } 接收引用有两种方式：使用 int 接收，或者使用 int\u0026amp; 接收，分别对应上面的 a 和 a1，其中 a 不能修改 i 的值，而 a1 可以。我的疑问是，既然返回值是一个引用，那么接收者不应该也得是一个引用吗，就好比一个函数返回的是 int 指针，那么接收者也必须是一个 int 指针，而不能是 int。\n对应上面的指针版：\nint *ret_val(int *a) { return a; } void receive_() { int i = 10; int a = ret_val(\u0026amp;i); // 错误，不能用 int 接收指针 int *a1 = ret_val(\u0026amp;a); *a1 = 50; int aa = *a1; } ","date":"2020年04月15日","permalink":"/posts/2021-4-15-cpp_func_return_reference/","summary":"有如下函数，其返回了一个引用：\nint \u0026amp;ret_val(int \u0026amp;a) { return a; } 另一个函数用来接收返回的引用：","title":"c++：如何理解函数返回一个引用【未完】"},{"contents":"List // List代表一个双向链表。List零值为一个空的、可用的链表。 type List struct { // 哨兵节点，连接链表的头节点和尾结点，root.next 连接的是第一个结点，root.prev 连接的是最后一\t// 个结点 root Element // 当前列表长度，不包括哨兵节点 len int } 初始化 // 初始化或清空 list l。 func (l *List) Init() *List { l.root.next = \u0026amp;l.root l.root.prev = \u0026amp;l.root l.len = 0 return l } func (l *List) lazyInit() { if l.root.next == nil { l.Init() } } 插入 // insert inserts e after at, increments l.len, and returns e. func (l *List) insert(e, at *Element) *Element { e.prev = at e.next = at.next e.prev.next = e e.next.prev = e e.list = l l.len++ return e } // insertValue is a convenience wrapper for insert(\u0026amp;Element{Value: v}, at). func (l *List) insertValue(v interface{}, at *Element) *Element { return l.insert(\u0026amp;Element{Value: v}, at) } // PushBack inserts a new element e with value v at the back of list l and returns e. func (l *List) PushBack(v interface{}) *Element { l.lazyInit() return l.insertValue(v, l.root.prev) } 获取 Front() 获取链表的第一个节点\n// 返回列表 l 的第一个元素；如果列表为空，则返回nil。 func (l *List) Front() *Element { if l.len == 0 { return nil } // 这里可以看到是用 root.next 连接第一个结点 return l.root.next } Back() 获取链表的最后一个节点\n// 返回列表l的最后一个元素；如果列表为空，则返回nil。 func (l *List) Back() *Element { if l.len == 0 { return nil } // 这里可以看到是用 root.prev 连接最后一个节点 return l.root.prev } ","date":"2020年02月24日","permalink":"/posts/2021-2-24-go-container-list-source-code/","summary":"List // List代表一个双向链表。List零值为一个空的、可用的链表。 type List struct { // 哨兵节点，连接链表的头节点和尾结点，root.","title":"go container/list 源码分析【未完】"},{"contents":"场景 在 LeetCode 255.用队列实现栈 这道题里，需要用到两个队列来实现一个栈，并且在算法流程中，需要交换这两个队列的内容，例如：队列 A =\u0026gt; [1, 2, 3]，队列 B =\u0026gt; [555]，交换后：A =\u0026gt; [555]，B =\u0026gt; [1, 2, 3]。\ngo 标准库中并没有提供队列的实现，但是提供了 container/list 这样一个双向链表，所以可以用 list 来模拟一个队列，定义如下结构体：\ntype MyStack struct { Queue1 *list.List\t// 队列 1 Queue2 *list.List\t// 队列 2 } 在 push 方法中需要交换两个队列的内容：\n写法1 func (this *MyStack) Push(x int) { // 省略其他代码 // 在这里交换值 this.Queue1, this.Queue2 = this.Queue2, this.Queue1 } 上面的代码是可以正确运行的，这里的交换语句让 Queue1 重新指向了另一个 list，Queue2 也同样如此，例如：Queue1 =\u0026gt; 0x0001，Queue2 =\u0026gt; 0x0013，交换后：Queue1 =\u0026gt; 0x0013，Queue2 =\u0026gt; 0x0001，这里仅仅是改变了结构体成员属性的值（指向），并没有对属性 list 内部进行更改。\n写法2 为了让代码更简洁一些，我尝试用两个变量保存 this.Queue1和 this.Queue2：\nfunc (this *MyStack) Push(x int) { q1 := this.Queue1 q2 := this.Queue2 // 省略其他代码 // 在这里交换值 q1, q2 = q2, q1 } 上面这段代码会出错，因为这里的交换的是仅仅两个变量 q1 和 q2 的值，并没有对结构体中的属性造成影响。\n写法3 func (this *MyStack) Push(x int) { q1 := this.Queue1 q2 := this.Queue2 // 省略其他代码 // 在这里交换值 *q1, *q2 = *q2, *q1 } 因为 q1 和 q2 都是指针变量，所以可以通过 *取内容符 来交换值，就和交换两个指针参数值一样，看起来也没有任何问题，但是运行结果却诡异的出现了死循环，而且排查了许久都找不到原因。\n测试 Struct ll 定义一个类似的结构体：\ntype ll struct { l1 *list.List l2 *list.List } swap() 类似 方法三 中的取值交换方式：\nfunc swap(l1, l2 *list.List) { *l1, *l2 = *l2, *l1 } _print 定义一个输出函数用来友好输出链表，输出值的同时也输出地址值：\nfunc _print(name string, l *list.List) { fmt.Print(name + \u0026quot;: \u0026quot;) for i := l.Front(); i != nil; i = i.Next() { fmt.Printf(\u0026quot;%v[%p] -\u0026gt; \\t\u0026quot;, i.Value, i) } fmt.Println() } 测试代码：\n// 创建两个 list l1 := list.New() l2 := list.New() // 为两个链表添加值，l1: 1 -\u0026gt; 2 -\u0026gt; 3，l2: 666 l1.PushBack(1) l1.PushBack(2) l1.PushBack(3) l2.PushBack(666) // 输出交换前的值 fmt.Println(\u0026quot;before: \u0026quot;) _print(\u0026quot;l1\u0026quot;, l1) _print(\u0026quot;l2\u0026quot;, l2) // 交换函数 swap(l1, l2) // 输出交换后的值 fmt.Println(\u0026quot;after: \u0026quot;) _print(\u0026quot;l1\u0026quot;, l1) _print(\u0026quot;l2\u0026quot;, l2) 运行结果：\nbefore: l1: 1[0xc00009cde0] -\u0026gt; 2[0xc00009ce10] -\u0026gt; 3[0xc00009ce40] -\u0026gt; l2: 666[0xc00009ce70] -\u0026gt; after: l1: 666[0xc00009ce70] -\u0026gt; l2: 1[0xc00009cde0] -\u0026gt; 2[0xc00009ce10] -\u0026gt; 3[0xc00009ce40] -\u0026gt; 两个链表的值确实已经发生了改变，但是为什么在 方法 3 中会产生诡异的错误呢？继续尝试追加值：\n// 交换完之后 pushback fmt.Println(\u0026quot;after swap pushback: \u0026quot;) l2.PushBack(555) l1.PushBack(6) // 输出两个链表 _print(\u0026quot;l1\u0026quot;, l1) _print(\u0026quot;l2\u0026quot;, l2) 运行结果：\nafter swap pushback: l1: 666[0xc000104e70] -\u0026gt; 6[0xc000104ed0] -\u0026gt; 555[0xc000104ea0] -\u0026gt; \u0026lt;nil\u0026gt;[0xc000104db0] -\u0026gt; l2: 1[0xc000104de0] -\u0026gt; 2[0xc000104e10] -\u0026gt; 3[0xc000104e40] -\u0026gt; 上面的代码向 l1 尾部添加了 元素 6，向 l2 尾部添加元素 555，正确结果应该是：\nl1: 666 -\u0026gt; 6\nl2: 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 555\n但运行结果却十分诡异：\nl1: 666 -\u0026gt; 6 -\u0026gt; 555 -\u0026gt; nil\nl2: 1 -\u0026gt; 2 -\u0026gt; 3\n本来应该添加到 l2 的 555 被添加到了 l1，并且 l1 还多了一个 额外的 nil\n分析 结合源码进行了分析（不一定正确）\n至于为什么会出现 测试 中的诡异情况，发现自己水平有限，画图画的晕头转向，找不出一个合理的原因，只能草草归结于 list 的环形被破坏，导致产生无法预料的结果。\n总结 写法1 this.Queue1, this.Queue2 = this.Queue2, this.Queue1 只是交换了结构体属性 q1 和 q2 的指向，没有对 list 内部进行修改，而 写法3 *this.Queue1, *this.Queue2 = *this.Queue2, *this.Queue1 则是直接对 list 内部进行了修改，所以产生了无法预料的后果，这也充分说明了指针的危险性。\n","date":"2020年02月24日","permalink":"/posts/2021-2-24-go-container-list-swap/","summary":"场景 在 LeetCode 255.用队列实现栈 这道题里，需要用到两个队列来实现一个栈，并且在算法流程中，需要交换这两个队列的内容，例如：队列 A =\u0026gt; [1, 2, 3]，队列 B =\u0026gt; [555]，交换后：A =\u0026gt; [555]，B =\u0026gt; [1, 2, 3]。","title":"go 交换 container/list 的值"},{"contents":"go 基础之使用原生 http 库发送 post 请求\n方式 1 // 生成请求参数键值对 data := url.Values{\u0026quot;propId\u0026quot;: {giftId}, \u0026quot;propCount\u0026quot;: {count}, \u0026quot;roomId\u0026quot;: {c.RoomId}} // Encode() 编码成字符串，并包装成一个 io.Reader body := strings.NewReader(data.Encode()) // 传到第三个参数 req, err := http.NewRequest(\u0026quot;POST\u0026quot;, \u0026quot;https://www.douyu.com/japi/prop/donate/mainsite/v1\u0026quot;, body) client := http.Client{} response, err := client.Do(req) 方式 2 var r http.Request r.ParseForm() r.Form.Add(\u0026quot;propId\u0026quot;, giftId) r.Form.Add(\u0026quot;propCount\u0026quot;, count) r.Form.Add(\u0026quot;roomId\u0026quot;, roomId) body := strings.NewReader(r.Form.Encode()) req, err := http.NewRequest(\u0026quot;POST\u0026quot;, \u0026quot;https://www.douyu.com/japi/prop/donate/mainsite/v1\u0026quot;, body) client := http.Client{} response, err := client.Do(req) 方式 3 // 直接拼接字符串 body := strings.NewReader(\u0026quot;propId=268\u0026amp;propCount=1\u0026amp;roomId=9999\u0026amp;bizExt=%7B%22yzxq%22%3A%7B%7D%7D\u0026quot;) req, err := http.NewRequest(\u0026quot;POST\u0026quot;, \u0026quot;https://www.douyu.com/japi/prop/donate/mainsite/v1\u0026quot;, body) client := http.Client{} response, err := client.Do(req) 其他 上面的三种方法，只是创建请求参数键值对的方式不同，最后都是调用了 http.Client.Do() 方法，除此之外，go 还封装了一些 post 方法，如下：\n// 方法 1 data := url.Values{\u0026quot;start\u0026quot;:{\u0026quot;0\u0026quot;}, \u0026quot;offset\u0026quot;:{\u0026quot;xxxx\u0026quot;}} body := strings.NewReader(data.Encode()) resp, err := http.Post(\u0026quot;url\u0026quot;, \u0026quot;application/x-www-form-urlencoded\u0026quot;, body) // 方法 2 var r http.Request r.ParseForm() r.Form.Add(\u0026quot;xxx\u0026quot;, \u0026quot;xxx\u0026quot;) body := strings.NewReader(r.Form.Encode()) http.Post(\u0026quot;xxxx\u0026quot;, \u0026quot;application/x-www-form-urlencoded\u0026quot;, body) // 方法 3 data := url.Values{\u0026quot;start\u0026quot;:{\u0026quot;0\u0026quot;}, \u0026quot;offset\u0026quot;:{\u0026quot;xxxx\u0026quot;}} // PostForm 的默认 content-type 是 application/x-www-form-urlencoded http.PostForm(\u0026quot;xxxx\u0026quot;, data) 发送 json bodyJson := `{\u0026quot;name\u0026quot;:\u0026quot;zhang3 by json\u0026quot;, \u0026quot;id\u0026quot;:\u0026quot;123 by json\u0026quot;}` body := bytes.NewReader([]byte(bodyJson)) req, err := http.NewRequest(http.MethodPost, __url__, body) if err != nil { log.Fatal(err) } // 设置 content-type 为 application/json req.Header.Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;application/json\u0026quot;) resp1, _ := http.DefaultClient.Do(req) defer resp1.Body.Close() fmt.Println(\u0026quot;(2)\u0026quot;, string(read(resp1.Body))) ","date":"2020年01月24日","permalink":"/posts/2021-1-24-go-post-request/","summary":"go 基础之使用原生 http 库发送 post 请求","title":"go 发送 post 请求"},{"contents":"在 c++ 的引用规则中，有这么一条：一旦初始化完成 ，引用将和它的初始值对象一直绑定在一起（即不能再指向其他对象）。抱着初学者的好奇心态，我尝试改变引用的指向，写出了如下代码：\n// 定义两个 string 变量 std::string s = \u0026quot;123\u0026quot;; std::string s2 = \u0026quot;456\u0026quot;; // 让 s1 引用 s std::string \u0026amp;s1 = s; std::cout \u0026lt;\u0026lt; \u0026quot;修改前：\u0026quot; \u0026lt;\u0026lt; \u0026quot;s1: \u0026quot; \u0026lt;\u0026lt; s1 \u0026lt;\u0026lt; std::endl; // 修改 s1 的引用？ s1 = s2; std::cout \u0026lt;\u0026lt; \u0026quot;修改后：\u0026quot; \u0026lt;\u0026lt; \u0026quot;s1: \u0026quot; \u0026lt;\u0026lt; s1 \u0026lt;\u0026lt; std::endl; // output 修改前：s1: 123 修改后：s1: 456 这里我让 s1 重新指向 s2，并且从输出结果来看，s1 的值也确实被修改了，不对啊，规则中明明说好了不能修改的，怎么实际代码中可以？为了验证引用是否真的被修改，将三个变量的指针打印出来：\nprintf(\u0026quot;指针 =\u0026gt; s1: %p | s2: %p | s: %p \\n\u0026quot;, \u0026amp;s1, \u0026amp;s2, \u0026amp;s); // output 指针 =\u0026gt; s1: 0x7ffee199e7c8 | s2: 0x7ffee199e7b0 | s: 0x7ffee199e7c8 s1 和 s3 是同一个地址，说明引用并未改变，所以并不是引用被修改了，而是我的理解出现了问题，代码中的 s1 = s2，并不是让引用重新指向 s2，而是把值赋给了与引用绑定的对象（等同于 s = s2），这里做一个简单验证就很好理解了：\n// 定义两个 string 变量 std::string s = \u0026quot;123\u0026quot;; std::string s2 = \u0026quot;456\u0026quot;; // 让 s1 引用 s std::string \u0026amp;s1 = s; std::cout \u0026lt;\u0026lt; \u0026quot;修改前：\u0026quot; \u0026lt;\u0026lt; \u0026quot;s1: \u0026quot; \u0026lt;\u0026lt; s1 \u0026lt;\u0026lt; std::endl; // 修改 s1 的引用？ s1 = s2; // 修改 s2 的值，如果 s1 的引用被修改为指向 s2，那么 s1 的值应该也会改为 \u0026quot;666\u0026quot; s2 = \u0026quot;666\u0026quot;; std::cout \u0026lt;\u0026lt; \u0026quot;修改后：\u0026quot; \u0026lt;\u0026lt; \u0026quot;s1: \u0026quot; \u0026lt;\u0026lt; s1 \u0026lt;\u0026lt; std::endl; // output 修改前：s1: 123 修改后：s1: 456 s1 并没有被修改为 \u0026ldquo;666\u0026rdquo;，所以可以印证上面的说法。\n","date":"2020年01月11日","permalink":"/posts/2021-1-11-reference_c++/","summary":"在 c++ 的引用规则中，有这么一条：一旦初始化完成 ，引用将和它的初始值对象一直绑定在一起（即不能再指向其他对象）。抱着初学者的好奇心态，我尝试改变引用的指向，写出了如下代码：\n// 定义两个 string 变量 std::string s = \u0026quot;123\u0026quot;; std::string s2 = \u0026quot;456\u0026quot;; // 让 s1 引用 s std::string \u0026amp;s1 = s; std::cout \u0026lt;\u0026lt; \u0026quot;修改前：\u0026quot; \u0026lt;\u0026lt; \u0026quot;s1: \u0026quot; \u0026lt;\u0026lt; s1 \u0026lt;\u0026lt; std::endl; // 修改 s1 的引用？ s1 = s2; std::cout \u0026lt;\u0026lt; \u0026quot;修改后：\u0026quot; \u0026lt;\u0026lt; \u0026quot;s1: \u0026quot; \u0026lt;\u0026lt; s1 \u0026lt;\u0026lt; std::endl; // output 修改前：s1: 123 修改后：s1: 456 这里我让 s1 重新指向 s2，并且从输出结果来看，s1 的值也确实被修改了，不对啊，规则中明明说好了不能修改的，怎么实际代码中可以？为了验证引用是否真的被修改，将三个变量的指针打印出来：","title":"c++ 引用：如何理解\"引用被初始化后，不能再指向其他对象\""},{"contents":"Linux 的硬链接和软链接\n索引节点 在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)，可以通过 ls -li 查看每个文件的索引节点：\nls -li total 56 8758859 -rw-r--r-- 1 zz staff 364 12 27 11:57 bufio_read.go 7957721 -rw-r--r-- 1 zz staff 383 12 13 22:30 bufio_scann.go 10097171 -rw-r--r-- 1 zz staff 225 1 8 12:04 deferTest.go 10164017 -rw-r--r-- 1 zz staff 464 1 8 20:43 inToOut.go 8751290 -rw-r--r-- 1 zz staff 276 12 26 22:13 scanfTest.go 7173127 -rw-r--r-- 1 zz staff 338 12 2 14:45 t1.go 7173519 -rw-r--r--@ 1 zz staff 1520 12 2 14:59 test.go 硬链接 基本命令：\nln f1 f2 #创建f1的一个硬连接文件f2 硬链接可以简单把它想成 C 语言中的指针，它指向了物理硬盘的一个区块，多个硬链接会指向同一个区块，（可以理解为 C 中的多个指针指向了同一块内存空间）。\ntouch f \u0026amp;\u0026amp; echo \u0026quot;hello\u0026quot; \u0026gt; f # 创建一个文件并向其写入内容 ln f hard_f # 创建 f 的第一条硬链接 hard_f ln f hard_f1 # 创建 f 的第二条硬链接 hard_f1 ll -i 10966044 -rw-r--r-- 3 zz staff 6B 1 19 11:52 f 10966044 -rw-r--r-- 3 zz staff 6B 1 19 11:52 hard_f 10966044 -rw-r--r-- 3 zz staff 6B 1 19 11:52 hard_f1 # 两个硬链接的 inode index 与源文件相同 因为指向的是同一个硬盘区块，所以通过硬链接更改文件内容时，会导致其他的硬链接及源文件也被更改：\ncat f hard_f hard_f1 # 输出三个文件的初始值 hello hello hello echo \u0026quot;123\u0026quot; \u0026gt; hard_f # 修改 hard_f 的内容 cat f hard_f hard_f1 # 三个文件的值已经全部更改了 123 123 123 事实上文件系统会维护一个引用计数，只要有文件指向这个区块，它就不会从硬盘上消失。\nrm f # 删除源文件 cat hard_f hard_f1 # 尝试输出两个链接文件的内容 123 123 # 硬链接文件可以正常输出 硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。\n软链接 基本命令：\nln -s f1 soft 软链接也称之为符号连接（Symbolic Link），软链接文件有类似于Windows的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含有另一文件的位置信息，在硬盘上有独立的区块，访问时替换自身路径。\n与硬链接不同，软链接的 inode_index 与源文件是不同的，每个软链接的 inode_index 也不同，如下：\nln -s f soft_f1\t# 创建两个软链接 ln -s f soft_f2 ll -i 10973146 -rw-r--r-- 1 zz staff 4B 1 19 17:19 f 10973197 lrwxr-xr-x 1 zz staff 1B 1 19 17:19 soft_f1 -\u0026gt; f 10973203 lrwxr-xr-x 1 zz staff 1B 1 19 17:20 soft_f2 -\u0026gt; f 软链接的显示形式也比较特殊，为 软链接文件 -\u0026gt; 源文件。\n删除源文件后，所有关联的的软链接文件仍然存在（因为两个是不同的文件），但指向的是一个无效的链接：\nrm f\t# 删除源文件 cat soft_f1 soft_f2\t# 尝试输出两个软链接文件 cat: soft_f1: No such file or directory\t# 无法输出内容：找不到文件 cat: soft_f2: No such file or directory 总结 硬链接是指针，所有的硬链接都是指向同一个磁盘块。 删除一个指针不会真正删除文件，只有把所有的指针都删除才会真正删除文件。 软连接是另外一种类型的文件，保存的是它指向文件的全路径， 访问时会替换成绝对路径\n","date":"2020年01月11日","permalink":"/posts/2021-1-19-soft-hard-link/","summary":"Linux 的硬链接和软链接\n索引节点 在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)，可以通过 ls -li 查看每个文件的索引节点：","title":"硬链接和软链接"},{"contents":"英语 程序员工作中常见的英语词汇 人人都能用英语-李笑来 大前端类 Javascript 资源：Airbnb javascript编码规范 Fluter 资源：Fluter样式模板 电子书：Fluter实战 Vue.js 软件：mavonEditor 基于vue的markdown在线编辑器 微信小程序 资源：awesome-wechat-weapp 后端类 面试：后端面试基础知识合集 Python 资源：Python入门教程 资源：Python学习100天 资源：PyConChina 用户组 资源：Guido PEG系列文章翻译 资源：设计模式Python实现 资源：各种算法的Python实现 资源：Django优秀资料大全 awesome django 面试：关于Python的面试题合集 电子书：Django设计模式和最佳实践，基于django1.7 电子书：Python CookBook 第三版 电子书：利用Python进行数据分析·第2版 软件：httpx下一代的python http client 软件：requests-async 异步请求的http client Golang 资源：Awosome Golang 资源：Awosome Golang CN 资源：Golang语言知识图谱 资源：设计模式Golang实现 软件：lris: 一款优秀的web框架 软件：Kratos：bilibili开源的一套Go微服务框架，包含大量微服务相关框架及工具 软件：wide：一款基于 Web 的 Go 语言 IDE 电子书：GO语言高级编程 数据库类 大数据类 资源：大数据入门指南 算法类 资源：机器学习(Machine Learning)、深度学习(Deep Learning)、NLP面试中常考到的知识点和代码实现 机器学习 资源：机器学习100天-中文版 资源：机器学习100天-英文版 资源：西瓜书(周志华《机器学习》)学习笔记 电子书：动手学深度学习 电子书：深度学习工程师生存指南 Devops 资源：Devops学习指南，包括面试题 资源：运维实践指南 资源：应急响应实战笔记 软件：walle，一款开源上线部署系统 软件：CODO，基于tonardo的微服务devops解决方案 区块链 资源：区块链中文资源 其他 资源：markdown在线格式化项目，公众号作者推荐使用 资源：计算机技术的自学之路 面试：2019最新各大互联网面试题合集 资源：中国科学技术大学课程资源 软件：lottery 年会抽奖 资源：程序员找工作黑名单单位 电子书：免费的计算机类书籍大集合 ","date":"2019年11月04日","permalink":"/resource/","summary":"英语 程序员工作中常见的英语词汇 人人都能用英语-李笑来 大前端类 Javascript 资源：Airbnb javascript编码规范 Fluter 资源：Fluter样式模板 电子书：Fluter实战 Vue.","title":"资料库"},{"contents":" 欢迎来到我的小站呀，很高兴遇见你！🤝\n🏠 关于本站 👨‍💻 博主是谁 ⛹ 兴趣爱好 📬 联系我呀 ","date":"2019年01月25日","permalink":"/posts/about/","summary":" 欢迎来到我的小站呀，很高兴遇见你！🤝\n🏠 关于本站 👨‍💻 博主是谁 ⛹ 兴趣爱好 📬 联系我呀 ","title":"关于"},{"contents":"👏 欢迎使用 Gridea ！\n✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意\u0026hellip; \u0026hellip;\nGithub\nGridea 主页\n示例网站\n特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作\n🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片\n🏷️ 你可以对文章进行标签分组\n📋 你可以自定义菜单，甚至可以创建外部链接菜单\n💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端\n🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台\n💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统\n🇬🇧 你可以使用中文简体或英语\n🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力\n🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步\n🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃\n未来，它一定会成为你离不开的伙伴\n尽情发挥你的才华吧！\n😘 Enjoy~\n","date":"2018年12月12日","permalink":"/posts/hello-gridea/","summary":"👏 欢迎使用 Gridea ！\n✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意\u0026hellip; \u0026hellip;","title":"Hello Gridea"},{"contents":"","date":"0001年01月01日","permalink":"/search/","summary":"","title":"包含关键词 的文章"},{"contents":"","date":"0001年01月01日","permalink":"/tags/","summary":"","title":"标签"},{"contents":"","date":"0001年01月01日","permalink":"/categories/","summary":"","title":"分类"},{"contents":"","date":"0001年01月01日","permalink":"/archive/","summary":"","title":"归档"},{"contents":"个人信息 吴秀民/男/1988 本科/临沂大学/计算机科学与技术 工作年限：9年 技术博客：https://pylixm.cc/ Github：https://github.com/pylixm 期望职位：运维高级开发工程师/高级开发工程师 期望薪资：年薪60w，特别喜欢的公司可例外 期望城市：北京 联系方式 手机/微信：18612410531 Email：pyli.xm#gmail.com QQ：20894205 技能清单 技术方面\n熟练掌握Python开发，熟悉Golang/java的开发； 善于使用django 框架开发web系统，可个人负责开发业务复杂的前后端系统功能。熟悉flask 、tornado 、fastapi、sanic等python web 框架； 熟练掌握SaltStack/Ansible等运维自动化工具，阅读过SaltStack的部分源码； 熟练掌握Git/Svn等版本管理工具和工作流程； 熟悉vue.js生态，可开发业务复杂的前后端分离项目； 熟悉MySQL/PostgreSQL等数据库，Redis/Memcached/MongoDB等NOSQL； 熟悉Docker、K8S、Rancher等容器技术； 熟悉Prometheus、Grafana等生态组件； 熟悉ELK、KafKa等大数据组件； 熟悉LVS, Nginx等4、7层负载均衡技术； 熟悉TCP/IP，HTTP，Restful等互联网相关技术标准； 熟悉Linux下的自动化系统运维和开发的生态知识； 熟悉阿里云/腾讯云/华为云/微博开放平台/微信小程序； 了解互联网产品的生命周期和发展过程、开发团队管理和项目管理、应用发布和版本控制，具有良好的文档能力； 良好的自我驱动能力和时间管理，积极参与并贡献开源项目； 管理方面\n规划项目长期目标和短期目标 技术选型，方案设计，模块和功能设计 定期项目总结，向下管理，向上汇报 工作经历 获得场景视频（ 2021年4月 ~ 今 ） 就职运维部，任运维平台架构师，负责组织开发公司自动化运维平台、DevOps、容器等平台技术的开发落地工作。\n负责管理运维开发团队（5人），制定全年OKR，进行任务规划、拆分，对组员进行绩效考核。\n主要参与项目如下：\n项目名称 角色 描述 自动化运维平台 公司大一统运维平台 包括资产管理、业务组织管理、CICD、监控告警、数据库管理、容器管理等模块 重点项目 自动化运维平台 平台定位公司级别的自动化IT基础设施平台，包括资产管理、监控告警、CICD、数据库管理、容器管理等模块。\n作为项目架构师负责人，规划系统各模块的功能，组织组员高效的完成计划内的任务。\n资产管理：基于服务树标签，对公司资产进行了归类整理统一管理，增加了多源的自动采集。 CICD：使用 ansible 打通服务器管理通道，基于Jenkins 和自研任务调度系统，打通包括上下线流量、开关报警、健康监测、编译发布应用的DevOps全流程。将公司整体的上线时间由天缩短到分钟级别。 监控告警：基于Openfalcon 和 Prometheus做的二次开发，实现了基础、业务、容器的监控告警统一管理。包括云和自建机房的服务器、交换机等基础监控报警，业务进程、端口和自定义的业务监控，以及容器平台的各种容器、pod、node等容器监控。告警机制丰富可分组，可升级，实现了告警抑制、合并、静默、忽略等特殊处理。 数据库管理：对云和自建数据库做了实例和库的自动采集，利用开源的 Yearning 实现了数据的自主管理。 容器平台：自建了 K8S 容器平台，使用 Rancher 开源工具进行自主化管理，使用Prometheus监控生态采集容器相关监控指标和自建监控结合统一报警管理，基于Jenkins Pipline 实现容器的自动打包发布和回滚。 主要技术或软件关键字：Python、Django、vue.js、mysql、redis、Docker、Kubernetes、Rancher、Prometheus、Jenkins\n新浪微博 （ 2018年4月 ~ 2021年3月 ） 就职新浪微博SRE团队，主要负责自动化运维平台的研发工作和视频通讯等业务线的运维工作。主要参与项目如下：\n项目名称 角色 描述 微博自动化运维平台 运维底层管理通道 运维侧的管理系统，包括资产管理、CICD、监控报警等，为上层服务提供底层通道 配置管理系统 配置文件和脚本的管理系统 主要为手动和自动管理脚本和7层LB配置，提供系统功能服务 业务保障系统 业务侧的自动化平台 包括资产管理、利用率管理、CICD的上层封装调用 日志存储系统 日志级别排障管理 收集保存了业务从7层到上传服务的链路日志，方便排查问题 业务日常保障 日常保障 业务报警处理和新业务接入自动化运维体系 重点项目 微博自动化运维平台项目 「微博自动化运维平台项目」作为微博研发平台的底层自动化运维平台，提供了上线发布、监控预警、任务处理、流量切换、自动扩缩容和服务管理等功能。其间做了如下重点工作：\n作为主程参与开发了系统的DNS解析切换功能模块。针对老版本的域名切换做了用户体验的优化，缩短了操作路径，提高了操作效率。 作为主程为该项目首次引入vue.js框架，搭建了移动端的开发脚手架，并完成了该系统监控预警、上线发布和流量切换等模块的移动端的开发。 推动业务产品线接入运维系统，包括Java技术栈、Golang技术栈、Python技术栈的工程项目的接入，编写接入系统流程需要的各种任务处理脚本。 主要技术或软件关键字：Golang、Python、vue.js、mysql、redis、shell、Docker、Graphit、Grafana、ELK、Search Guard、K8S、kafka\n微博业务保障系统 该系统面向业务人员，将自动化系统做进一步的包装，实现RBAC的权限控制模型，将运维系统的部分功能直接开发给业务线。期间主要做了如下工作：\n搭建开发脚手架，前端使用Vue生态搭建，后端使用Beego实现。 独立实现了RBAC模型的权限控制，实现菜单基于角色的动态加载。 实现CMDB、代码发布和监控报警查看的功能模块的移动化前端实现。 主要技术或软件关键字：Golang、vue.js、mysql、redis、Docker、Beego\n日志收集管理系统 该系统基于ELK技术栈，实现了Nginx、ATS等中间件的日志的收集和存储，方便了问题排查，为后期的告警计算提供了数据源。16台设备组成的ES集群，日收集日志量在2.6T，每5s的入库数据量在3W左右。期间主要做了如下工作：\n基于ELK独立搭建了日志收集系统，使用ELK生态中的E和K作为存储和展示组件。 为减少资源消耗，使用Python开发了日志的推送端组件。 主要技术或软件关键字：ELK、Python、Kafka\n汽车之家 ( 2015年6月 ~ 2018年4月) 就职基础平台运维开发团队，主要从事运维自动化平台的研发工作。主要参与项目如下：\n项目名称 角色 描述 资产管理系统 底层数据银行 对有形和无形资产实现效率管理，为上层服务提供数据保障 配置管理 中间件层自动化 实现中间件的自动化安装和配置 发布系统 CICD 实现业务代码的流程化发布 监控系统 保障依据 实现对基础设施和业务服务的监控数据采集和保障 Salt改造 底层通道 保障了底层通道的稳定性和安全性 运维统一平台项目 入口门户 统一各运维系统认证授权 重点项目 资产管理系统项目 本系统作为公司权威的资产管理中心(包括有形资产和无形资产), 利用工作流工具维护数据的准确性，以数据来解读运维平时工作中体现的业务思维。期间主要从事如下工作：\n作为项目负责人，参与了项目的改版，优化了用户体验，提高了数据的准确性，数据准确性基本保证在99%，增大了资产数据的覆盖率。 优化了前端页面和后端表结构，提高前端查询和展示速度。 增加了数据变更字段级别的记录，便于追溯和审计。 补充增加了虚拟资产「IP池」的管理。 构建了一整套，基于对称加密的API 体系平台，方便上层系统调用。 为保证数据的准确性，我们开发制定了以“盘”、“审”、“罚”为核心的自我审计方案。通过系统自查加规章制度的方式，大大提高了数据的准确率。详情可以见当时总结分享的文章聊聊CMDB的资产审计 主要技术或软件关键字：Python、Django、vue.js、Mysql、Celery、Redis、Puppet\n基于SaltStack的底层架构扩展 在运维自动化的过程中，需要一个批量机器的管理工具。经过调研，同时结合我们团队自身的技术栈，选择了SaltStack。但是它上层的API接口，并不能满足我们的需求。我们根据自身需求，对它做了改造：\n封装其Restfull API，使其可以横向扩展而不依赖Salt-Master; 增加Salt命令下发及模块执行的审计功能； 增加Salt的权限控制，可以根据Salt-Minion范围及执行模块做权限划分； 通过对SaltStack的改造，夯实了整个自动化操作的底层通道基础，使上层saas的自动化处理更加稳定、安全。在该项目期间，主要做了如下重点工作：\n开发了上层的API服务组件，提供了和外部服务交互的API，支持命令审计功能、用户权限功能等功能。 优化整体架构和API服务，使单台机器的QPS由200提升到1000+，详细可参考当时总结分析的文章记一次Tornado QPS优化。 主要技术或软件关键字：Python、Tornado、Mysql、Redis、SQLAchemy、Rabbitmq、SaltStack\n配置管理系统项目 该项目主要负责基础架构的各中间件的自动安装和配置，包括Tomcat、IIS、Squid等。期间主要工作如下：\n构建前端架构脚手架。 前端web端你的设计、开发和测试。 基于SaltStack的模块的自动化安装下发逻辑构建。 主要技术或软件关键字：Python、Django、Mysql、Redis、SaltStack\n运维统一平台项目 该项目作为各运维平台的统一入口，和外界交互的网关系统。该项目中，主要参与重点工作如下：\n和各业务方讨论确认了资源生命周期的各流程，包括上架、上线、变更、下线、退库流程细节。 基于流程，参与了各阶段的API 接口对接、设计和开发。 按照统一平台项目前端脚手架，统一改版了需要接入的系统页面。 主要技术或软件关键字：Python、Django、Mysql、Vue.js\n山东尚捷科技 ( 2012年7月 ~ 2015年6月) 期间主要作为Python研发，主要做了如下重点工作：\n作为研发参与了多家银行的数据经营考核系统。 作为项目组长，负责某银行的信用卡进件管理系统的需求洽谈、文档的编写、系统的设计及开发。 主要技术或软件关键字：Python、Django、PostgreSQL、Rabbitmq、HTML/CSS、Jquery\n开源项目 Django-mdeditor：基于editor.md的Django-app组件，star 300+。 Python基础教程：根据自己工作学习经验总结的Python入门教程。 技术文章 pipenv 试用过程分析 vagrant 开发环境搭建 我心目中Tornado最佳实践 SQLAlchemy 数据库链接池问题排查记录 Django3.0异步使用分享 谈谈前后端分离和认证问题 致谢 感谢您花时间阅读我的简历，期待能有机会和您共事。\n","date":"0001年01月01日","permalink":"/resume/","summary":"个人信息 吴秀民/男/1988 本科/临沂大学/计算机科学与技术 工作年限：9年 技术博客：https://pylixm.cc/ Github：https://github.","title":"简历"}]