<!DOCTYPE html>
<html
  lang="zh"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>k8s Deployment | /dev/null</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css">
<script defer src="/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>













<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto&#43;Serif&#43;SC:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/atom-one-dark.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/python.min.js"
     crossorigin></script>
<link rel="stylesheet" href="/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="/js/fontawesome.min.7ecdf591e18d9b7d9a9acfee01f5545be9b15d3fb9a6235fc83f0f7b48df77c7d3fd123037395d75224bf17af86143c1.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
   integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" 
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
   integrity="sha384-&#43;XBljXPPiv&#43;OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>
<link rel="preconnect" href="https://www.google-analytics.com" crossorigin>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-135903670-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'UA-135903670-1');
</script>

<meta name="referrer" content="no-referrer" />
<style>
    .search-container {
    margin-top: -0.3rem;
    }
    .search-container .search {
    border: 1px solid #e2e8f0;
    border-radius: 4px;
    }
    .search-container input {
    padding-left: 1rem;
    line-height: 2rem;
    outline: none;
    background: transparent;
    }
    .search-container button {
    font-size: 0.8rem;
    margin-right: 0.5rem;
    color: #e2e8f0;
    }

     
    .categories-card {
    margin: 0 auto;
     
    display: flex;
    align-items: center;
    justify-content: space-between;
    flex-direction: row;
    flex-wrap: wrap;
    line-height: 1.6rem;
    }

    .categories-card .card-item {
    font-size: .875rem;
    text-align: left;
    width: 45%;
    display: flex;
    align-items: flex-start;
    margin-top: 2rem;
    min-height: 10rem;
    padding: 0 2%;
    position: relative;
    }

    .categories-card .card-item .card-item-wrapper {
    width: 100%;
    overflow: hidden;
    }

    .categories-card .card-item .card-item-wrapper .card-item-title {
    font-size: 1.2rem;
    font-weight: bold;
    display: inline-block;
    margin-top: 1rem;
    margin-bottom: .75rem;
    }

    .categories-card .card-item .card-item-wrapper span {
    float: right;
    padding-right: 1rem;
    }

    .archive-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    box-sizing: border-box;
    margin: .25rem 0 .25rem 1.5rem;
    }

    .more-post {
    text-align: right;
    }
    .tag-cloud-tags {
    margin: 10px 0;
    }

    .tag-cloud-tags a {
    display: inline-block;
    position: relative;
    margin: 5px 10px;
    }
    
    .archive .single-title {
    text-align: right;
    }

    .archive .group-title {
    margin-top: 1.5rem;
    margin-bottom: 1rem;
    }
    .archive .archive-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    box-sizing: border-box;
    margin: 0.25rem 0 0.25rem 1.5rem;
    }
     

</style>
<script src="/fontawesome/js/all.min.js"></script>
<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="deplopyment 用于 pod 的更新相关操作。
如何更新 pod 更新是一个非常常见的场景，比如：当前 pod 运行的是某个 v1 版本的镜像，一个月后该镜像发布了 v2 版本，此时想将所有的 pod 更新到 v2 版本，该怎么做呢？">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"文章",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"k8s Deployment",
      "item":"/posts/k8s-deployment/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/k8s-deployment/"
    },
    "headline": "k8s Deployment | \/dev\/null","datePublished": "2022-05-13T10:53:35+00:00",
    "dateModified": "2022-05-13T10:53:35+00:00",
    "wordCount":  2487 ,
    "publisher": {
        "@type": "Person",
        "name": "void",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "deplopyment 用于 pod 的更新相关操作。\n如何更新 pod 更新是一个非常常见的场景，比如：当前 pod 运行的是某个 v1 版本的镜像，一个月后该镜像发布了 v2 版本，此时想将所有的 pod 更新到 v2 版本，该怎么做呢？"
}
</script><meta property="og:title" content="k8s Deployment | /dev/null" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/k8s-deployment/" />




<meta property="og:description" content="deplopyment 用于 pod 的更新相关操作。
如何更新 pod 更新是一个非常常见的场景，比如：当前 pod 运行的是某个 v1 版本的镜像，一个月后该镜像发布了 v2 版本，此时想将所有的 pod 更新到 v2 版本，该怎么做呢？" />




<meta property="og:locale" content="zh" />




<meta property="og:site_name" content="/dev/null" />






<meta property="article:published_time" content="2022-05-13T10:53:35&#43;00:00" />


<meta property="article:modified_time" content="2022-05-13T10:53:35&#43;00:00" />



<meta property="article:section" content="posts" />


<meta property="article:tag" content="k8s" />





<meta property="og:see_also" content="/posts/k8s-service/" />

<meta property="og:see_also" content="/posts/k8s-nginx-pod-de-containerport-wen-ti/" />

<meta property="og:see_also" content="/posts/k8s-daemonset/" />

<meta property="og:see_also" content="/posts/k8s-replicaset/" />

<meta property="og:see_also" content="/posts/k8s-replicationcontroller/" />

<meta property="og:see_also" content="/posts/k8s-cun-huo-tan-zhen/" />




  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">/dev/null</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">关于</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">文章</a>
            <a href="/docs/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">文档</a>
            <a href="/tags/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">标签</a>
            <a href="/categories/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">分类</a>
            <a href="/archive/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">归档</a>
        </div>

        <div class="flex">
            
            <div class="search-container relative pt-4 md:pt-0">
                <div class="search">
                    <form role="search" class="search-form" action="/search" method="get">
                    <label>
                        <input name="q" type="text" placeholder="搜索 ..." class="search-field">
                    </label>
                    <button>
                        <i class="fas fa-search"></i>
                    </button>
                    </form>
                </div>
            </div>


            <div class="relative pt-4 md:pt-0" style="margin-left: 1rem">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">浅色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">深色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">自动</span>
                </div>
            </div>

            
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8">
  
  
  <div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12">
    <div
      class=" bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"
    >
      <article class="prose">
  <h1 class="mb-4">k8s Deployment</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2022-05-13</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>12分钟阅读时长</span>
  </div>

  

  
</div>


  
  

  <p><strong>deplopyment</strong> 用于 pod 的更新相关操作。</p>
<h1 id="如何更新-pod">如何更新 pod</h1>
<p>更新是一个非常常见的场景，比如：当前 pod 运行的是某个 v1 版本的镜像，一个月后该镜像发布了 v2 版本，此时想将所有的 pod 更新到 v2 版本，该怎么做呢？</p>
<p>比较容易想到的以下几种方式：</p>
<h2 id="先删除所有旧版本的-pod再创建新版本的-pod"><strong>先删除所有旧版本的 pod，再创建新版本的 pod</strong></h2>
<p>实践如下：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-v1
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
</code></pre>
<p>创建一个基于 luksa/kubia:v1 的镜像的 pod，同时创建一个 service</p>
<pre><code class="language-shell">$ kubectl get po
NAME             READY   STATUS    RESTARTS   AGE
kubia-v1-td826   1/1     Running   0          4m16s
kubia-v1-k8hz4   1/1     Running   0          4m16s
kubia-v1-7kpmg   1/1     Running   0          4m16s

$ kubectl get rc
NAME       DESIRED   CURRENT   READY   AGE
kubia-v1   3         3         3       5m

$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.43.0.1       &lt;none&gt;        443/TCP   37h
kubia        ClusterIP   10.43.191.168   &lt;none&gt;        80/TCP    5m3s

$ curl kubia
This is v1 running in pod kubia-v1-7kpmg		
</code></pre>
<p>测试访问，输出表示当前运行的是 v1 版本</p>
<p>现在来进行升级操作，将 <code>luksa/kubia:v1</code> 升级为 <code>luksa/kubia:v2</code>。</p>
<p>修改 yaml：</p>
<pre><code class="language-yaml">spec:
      containers:
      - image: luksa/kubia:v2
      
</code></pre>
<p>执行 yaml：</p>
<pre><code class="language-shell">$ kubectl apply -f kubia-rc-and-service-v1.yaml
replicationcontroller/kubia-v1 configured
service/kubia unchanged

$ curl 10.43.191.168
This is v1 running in pod kubia-v1-7kpmg
</code></pre>
<p>发现输出的还是 v1，可能是因为 rc 已经有 3 个 pod 了，修改 image 不会令其删除旧的并创建新的，那就只好手动删除旧 pod 了：</p>
<pre><code class="language-shell">$ kubectl delete po -l app=kubia
pod &quot;kubia-v1-td826&quot; deleted
pod &quot;kubia-v1-k8hz4&quot; deleted
pod &quot;kubia-v1-7kpmg&quot; deleted
</code></pre>
<p>此时删除了所有 app=kubia 的 pod（在我的机器上测试时，这个删除操作阻塞了将近 10s），rc 检测到其管理的 pod 数量不足所需的数量 3，那么就会新创建 3 个 pod，因为之前改掉了创建 template 中的镜像为 v2，所以现在 rc 创建出来的都是新版本的 pod 了：</p>
<pre><code class="language-shell">$ kubectl get po
NAME             READY   STATUS    RESTARTS   AGE
kubia-v1-b5lkr   1/1     Running   0          37s
kubia-v1-tq7nh   1/1     Running   0          37s
kubia-v1-w6r6t   1/1     Running   0          37s

$ curl 10.43.191.168
This is v2 running in pod kubia-v1-w6r6t
</code></pre>
<p>经过上面的一系列操作，终于完成了 pod 的升级，这种方式有一个问题：删除 pod 后到 rc 创建完成这段时间，是没有 pod 能够提供服务的，这就会导致程序在一段时间内不可用，这对于一些应用是无法接收的。</p>
<h2 id="先创建新版本的-pod再删除所有旧版本的-pod"><strong>先创建新版本的 pod，再删除所有旧版本的 pod</strong></h2>
<p>先将镜像退回至 v1 版本，采用和上面相同的方式，这里就不贴具体操作了。</p>
<p>复制一份之前的 yaml，在该基础上进行修改：</p>
<pre><code class="language-shell">$ cp kubia-rc-and-service-v1.yaml kubia-rc-and-service-v2.yaml
</code></pre>
<p>修改为：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-v2 # 修改
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia-v2 # 修改
    spec:
      containers:
      - image: luksa/kubia:v2 # 修改
        name: nodejs
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  selector:
    app: kubia-v2 # 修改
  ports:
  - port: 80
    targetPort: 8080
</code></pre>
<p>修改之处已经标出，注意这里还更新了 service 的选择器，让其为 app=kubia-v2 的 pod 提供对外服务。</p>
<pre><code class="language-shell">$ kubectl apply -f kubia-rc-and-service-v2.yaml
replicationcontroller/kubia-v2 created
service/kubia configured

$ kubectl get rs
No resources found in default namespace.

$ kubectl get rc
NAME       DESIRED   CURRENT   READY   AGE
kubia-v1   3         3         3       68m
kubia-v2   3         3         3       18s

$ kubectl get po
NAME             READY   STATUS    RESTARTS   AGE
kubia-v1-7pxtm   1/1     Running   0          11m
kubia-v1-zt5pg   1/1     Running   0          11m
kubia-v1-gnssp   1/1     Running   0          11m
kubia-v2-46w9g   1/1     Running   0          20s
kubia-v2-66wqr   1/1     Running   0          20s
kubia-v2-bvjqc   1/1     Running   0          20s
</code></pre>
<p>此时访问 service：</p>
<pre><code class="language-shell">$ curl 10.43.191.168
This is v2 running in pod kubia-v2-dj8cf
</code></pre>
<p>发现已经切换到了 v2</p>
<p>后续再删除 v1 的 pod：</p>
<pre><code class="language-shell">$ kubectl get rc
NAME       DESIRED   CURRENT   READY   AGE
kubia-v1   3         3         3       76m
kubia-v2   3         3         3       98s

$ kubectl delete rc kubia-v1
replicationcontroller &quot;kubia-v1&quot; deleted

$ kubectl get po
NAME             READY   STATUS        RESTARTS   AGE
kubia-v2-dj8cf   1/1     Running       0          2m9s
kubia-v2-4q5tv   1/1     Running       0          2m9s
kubia-v2-rbbfd   1/1     Running       0          2m9s
kubia-v1-gnssp   1/1     Terminating   0          19m
kubia-v1-7pxtm   1/1     Terminating   0          19m
kubia-v1-zt5pg   1/1     Terminating   0          19m
</code></pre>
<p>等待一会，这些 Terminating 状态的 pod 就会消失了。</p>
<p>使用这种方式可以做到不停机更新，但是需要更多的硬件资源，因为在短时间内会同时运行 2 倍的 pod。</p>
<h2 id="交替执行删一个旧的创建一个新的滚动更新">交替执行，删一个旧的，创建一个新的（滚动更新）</h2>
<p>这种方式比较繁琐，就不演示了，大致的思路是：和 <strong>先创建新版本的 pod，再删除所有旧版本的 pod</strong> 中的做法一样，再创建一个用于管理 v2 的 rc（这里称为 rc-v2，之前的称为 rc-v1），但是 replicas 置为 0，此时有两个 rc：rc-v1，replicas 为 3，rc-v2，replicas 为 0，然后开始交替执行，rc-v1 缩容为 2，rc-v2 扩容到 1，v1 缩容到 1，v2 扩容到 2，以此类推，来完成升级。</p>
<p>这种方式避免了前面两种同时创建、同时删除的缺点， 但是其也有自己的缺点：较为繁琐，且容易出错，一套流程下来要敲非常多的命令。</p>
<p>好在强大的 k8s 提供了指令来完成 pod 的更新操作：</p>
<pre><code class="language-shell">$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2
</code></pre>
<blockquote>
<p>⚠️ 该命令已经被删除 ：error: unknown command &ldquo;rolling-update&rdquo; for &ldquo;kubectl&rdquo;</p>
<p><a href="https://stackoverflow.com/questions/65303683/why-kubectl-removed-command-rolling-update">https://stackoverflow.com/questions/65303683/why-kubectl-removed-command-rolling-update</a></p>
<p><a href="https://github.com/kubernetes/kubectl/commit/d3af7e08624bfa7c2f52714b47cfe96a52d15fc0">https://github.com/kubernetes/kubectl/commit/d3af7e08624bfa7c2f52714b47cfe96a52d15fc0</a></p>
<p>大致原因是：该命令是在客户端执行的，而不是在 k8s 内部执行，相比之下客户端更容易遇到一些网络中断、终端异常退出等错误，这会导致 pod 和 rc 处于中间状态</p>
</blockquote>
<h1 id="使用-deployment-进行更新">使用 deployment 进行更新</h1>
<p>使用如下 yaml：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
        - image: luksa/kubia:v1
          name: nodejs

</code></pre>
<p>创建：</p>
<pre><code class="language-shell">$ kubectl apply -f deployment.yaml --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/kubia created
</code></pre>
<p>书上着重强调了：<code>确保在创建时使用了 --record 选项。 这个选项会记录历史版本号， 在之后的操作中非常有用</code>。然而这里直接提示该选项已被废弃，难道已经默认记录版本号了吗？</p>
<pre><code class="language-shell">$ kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
kubia-5f6cdb7bf7-4pn9b   1/1     Running   0          7m26s
kubia-5f6cdb7bf7-kt8gp   1/1     Running   0          7m26s
kubia-5f6cdb7bf7-f6w5g   1/1     Running   0          7m26s

$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.43.0.1       &lt;none&gt;        443/TCP   2d
kubia        ClusterIP   10.43.191.168   &lt;none&gt;        80/TCP    11h

$ curl 10.43.191.168
This is v1 running in pod kubia-5f6cdb7bf7-f6w5g
</code></pre>
<p>开始执行滚动升级：</p>
<pre><code class="language-shell">$ kubectl patch deployment kubia -p '{&quot;spec&quot;: {&quot;minReadySeconds&quot;: 10}}'
deployment.apps/kubia patched

$ kubectl set image deployment kubia nodejs=luksa/kubia:v2 # 更新镜像
</code></pre>
<p>这里通过 patch 为 这个 deployment 添加了一条 minReadySeconds: 10 的属性，这个属性的作用是：指定新创建的 pod 至少要成功运行多久之后，才能将其视为可用，在后面会详细说明这个属性。</p>
<p>使用 set image 命令来进行更新，其中 nodejs 是 template.sepc.containers.name 指定的。之后更新就开始了，可以开一个新的 shell 观察升级过程：</p>
<pre><code class="language-shell">$ while true;do curl 10.43.191.168; sleep 0.5s;done
This is v1 running in pod kubia-5f6cdb7bf7-f6w5g
This is v2 running in pod kubia-88894bbd4-pbr5n
This is v2 running in pod kubia-88894bbd4-h8mck
This is v1 running in pod kubia-5f6cdb7bf7-f6w5g
This is v1 running in pod kubia-5f6cdb7bf7-f6w5g
This is v2 running in pod kubia-88894bbd4-w74nb
This is v2 running in pod kubia-88894bbd4-pbr5n
This is v1 running in pod kubia-5f6cdb7bf7-f6w5g
This is v2 running in pod kubia-88894bbd4-h8mck
This is v2 running in pod kubia-88894bbd4-h8mck
This is v2 running in pod kubia-88894bbd4-w74nb
This is v2 running in pod kubia-88894bbd4-h8mck
This is v2 running in pod kubia-88894bbd4-h8mck
This is v2 running in pod kubia-88894bbd4-w74nb
This is v2 running in pod kubia-88894bbd4-pbr5n
</code></pre>
<p>可以看到，刚开始请求会打到 v1 或者 v2，到后面就全部是 v2 了，说明升级已经完成，同时也说明该命令使用的是 <strong>滚动更新</strong> 的方式，其实具体使用哪种方式是可以配置的，字段是 .spec.strategy.type，默认为 RollingUpdate，也就是滚动更新，还有一种 Recreate，采用的是先删除所有旧的，再创建新 pod 的方式。</p>
<h1 id="使用-deployment-进行回滚">使用 deployment 进行回滚</h1>
<h2 id="为什么需要回滚">为什么需要回滚</h2>
<p>可能发布的新版本存在一些 bug，但是在更新之前没有发现，等发现以后整个系统已经更新成新版本了，此时就需要进行回滚操作了。</p>
<h2 id="实践">实践</h2>
<p>接着上面的操作，此时查看 rs，会发现之前的老 rs 没有被删除：</p>
<pre><code class="language-shell">$ kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
kubia-88894bbd4    3         3         3       29m
kubia-5f6cdb7bf7   0         0         0       87m

$ kubectl describe kubia-5f6
...
Pod Template:
  Labels:  app=kubia
           pod-template-hash=5f6cdb7bf7
  Containers:
   nodejs:
    Image:        luksa/kubia:v1
...

$ kubectl describe rs kubia-888
...
Pod Template:
  Labels:  app=kubia
           pod-template-hash=88894bbd4
  Containers:
   nodejs:
    Image:        luksa/kubia:v2
...
</code></pre>
<p>为什么要保留旧的 rs 呢？<del>这里先卖个关子</del> 为的是能够完成回滚操作</p>
<p>开始升级，这里会更新到 v3 ，这是一个带有错误的版本，访问从 5 次开始就会返回一个 500 错误：</p>
<pre><code class="language-shell">$ kubectl set image deployment kubia nodejs=luksa/kubia:v3 # 更新到 v3
deployment.apps/kubia image updated

$ kubectl rollout status deployment kubia # 查看更新状态
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 old replicas are pending termination...
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 old replicas are pending termination...
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 old replicas are pending termination...
deployment &quot;kubia&quot; successfully rolled out
</code></pre>
<p>在另一个终端查看访问 service 的结果：</p>
<pre><code class="language-shell">$ while true;do curl 10.43.191.168; sleep 0.5s;done
This is v3 running in pod kubia-56ff78687-kc77b
Some internal error has occurred! This is pod kubia-56ff78687-2b7tz
This is v3 running in pod kubia-56ff78687-kc77b
Some internal error has occurred! This is pod kubia-56ff78687-2b7tz
This is v2 running in pod kubia-88894bbd4-h8mck
Some internal error has occurred! This is pod kubia-56ff78687-2b7tz
This is v3 running in pod kubia-56ff78687-mbqdw
This is v3 running in pod kubia-56ff78687-mbqdw
Some internal error has occurred! This is pod kubia-56ff78687-mbqdw
This is v3 running in pod kubia-56ff78687-kc77b
This is v2 running in pod kubia-88894bbd4-h8mck
</code></pre>
<p>可以看到有一些报错了：Some internal error has occurred! This is pod kubia-56ff78687-mbqdw</p>
<p>对外提供一个有 bug 的 pod 显然是不行的，此时就需要执行回滚操作了：</p>
<pre><code class="language-shell">$ kubectl rollout undo deployment kubia
deployment.apps/kubia rolled back
</code></pre>
<p>在另一个终端查看访问 service 的结果：</p>
<pre><code class="language-shell">$ while true;do curl 10.43.191.168; sleep 0.5s;done
This is v2 running in pod kubia-88894bbd4-rvzvb
This is v2 running in pod kubia-88894bbd4-rvzvb
This is v2 running in pod kubia-88894bbd4-pxxwt
This is v2 running in pod kubia-88894bbd4-pxxwt
This is v2 running in pod kubia-88894bbd4-4wknk
This is v2 running in pod kubia-88894bbd4-4wknk
</code></pre>
<p>发现已经回滚到 v2 版本了。</p>
<p>查看更新记录：</p>
<pre><code class="language-shell">$ kubectl rollout history deployment kubia
deployment.apps/kubia
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=deployment.yaml --record=true
3         kubectl apply --filename=deployment.yaml --record=true
4         kubectl apply --filename=deployment.yaml --record=true
</code></pre>
<p>这里显示的和书中的有所不同，书中显示的是：</p>
<pre><code class="language-shell">$ kubectl rollout history deployment kubia
deployments &quot;kubia&quot;:
REVISION CHANGE-CAUSE
2 kubectl set image deployment kubia nodejs=luksa/kubia:v2 
3 kubectl set image deployment kubia nodejs=luksa/kubia:v3
</code></pre>
<p>感觉书上这种显示的友好一些，而且我执行的明明是 kubectl set image deployment kubia nodejs=xxx ，为什么这里记录的是 kubectl apply &ndash;filename=deployment.yaml &ndash;record=true ？</p>
<p>前面提到过，&ndash;record 已经被废弃，难道新版本对 CHANGE-CAUSE 进行了一些变更吗？暂时还不知道啥情况</p>
<p>可以通过指定版本来查看详细信息，这样就可以解决上面的问题了：</p>
<pre><code class="language-shell">$ kubectl rollout history deployment/kubia --revision=1
deployment.apps/kubia with revision #1
Pod Template:
  Labels:	app=kubia
	pod-template-hash=5f6cdb7bf7
  Annotations:	kubernetes.io/change-cause: kubectl apply --filename=deployment.yaml --record=true
  Containers:
   nodejs:
    Image:	luksa/kubia:v1
    Port:	&lt;none&gt;
    Host Port:	&lt;none&gt;
    Environment:	&lt;none&gt;
    Mounts:	&lt;none&gt;
  Volumes:	&lt;none&gt;
</code></pre>
<p>此时就可以看到该版本对应的镜像了。</p>
<p>至此已经经历了 v1 - v3 一共 3 个版本的迭代，有更新操作也有回滚操作，此时查看 rs：</p>
<pre><code class="language-shell">$ kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
kubia-5f6cdb7bf7   0         0         0       17h
kubia-88894bbd4    3         3         3       16h
kubia-56ff78687    0         0         0       15h
</code></pre>
<p>会发现每个版本的 rs 依然存在，为的就是能执行回滚操作，顺带一提，回滚不仅仅是上个版本，还可以是任意一个版本，使用 <code>kubectl rollout undo deployment kubia --to-revision=l</code> 即可。在使用了 deployment 后，系统中的 rs 就不用我们去管理了，所以我们也不需要自己去手动修改、删除 rs 了。</p>
<h1 id="控制滚动升级速率">控制滚动升级速率</h1>
<p>有两个属性可以用来控制升级速率：</p>
<ul>
<li>maxSurge：升级过程中最多允许超出 replicas 的 pod 数量，值可以是一个百分比或者一个数字</li>
<li>maxUnavailable：升级过程中最多允许的不可用 pod 数量，值可以是一个百分比或者一个数字</li>
</ul>
<p>这两个值定义在 .spec.strategy.rollingUpdate 中，下面对这两个属性进行展开说明。</p>
<p>前面提到过滚动升级的流程，大致是使用两个 rs，一个管理旧版本 pod，一个管理新版本 pod，管理旧版本的 rs 不断缩容，管理新版本的 rs 不断扩容，最终完成更新。通过上面的两个属性就可以对整个流程的速率进行控制，举个例子：现在一共有 3 个 pod 需要升级，此时将 maxUnavailable 设置为 2，代表最多允许有 2 个 pod 不可用，那么此时缩容 rs 就可以缩到 1。引用官方的说明：</p>
<blockquote>
<p><strong>最大不可用</strong></p>
<p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> 是一个可选字段，用来指定 更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是所需 Pods 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。 如果 <code>.spec.strategy.rollingUpdate.maxSurge</code> 为 0，则此值不能为 0。 默认值为 25%。</p>
<p>例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。 新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容， 确保在更新期间可用的 Pods 总数在任何时候都至少为所需的 Pod 个数的 70%。</p>
<p><strong>最大峰值</strong></p>
<p><code>.spec.strategy.rollingUpdate.maxSurge</code> 是一个可选字段，用来指定可以创建的超出期望 Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。 如果 <code>MaxUnavailable</code> 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。 此字段的默认值为 25%。</p>
<p>例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod 的总数不超过所需 Pod 总数的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩容， 同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%。</p>
</blockquote>
<p>对于书上的表 9.2 我有点疑问，对这两个属性的描述都是：<code>当把百分数转换成绝对值时，会将数字四舍五入</code>，但是后面又举了个例子：</p>
<blockquote>
<p>由于在之前场景中，设置的期望副本数为 3，上述的两个属性都设置为 25%, maxSurge 允许最多 pod数量达到 4， 同时 maxUnavailable 不允许出现任何不可用的pod (也就是说三个pod&amp;、须一直处于可运行状态)</p>
</blockquote>
<p>副本数为 3，maxUnavailable 设置为 25%，3*25% = 0.75，四舍五入等于 1，那么应该允许有 1 个 pod 不可用，为什么书上说的是不允许任何 pod 不可用？</p>
<p>在看官方的文档：maxUnavailable 说的是：百分比值会转换成绝对数并去除小数部分</p>
<p>maxSurge 说的是：百分比值会通过向上取整转换为绝对数</p>
<p>这显然和书上说的四舍五入有出入，那么到底实际情况如何呢？只有源码能揭晓答案了。</p>
<p>对应的函数在 kubernetes/pkg/controller/deployment/util.deployment_util 下：</p>
<pre><code class="language-go">// desired 代表副本数量
func ResolveFenceposts(maxSurge, maxUnavailable *intstrutil.IntOrString, desired int32) (int32, int32, error) {
	// maxSurge 没有设置，则使用默认值 0
	// roundUp 设置为 true，代表向上取整
	surge, err := intstrutil.GetScaledValueFromIntOrPercent(intstrutil.ValueOrDefault(maxSurge, intstrutil.FromInt(0)), int(desired), true)
	if err != nil {
		return 0, 0, err
	}
	// maxUnavailable 没有设置，则使用默认值 0
	// roundUp 设置为 false，代表向下取整
	unavailable, err := intstrutil.GetScaledValueFromIntOrPercent(intstrutil.ValueOrDefault(maxUnavailable, intstrutil.FromInt(0)), int(desired), false)
	if err != nil {
		return 0, 0, err
	}
	// 所以 surge 和 unavailable 采用的策略是不同的
	// 比如 replicas 为 3，maxSurge 和 maxUnavailable 都设置为 25%，
	// 3 * 25 / 100 = 0.75，surge 采用向上取整，所以为 1，而 unavailable
	// 采用向下取整, 所以为 0

	if surge == 0 &amp;&amp; unavailable == 0 {
		// Validation should never allow the user to explicitly use zero values for both maxSurge
		// maxUnavailable. Due to rounding down maxUnavailable though, it may resolve to zero.
		// If both fenceposts resolve to zero, then we should set maxUnavailable to 1 on the
		// theory that surge might not work due to quota.
		unavailable = 1
	}

	return int32(surge), int32(unavailable), nil
}
</code></pre>
<p>调用的 GetScaledValueFromIntOrPercent 函数：</p>
<pre><code class="language-go">// intOrPercent 代表一个整数或者百分比字符串，total 代表总数量，roundUp 代表向上取整还是向下取整，
// 如果 intOrPercent 是 int，那么直接返回，如果是百分比，则返回 total*百分比后的值（还需要进行取整处理）
func GetScaledValueFromIntOrPercent(intOrPercent *IntOrString, total int, roundUp bool) (int, error) {
	if intOrPercent == nil {
		return 0, errors.New(&quot;nil value for IntOrString&quot;)
	}
	// 获取 intOrPercent 的值，该值可能是一个数字或者百分比
	value, isPercent, err := getIntOrPercentValueSafely(intOrPercent)
	if err != nil {
		return 0, fmt.Errorf(&quot;invalid value for IntOrString: %v&quot;, err)
	}
	// 如果该值是百分比
	if isPercent {
		// 如果设置了 roundUp（不知道咋翻译），则向上取整，比如 1.2 =&gt; 2（不是四舍五入）
		if roundUp {
			value = int(math.Ceil(float64(value) * (float64(total)) / 100))
		} else {
			// 否则向下取整，比如 1.5 =&gt; 1
			value = int(math.Floor(float64(value) * (float64(total)) / 100))
		}
	}
	return value, nil
}
</code></pre>
<p>测试一下：</p>
<pre><code class="language-go">func TestResolveFenceposts1(t *testing.T) {
	maxSurge := intstr.FromString(&quot;25%&quot;)
	maxUnavailable := intstr.FromString(&quot;25%&quot;)
	var replicas int32 = 3
	s, u, err := ResolveFenceposts(&amp;maxSurge, &amp;maxUnavailable, replicas)
	if err != nil {
		t.Error(err)
	}
	t.Log(s, u)
}

// Output:
// 1 0 
// 代表两个值都设置为 25% 的情况下，允许多出 1 个 pod，允许有 0 个 pod 不可用
</code></pre>
<p>所以书上说的四舍五入并不准确，官方文档才是正确的，不过最靠谱的还是看源码 （狗头）。</p>
<h2 id="实践-1">实践</h2>
<p>该 yaml 用来创建一个 replicas 为 3 的 deployment：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
</code></pre>
<p>运行：</p>
<pre><code class="language-shell">$ kubectl apply -f nginx-deployment.yaml --record
deployment.apps/nginx-deployment created

$ kubectl get rs                                    
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-7848d4b86f   3         3         3       6m57s

# 更新 deployment 的镜像版本，1.91 是一个不存在的版本
$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91

# 查看该 deployment 管理的 rs，发现多出来了一个 rs，其期望的 pod 数量为 1，
# 但是因为 1.91 版本不存在，所以 READY 为 0，这个多出来的 rs 就是用来做滚动更新的
$ kubectl get rs                                                
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-7848d4b86f   3         3         3       6h54m
nginx-deployment-b475d749b    1         1         0       7m8s

# 查看更新后的 deployment
$ kubectl get deploy                                            
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     1            3           6h59m

# 查看 deployment 的详细信息，发现滚动策略为 25% max unavailable, 25% max surge，也就是允许多出 1 个 pod，允许
# 有 0 个 pod 不可用，所以会创建一个新的 pod，所以那个新创建出来的 rs 的 replicas 为 1 
$ kubectl describe deploy nginx-deployment        
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 11 Oct 2022 17:21:35 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 4
                        kubernetes.io/change-cause: kubectl apply --filename=nginx-deployment.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  nginx-deployment-7848d4b86f (3/3 replicas created)
NewReplicaSet:   nginx-deployment-b475d749b (1/1 replicas created)
Events:
  Type    Reason             Age                 From                   Message
  ----    ------             ----                ----                   -------
  Normal  ScalingReplicaSet  9m9s                deployment-controller  Scaled down replica set nginx-deployment-b475d749b to 0
  Normal  ScalingReplicaSet  6m9s (x2 over 13m)  deployment-controller  Scaled up replica set nginx-deployment-b475d749b to 1
</code></pre>
<h1 id="升级过程中检测新版本是否可用">升级过程中检测新版本是否可用</h1>
<p>在回滚一节中说过，更新的新版本可能是一个无法工作的版本，此时需要回滚操作，那么有没有一种方法可以边更新边检测，发现不可用就停止更新呢？当然可以，使用一个额外的 minReadySeconds 属性 + 就绪探针就可以实现。</p>
<p>minReadySeconds 属性指定新创建的 pod 至少要成功运行多久之后，才能将其视为可用。在 pod 可用之前，滚动升级的过程不会继续。当所有容器的就绪探针返回成功时，pod 就被标记为就绪状态。</p>
<p>如果一个新的 pod 运行出错，就绪探针返回失败，或者在 minReadySeconds 时间内它的就绪探针出现了失败， 那么新版本的滚动升级将被阻止。</p>
<p>之前的笔记有写过存活探针，这里的是就绪探针，二者有所区别，但区别不是特别大，这里简单摘要一下官方的文档：</p>
<blockquote>
<p>livenessProbe（存活探针）</p>
<p>指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其<a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">重启策略</a>决定未来。如果容器不提供存活探针， 则默认状态为 <code>Success</code>。</p>
<p>readinessProbe（就绪探针）</p>
<p>指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 <code>Failure</code>。 如果容器不提供就绪态探针，则默认状态为 <code>Success</code>。</p>
<p>startupProbe（启动探针）</p>
<p>指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。如果启动探测失败，<code>kubelet</code> 将杀死容器，而容器依其 <a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">重启策略</a>进行重启。 如果容器没有提供启动探测，则默认状态为 <code>Success</code>。</p>
</blockquote>
<h2 id="实践-2">实践</h2>
<p>yaml 如下：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  minReadySeconds: 10 # pod 就绪后继续等待 10 秒
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0 # 确保升级过程中 pod 被挨个替换
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
        - image: luksa/kubia:v3
          name: nodejs
          readinessProbe: # 就绪探针
            periodSeconds: 1 # 探针每隔 1 秒执行一次
            httpGet:
              path: /
              port: 8080
  selector:
    matchLabels:
      app: kubia

</code></pre>
<p>这是一个“坏掉”的镜像：就绪探针检测不会成功，下面来执行 yaml：</p>
<pre><code class="language-shell">$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml
deployment.apps/kubia configured
</code></pre>
<p>apply 命令可以用 YAML 文件中声明的字段来更新 Deployment。不仅更新镜像，而且还添加了就绪探针，以及在 YAML 中添加或修改的其他声明。 如果新的 YAML 也包含 replicas 字段，当它与现有 Deployment 中的数量不一 致时，那么 apply 操作也会对 Dpeloymnet 进行扩容。</p>
<p>此时查看升级过程：</p>
<pre><code class="language-shell">$ kubectl rollout status deployment kubia
Waiting for deployment &quot;kubia&quot; rollout to finish: 1 out of 3 new replicas have been updated...
# 很久以后...
error: deployment &quot;kubia&quot; exceeded its progress deadline
</code></pre>
<p>整个流程卡在了更新第一个 pod，说明更新失败了，使用 curl 请求 service 看看：</p>
<pre><code class="language-shell">$ while true;do curl 10.43.191.168; sleep 0.5s;done
This is v2 running in pod kubia-88894bbd4-4wknk
This is v2 running in pod kubia-88894bbd4-pxxwt
This is v2 running in pod kubia-88894bbd4-rvzvb
This is v2 running in pod kubia-88894bbd4-4wknk
This is v2 running in pod kubia-88894bbd4-pxxwt
This is v2 running in pod kubia-88894bbd4-pxxwt
This is v2 running in pod kubia-88894bbd4-pxxwt
</code></pre>
<p>发现请求全部打在了 v2 pod 上，没有 v3 版本，这是符合需求的，请求不应该落到一个有问题的 pod 上</p>
<p>查看 pod：</p>
<pre><code class="language-shell">$ kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
kubia-88894bbd4-rvzvb    1/1     Running   0          23h
kubia-88894bbd4-4wknk    1/1     Running   0          23h
kubia-88894bbd4-pxxwt    1/1     Running   0          23h
kubia-74c44776ff-rm4xw   0/1     Running   0          98s
</code></pre>
<p>发现有一个 pod 没有处于 READY 状态，因为我们设置了 maxSurge 为 1，所以一共有 replicas + 1 个 pod，多出来就是这个没有 READY 的 v3 pod，因为它的探针检测失败了。通过 <code>kubectl describe po kubia-74c44776ff-rm4xw</code> 也可以看到，最后一行的事件显示：</p>
<pre><code class="language-shell">Warning  Unhealthy  2m25s (x22 over 2m45s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 500
</code></pre>

</article>


      
        <div class="my-4">
    
    <a href="/tags/k8s/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#k8s</a>
    
</div>
      

      



      

      
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
        <span class="text-primary-text block font-bold"
          >上一页</span
        >
        <a href="/posts/iptables/" class="block">linux iptables</a>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
        <span class="text-primary-text block font-bold">下一页</span>
        <a href="/posts/block-code/" class="block">一段莫名阻塞的 reflect 代码</a>
      
    </div>
  </div>


      



  <div id="valine-comments" class="mt-4"></div>
<script defer src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js" 
  integrity="sha384-e0&#43;DNUCJo75aOAzHQbFWYBCM9/S4f0BhRJXvEgbE3mMS85RM20MSSGStHuNdY2QK"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    new Valine({
      el: "#valine-comments",appId:"6KXGn05vaODkTMKM7zd5lWwl-gzGzoHsz",appKey:"qIMQwH4WrxTe8ds3Ua4HAbet",
    })
  });
</script>

    </div>
    
      <div class="col-span-2">
        
        
          <div
  class="
    bg-primary-bg
   prose sticky top-16 z-10 hidden px-6 py-4 lg:block"
>
  <h3>本页内容</h3>
</div>
<div
  class="sticky-toc  hidden px-6 pb-6 lg:block"
>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#先删除所有旧版本的-pod再创建新版本的-pod"><strong>先删除所有旧版本的 pod，再创建新版本的 pod</strong></a></li>
    <li><a href="#先创建新版本的-pod再删除所有旧版本的-pod"><strong>先创建新版本的 pod，再删除所有旧版本的 pod</strong></a></li>
    <li><a href="#交替执行删一个旧的创建一个新的滚动更新">交替执行，删一个旧的，创建一个新的（滚动更新）</a></li>
  </ul>

  <ul>
    <li><a href="#为什么需要回滚">为什么需要回滚</a></li>
    <li><a href="#实践">实践</a></li>
  </ul>

  <ul>
    <li><a href="#实践-1">实践</a></li>
  </ul>

  <ul>
    <li><a href="#实践-2">实践</a></li>
  </ul>
</nav>
</div>
<script>
  window.addEventListener("DOMContentLoaded", () => {
    enableStickyToc();
  });
</script>

        
      </div>
    

    
    
      <div
        class=" bg-secondary-bg prose col-span-2 rounded p-6 lg:col-span-6"
      >
        <h3>相关</h3>
        
          <a href="/posts/k8s-service/" class="no-underline">k8s service</a>
          <br />
        
          <a href="/posts/k8s-nginx-pod-de-containerport-wen-ti/" class="no-underline">k8s nginx pod 的 containerPort 问题</a>
          <br />
        
          <a href="/posts/k8s-daemonset/" class="no-underline">k8s DaemonSet</a>
          <br />
        
          <a href="/posts/k8s-replicaset/" class="no-underline">k8s ReplicaSet</a>
          <br />
        
          <a href="/posts/k8s-replicationcontroller/" class="no-underline">k8s ReplicationController</a>
          <br />
        
          <a href="/posts/k8s-cun-huo-tan-zhen/" class="no-underline">k8s 存活探针</a>
          <br />
        
      </div>
    
  </div>

  
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        hljs.highlightAll();
      });
    </script>

          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 0000 <a>null</a> 
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
    </footer>
  </body>
</html>
