<!DOCTYPE html>
<html
  lang="zh"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>k8s service | /dev/null</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css">
<script defer src="/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>













<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto&#43;Serif&#43;SC:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/atom-one-dark.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/python.min.js"
     crossorigin></script>
<link rel="stylesheet" href="/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="/js/fontawesome.min.7ecdf591e18d9b7d9a9acfee01f5545be9b15d3fb9a6235fc83f0f7b48df77c7d3fd123037395d75224bf17af86143c1.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
   integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" 
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
   integrity="sha384-&#43;XBljXPPiv&#43;OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>
<link rel="preconnect" href="https://www.google-analytics.com" crossorigin>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-135903670-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'UA-135903670-1');
</script>

<meta name="referrer" content="no-referrer" />
<style>
    .search-container {
    margin-top: -0.3rem;
    }
    .search-container .search {
    border: 1px solid #e2e8f0;
    border-radius: 4px;
    }
    .search-container input {
    padding-left: 1rem;
    line-height: 2rem;
    outline: none;
    background: transparent;
    }
    .search-container button {
    font-size: 0.8rem;
    margin-right: 0.5rem;
    color: #e2e8f0;
    }

     
    .categories-card {
    margin: 0 auto;
     
    display: flex;
    align-items: center;
    justify-content: space-between;
    flex-direction: row;
    flex-wrap: wrap;
    line-height: 1.6rem;
    }

    .categories-card .card-item {
    font-size: .875rem;
    text-align: left;
    width: 45%;
    display: flex;
    align-items: flex-start;
    margin-top: 2rem;
    min-height: 10rem;
    padding: 0 2%;
    position: relative;
    }

    .categories-card .card-item .card-item-wrapper {
    width: 100%;
    overflow: hidden;
    }

    .categories-card .card-item .card-item-wrapper .card-item-title {
    font-size: 1.2rem;
    font-weight: bold;
    display: inline-block;
    margin-top: 1rem;
    margin-bottom: .75rem;
    }

    .categories-card .card-item .card-item-wrapper span {
    float: right;
    padding-right: 1rem;
    }

    .archive-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    box-sizing: border-box;
    margin: .25rem 0 .25rem 1.5rem;
    }

    .more-post {
    text-align: right;
    }
    .tag-cloud-tags {
    margin: 10px 0;
    }

    .tag-cloud-tags a {
    display: inline-block;
    position: relative;
    margin: 5px 10px;
    }
    
    .archive .single-title {
    text-align: right;
    }

    .archive .group-title {
    margin-top: 1.5rem;
    margin-bottom: 1rem;
    }
    .archive .archive-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    box-sizing: border-box;
    margin: 0.25rem 0 0.25rem 1.5rem;
    }
     

</style>
<script src="/fontawesome/js/all.min.js"></script>
<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="什么是 service pod 往往需要与集群内的其他 pod 进行通信，可能也有外部的客户端需要访问集群中的 pod，如果需要管理员手动将 pod 的地址提供给访问者，那就太麻烦了，而且在 k8s 中 pod 的地址是不确定的、变动的：pod 可能被 rc、rs 缩减，或者发生异常下线，或者节点异常。所以 k8s 需要提供一种统一的对外访问的资源类型，也就是 service。">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"文章",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"k8s service",
      "item":"/posts/k8s-service/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/k8s-service/"
    },
    "headline": "k8s service | \/dev\/null","datePublished": "2022-05-08T15:46:11+00:00",
    "dateModified": "2022-05-08T15:46:11+00:00",
    "wordCount":  2382 ,
    "publisher": {
        "@type": "Person",
        "name": "void",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "什么是 service pod 往往需要与集群内的其他 pod 进行通信，可能也有外部的客户端需要访问集群中的 pod，如果需要管理员手动将 pod 的地址提供给访问者，那就太麻烦了，而且在 k8s 中 pod 的地址是不确定的、变动的：pod 可能被 rc、rs 缩减，或者发生异常下线，或者节点异常。所以 k8s 需要提供一种统一的对外访问的资源类型，也就是 service。"
}
</script><meta property="og:title" content="k8s service | /dev/null" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/k8s-service/" />




<meta property="og:description" content="什么是 service pod 往往需要与集群内的其他 pod 进行通信，可能也有外部的客户端需要访问集群中的 pod，如果需要管理员手动将 pod 的地址提供给访问者，那就太麻烦了，而且在 k8s 中 pod 的地址是不确定的、变动的：pod 可能被 rc、rs 缩减，或者发生异常下线，或者节点异常。所以 k8s 需要提供一种统一的对外访问的资源类型，也就是 service。" />




<meta property="og:locale" content="zh" />




<meta property="og:site_name" content="/dev/null" />






<meta property="article:published_time" content="2022-05-08T15:46:11&#43;00:00" />


<meta property="article:modified_time" content="2022-05-08T15:46:11&#43;00:00" />



<meta property="article:section" content="posts" />


<meta property="article:tag" content="k8s" />





<meta property="og:see_also" content="/posts/k8s-nginx-pod-de-containerport-wen-ti/" />

<meta property="og:see_also" content="/posts/k8s-daemonset/" />

<meta property="og:see_also" content="/posts/k8s-replicaset/" />

<meta property="og:see_also" content="/posts/k8s-replicationcontroller/" />

<meta property="og:see_also" content="/posts/k8s-cun-huo-tan-zhen/" />

<meta property="og:see_also" content="/posts/k8s-pod-bi-ji/" />




  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">/dev/null</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">关于</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">文章</a>
            <a href="/docs/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">文档</a>
            <a href="/tags/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">标签</a>
            <a href="/categories/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">分类</a>
            <a href="/archive/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">归档</a>
        </div>

        <div class="flex">
            
            <div class="search-container relative pt-4 md:pt-0">
                <div class="search">
                    <form role="search" class="search-form" action="/search" method="get">
                    <label>
                        <input name="q" type="text" placeholder="搜索 ..." class="search-field">
                    </label>
                    <button>
                        <i class="fas fa-search"></i>
                    </button>
                    </form>
                </div>
            </div>


            <div class="relative pt-4 md:pt-0" style="margin-left: 1rem">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">浅色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">深色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">自动</span>
                </div>
            </div>

            
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8">
  
  
  <div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12">
    <div
      class=" bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"
    >
      <article class="prose">
  <h1 class="mb-4">k8s service</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2022-05-08</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>12分钟阅读时长</span>
  </div>

  

  
</div>


  
  

  <h1 id="什么是-service">什么是 service</h1>
<p>pod 往往需要与集群内的其他 pod 进行通信，可能也有外部的客户端需要访问集群中的 pod，如果需要管理员手动将 pod 的地址提供给访问者，那就太麻烦了，而且在 k8s 中 pod 的地址是不确定的、变动的：pod 可能被 rc、rs 缩减，或者发生异常下线，或者节点异常。所以 k8s 需要提供一种统一的对外访问的资源类型，也就是 service。</p>
<p>service 为 <strong>一组功能相同</strong>（逻辑意义上的，例如使用相同的标签）的 pod 提供单一不变的接入点，它的 IP 和端口不会改变，所以客户端可以统一连接到 service，service 再将请求转发给其管理的某一个 pod（会负载均衡的访问），这样就可以解决上面的 pod 地址变动问题。</p>
<h1 id="创建-service">创建 service</h1>
<p>下面的 yaml 可以创建一个 service：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80 # 该 service 的端口
    targetPort: 80 # 转发到 pod 的 80 端口
  selector:
    app: kubia # 具有 app=kubia 标签的 pod 都属于该 service
</code></pre>
<blockquote>
<p>⚠️ targetPort 必须指向一个有效的、被监听的 port，否则之后访问该 service 会报错 Connection refused</p>
</blockquote>
<p>创建完成后，可以使用下面的命令进行查看，获取 service 的 IP 地址：</p>
<pre><code class="language-shell">$ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP   23h
kubia        ClusterIP   10.43.87.107   &lt;none&gt;        80/TCP    23h
</code></pre>
<h2 id="测试"><strong>测试</strong></h2>
<p>知道 service 的 IP 地址后就可以进行测试了，有以下几种方式</p>
<ol>
<li>
<p>在 node 执行 curl serviceIP</p>
<pre><code class="language-shell">$ curl 10.43.87.107
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
# 省略 ...
</code></pre>
<p>因为我转发的容器是 nginx，所以会输出 nginx 的 index.html，这里 curl 的地址没有写端口号，是因为不写端口号会默认走 80 端口，然后这个 svc 的 port 刚好也是 80，所以可以省略。</p>
</li>
<li>
<p>进入一个容器内部执行 curl</p>
<pre><code class="language-shell">$ kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-2qkls   1/1     Running   0          24h
kubia-qgtjw   1/1     Running   0          24h
kubia-mbghg   1/1     Running   0          24h
nginx         1/1     Running   0          20h
$ kubectl exec -it kubia-2qkls -- curl 10.43.87.107
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
# 省略 ...
</code></pre>
<p>如果用过 docker，那么这个 exec 应该非常熟悉了</p>
</li>
</ol>
<h1 id="服务发现">服务发现</h1>
<p>虽然 service 提供了一个统一访问的 pod 的途径，但是还存在一个问题：访问者如何知道 service 的地址？如果需要管理员手动执行命令查看 service 的 IP 并告诉访问者，这显然有些麻烦，不够智能，好在牛逼的 k8s 为这一问题提供了服务发现的解决方案。</p>
<h2 id="使用环境变量">使用环境变量</h2>
<p>⚠️ 这种方式需要 <strong>service 先于 pod 创建</strong>，这样在 pod 创建过程中，k8s 会将其所属的 service 地址写入到其环境变量中，pod 通过环境变量就可以知道 service 的地址了。</p>
<p>service 的环境变量名是 matadata.name_SERVICE_HOST 和 matadata.name_SERVICE_PORT，name 会转换为全大写，- 会转换为下划线</p>
<p><strong>实践</strong></p>
<p>service 依然沿用上面的 yaml：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80 # 该 service 的端口
    targetPort: 80 # 转发到 pod 的 80 端口
  selector:
    app: kubia # 具有 app=kubia 标签的 pod 都属于该 service
</code></pre>
<p>客户端 pod yaml，这是一个 go 程序，通过 http 来访问 nginx，输出 nginx index.html</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: http-get
spec:
  containers:
  - name: http-get
    image: stdoutt/http-get-env-arm64:v1
    args: [&quot;-hostEnv&quot;, &quot;KUBIA_SERVICE_HOST&quot;, &quot;-portEnv&quot;, &quot;KUBIA_SERVICE_PORT&quot;]
</code></pre>
<p>注意该镜像只支持 arm64，需要传递两个参数，分别是 service host 的环境变量名和 port 的环境变量名，前面提到过这两个环境变量的命名规则，因为我创建的 service metadata.name 是 kubia，所以两个环境变量名分别是：KUBIA_SERVICE_HOST 和 KUBIA_SERVICE_PORT。</p>
<p>创建客户端 pod：</p>
<pre><code class="language-shell">$ kubectl apply -f http-get.yaml
pod/http-get created
</code></pre>
<p>查看 pod 日志：</p>
<pre><code class="language-shell">$ kubectl logs http-get
# 程序会打印出两个环境变量
KUBIA_SERVICE_HOST:10.43.65.224
KUBIA_SERVICE_PORT:80
url: http://10.43.65.224:80
# nginx index.html 成功输出了
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
<p>通过日志内容，说明 pod 成功通过环境变量获取到了 service 的地址，并通过 service 访问到了 pod。</p>
<p>附：</p>
<p>go 源码：</p>
<pre><code class="language-go">package main

import (
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;io&quot;
	&quot;net/http&quot;
	&quot;os&quot;
)

var hostServiceEnvName = flag.String(&quot;hostEnv&quot;, &quot;&quot;, &quot;k8s service host env name&quot;)
var portServiceEnvName = flag.String(&quot;portEnv&quot;, &quot;&quot;, &quot;k8s service port env name&quot;)

func main() {
	flag.Parse()
	host := os.Getenv(*hostServiceEnvName)
	port := os.Getenv(*portServiceEnvName)
	url_ := fmt.Sprintf(&quot;http://%v:%v&quot;, host, port)
	fmt.Printf(&quot;%v:%v\n&quot;, *hostServiceEnvName, host)
	fmt.Printf(&quot;%v:%v\n&quot;, *portServiceEnvName, port)
	fmt.Printf(&quot;url: %v\n&quot;, url_)
	resp, err := http.Get(url_)
	if err != nil {
		panic(err)
	}
	defer resp.Body.Close()

	b, err := io.ReadAll(resp.Body)
	if err != nil {
		panic(err)
	}
	fmt.Println(string(b))
}
</code></pre>
<p>Dockerfile</p>
<pre><code class="language-dockerfile">FROM golang:alpine AS builder

# 为我们的镜像设置必要的环境变量
ENV GO111MODULE=on \
    CGO_ENABLED=0 \
    GOOS=linux \
    GOARCH=arm64

# 移动到工作目录：/build
WORKDIR /build

# 将代码复制到容器中
COPY . .

# 将我们的代码编译成二进制可执行文件 app
RUN go build -o app .

###################
# 接下来创建一个小镜像
###################
FROM scratch

# 从builder镜像中把/dist/app 拷贝到当前目录
COPY --from=builder /build/app /

# 需要运行的命令
ENTRYPOINT [&quot;/app&quot;]
</code></pre>
<h2 id="使用-dns">使用 DNS</h2>
<p>除了环境变量外，还有一张更简单的方式：通过 DNS 发现服务，之后便可以通过 FQDN 进行访问，格式类似于：backend-database.default.svc.cluster.local，其中 backend-database 是 service 的名字，default 是 service 所在的命名空间，svc.cluster.local 是在所有集群本地服务名称中使用的可配置集群域后缀（这里不懂）。</p>
<blockquote>
<p>⚠️ 客户端仍然需要知道服务的端口号。如果服务使用标准端口号(例如，HTTP 的 80 端口或 Postgres 的 5432 端口)，这样是没问题的。 如果并不是标准端口， 客户端可以从环境变量中获取端口号 。</p>
<p>（摘抄自书上）</p>
<p>这里没看懂，都直接通过域名访问了，为什么客户端还需要知道端口号？</p>
</blockquote>
<p>如果客户端 pod 和被访问 pod 在同一个命名空间，那么可以直接用 service.name 进行访问，后面的可以全部省略，比如下面的例子，直接使用 service 的名字 kubia 进行访问：</p>
<pre><code class="language-shell"># 查看 service，获取 service 的名字
$ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP   33h
kubia        ClusterIP   10.43.65.224   &lt;none&gt;        80/TCP    8h

# 查看所有的 pod
$ kubectl get po
NAME          READY   STATUS      RESTARTS         AGE
kubia-msv7g   1/1     Running     0                8h
kubia-8d2gm   1/1     Running     0                8h
kubia-r4fcs   1/1     Running     0                8h

# 选择 1 个容器，在内部执行 curl 域名
$ kubectl exec kubia-msv79 -- curl kubia
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;

# curl 完整域名
$ kubectl exec -it kubia-msv79 -- curl kubia.default.svc.cluster.local
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
<h1 id="集群内访问">集群内访问</h1>
<h2 id="clusterip">ClusterIP</h2>
<p>通过集群内部 IP 地址暴露服务，但该地址 <strong>仅在集群内部</strong> 可见、可达，它无法被集群外部的客户端访问，是 service 的默认访问类型。如果不明确指定 clusterIP，则由 K8S 动态指定一个，也支持用户手动明确指定。</p>
<p>下面这个 yaml 创建了一个没有明确指定 clusterIP 的 service，以及一个创建 nginx 的 rs：</p>
<pre><code class="language-yaml">---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 80 # 该 service 的端口
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
</code></pre>
<p>运行后查看：</p>
<pre><code class="language-shell">$ kubectl get svc
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes               ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP   31h
nginx-service            ClusterIP   10.43.27.242   &lt;none&gt;        80/TCP    7h9m

$ kubectl describe svc nginx-service
Name:              nginx-service
Namespace:         default
Labels:            &lt;none&gt;
Annotations:       &lt;none&gt;
Selector:          app=nginx
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.27.242
IPs:               10.43.27.242
Port:              &lt;unset&gt;  80/TCP
TargetPort:        80/TCP
Endpoints:         10.42.0.30:80,10.42.0.32:80,10.42.0.36:80
Session Affinity:  None
Events:            &lt;none&gt;

$ curl 10.43.27.242
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
<p>发现这个 service 是 ClusterIP 类型的，且 k8s 自动指定了一个地址，并且可以成功访问。</p>
<p>也可以手动指定一个 clusterIP，在上面的 yaml 的 service 部分中修改一下：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: nginx-service1 # 起个新名字
spec:
  ports:
  - port: 80 # 该 service 的端口
  clusterIP: 10.43.27.66 # 手动指定 ip
  selector:
    app: nginx
</code></pre>
<p>运行：</p>
<pre><code class="language-shell">$ kubectl apply -f svc_clusterip.yaml
service/nginx-service1 created
$ kubectl get svc
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes               ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP   32h
nginx-service            ClusterIP   10.43.27.242   &lt;none&gt;        80/TCP    7h16m
nginx-service1           ClusterIP   10.43.27.66    &lt;none&gt;        80/TCP    10s

$ curl 10.43.27.66 # 可以成功访问
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
<p>但是不推荐手动指定 clusterIP，可能会造成 IP 冲突。</p>
<p><img src="https://raw.githubusercontent.com/autsu/diagrams/master/img/k8s_service_clusterip.webp" alt=""></p>
<h3 id="使用-kube-proxy-让外部访问-clusterip-service">使用 kube-proxy 让外部访问 ClusterIP Service</h3>
<p>正常来说 ClusterIP 类型的 Service 是无法从外部访问的，但是有种特殊的方法可以——使用 kube-proxy，也就是上图中的 proxy 部分。</p>
<p>具体的流程：</p>
<p>首先在集群内节点执行：</p>
<pre><code class="language-shell">$ kubectl proxy --address='0.0.0.0'  --accept-hosts='^*$' --port=8081
</code></pre>
<p>注意要加上 <code>--address</code> 和 <code>--accept-hosts</code>，否则访问会返回 <code>Forbidden</code>。</p>
<p>然后在外部机器执行以下格式语句进行访问：</p>
<pre><code class="language-shell">$ curl http://[nodeIP]:[port]/api/v1/namespaces/[namespace-name]/services/[service-name]/proxy
</code></pre>
<p>比如：</p>
<pre><code class="language-shell">$ curl -X GET -L http://192.168.31.50:8081/api/v1/namespaces/default/services/nginx/proxy
</code></pre>
<p>PS: &ldquo;curl -L&rdquo; 中的 &ldquo;-L&rdquo; 是 curl 命令中的选项，它的含义是 &ldquo;Follow any redirections&rdquo;. 也就是说，如果服务器返回了一个重定向响应，那么 curl 命令就会自动跟随重定向并请求重定向的地址。</p>
<p>在我的机器上测试，-X GET 可以不指定，但是 -L 一定要指定，否则返回结果为空</p>
<p>感觉有点类似 NodePort，也是使用节点 IP + proxy 开启的端口进行访问，不过前提是你的外部机器可以通过 IP 访问到这个节点</p>
<h1 id="对外访问">对外访问</h1>
<blockquote>
<p>参考：</p>
<p><a href="https://www.do1618.com/archives/1235/kubernetes-nodeport-vs-loadbalancer-vs-ingress%EF%BC%9F-%E6%88%91%E4%BB%AC%E5%BA%94%E8%AF%A5%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E4%BD%BF%E7%94%A8%EF%BC%9F/">Kubernetes NodePort vs LoadBalancer vs Ingress？ 我们应该什么时候使用？</a></p>
<p><a href="https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0">Kubernetes NodePort vs LoadBalancer vs Ingress？ 我们应该什么时候使用？（原文）</a></p>
</blockquote>
<p>前面提到的这些都仅限于集群的内部访问，集群外是无法访问的，比如上面的 service IP 是 10.43.65.224，这只在集群内可以 curl ，集群外是不行的，而且这个 IP 即便是在集群内也是无法 ping 通的，因为这是一个<strong>虚拟 IP 地址</strong>。</p>
<p>有以下几种方式可以将 service 暴露给集群外部：</p>
<h2 id="nodeport">NodePort</h2>
<p><img src="https://raw.githubusercontent.com/autsu/diagrams/master/img/k8s_service_nodeport.webp" alt=""></p>
<p>NodePort 会在 <strong>每个 node 上开启一个端口</strong> 用来访问 service，这个端口定义在 spec.ports[0].nodePort，用户可以使用 <strong>节点IP:nodePort</strong> 进行访问，也可以使用 <strong>serviceIP:port</strong> 访问（这种方式其实就是前面介绍的常规 service 访问方式，只能在集群内部使用），nodePort 也可以不指定，会随机从 30000-32767 中选择一个。</p>
<p>实践：</p>
<p>需要准备以下 yaml：</p>
<ul>
<li>用来创建 nginx deployment 的 yaml：</li>
</ul>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      # manage pods with the label app: nginx
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
</code></pre>
<ul>
<li>创建 NodePort 类型的 service，用来访问 nginx pod：</li>
</ul>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: kubia-node-port
spec:
  type: NodePort
  ports:
  - port: 80 # 该 service 的端口
    targetPort: 80 # 转发到 pod 的 80 端口
    nodePort: 30123 # 30000-32767 之间
  selector:
    app: nginx
</code></pre>
<p>之后使用 apply -f 执行上面的两个 yaml（这里就不展示了）</p>
<p>查看 service：</p>
<pre><code class="language-shell">$ kubectl get svc
NAME              TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
kubernetes        ClusterIP   10.43.0.1     &lt;none&gt;        443/TCP        13d
kubia-node-port   NodePort    10.43.21.78   &lt;none&gt;        80:30123/TCP   6m12s
</code></pre>
<p>此时就可以使用集群中的任意一个节点的 IP 加 30123 端口进行访问了，比如我的是：</p>
<pre><code class="language-shell">$ ifconfig | grep 192
inet 192.168.64.4  netmask 255.255.255.0  broadcast 192.168.64.255

$ curl 192.168.64.4:30123
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
<h2 id="loadbalance">LoadBalance</h2>
<p><img src="https://raw.githubusercontent.com/autsu/diagrams/master/img/k8s_service_lb.webp" alt=""></p>
<p>LoadBalance（负载均衡），大致流程是：对外提供一个统一的 IP，这个 IP 就是负载均衡器，它会将用户请求负载均衡的分发给 pod，我个人的理解是，它在 NodePort 的基础上又添加了一层类似代理层的东西，让客户端访问更加方便。</p>
<p>举个例子：假设此时有 2 个工作节点 1.1.1.1 和 2.2.2.2，每个节点上运行 3 个 nginx pod，nodePort 为 30000，那么在 NodePort 下，客户端访问需要指定某个 node 的具体 IP，比如 1.1.1.1:30000 或者 2.2.2.2:30000；如果使用 loadBalance 的话，则会提供一个统一的公网 IP（比如 180.1.1.1）作为负载均衡器，用户只要访问这个 IP 就可以（当然还要添加端口号），负载均衡器会负载均衡的把请求分发到 1.1.1.1:30000 或者 2.2.2.2:30000，比如如下的 loadBalance yaml：</p>
<pre><code class="language-yaml">apiVersion: vl 
kind: Service 
metadata:
	name: kubia-loadbalancer
spec:
	type: LoadBalancer 
	ports:
	- port: 80 # loadBalance 的端口
		targetPort: 8080 # 转发到 pod 的 8080 端口
		nodePort: 30000
	selector:
		app: kubia
</code></pre>
<p>LoadBalance 通常由云服务商提供，所以实践起来可能麻烦一些，需要在腾讯云这种平台上实践，好像也有一些组件可以提供本地的使用，这部分我还没有去了解。</p>
<h2 id="ingress">Ingress</h2>
<p><img src="https://raw.githubusercontent.com/autsu/diagrams/master/img/k8s_service_ingress.webp" alt=""></p>
<blockquote>
<p><del>个人吐槽（请忽视）：</del></p>
<p><del>这个玩意是我目前为止用的最蛋疼的一个功能，不是不好用，而是连用都用不上，我在 minikube 上使用的 ingress-nginx 因为拉取的镜像地址被墙，导致根本无法开启 ingress 服务，又因为我的 minikube 是跑在 multipass 虚拟机上的，不知道如何共享宿主机的 vpn，导致这个问题一直无法解决，不得不吐槽一下，m1 的生态还是有点问题，就虚拟机这块，我找了半天，基本能用的只有这个简陋的 multipass （收费的 parallels 没有尝试），vmware 直接无法运行（提示什么该软件基于 Intel 但却尝试使用 rosetta2 运行），还有一个 virtualBox ，这个我直接懒得下了，据说都不支持 m1，而且这种网络问题搞得我真的很头大，不得不吐槽一下天朝的网络，花费大把时间去解决网络问题，还有看见某个流程一直卡在 pull 上，让人有一种想砸掉电脑的冲动，最蛋疼的是这个问题还没有什么靠谱的解决方式，在 minikube 和 ingress-nginx 的 github 上找到了关于国内拉取的 issue，但是基本也没什么有用的答案，还有一些网上的教程，给的修改版 yaml 直接跑都跑不起来。</del></p>
<p><del>在 mac 上跑 minikube 可以成功开启 ingress 插件，但是又会报 <code>Because you are using a Docker driver on darwin, the terminal needs to be open to run it</code> 错误，网上找了半天也没看见一个能用的解决方法</del></p>
<p><del>折腾了一天都没把 ingress 给跑起来，感觉是在纯纯的浪费时间</del></p>
</blockquote>
<p>LoadBalance 的方式存在一个缺点：只能为一种类型的 service 提供服务，比如上面的介绍的 loadbalance 只是用来访问 nginx 的，如果现在集群添加了一些 redis pod，那么又要新创建一个 LoadBalance 来提供对外服务（比如 spec.ports[0].port=6379, target=6379, nodePort=30001），客户端通过 <strong>LoadBalanceIP:30000</strong> 这个地址来完成对 nginx pod 的访问，通过 <strong>LoadBalanceIP:30001</strong> 来完成对 redis pod 的访问，因为 LoadBalanceIP 是公网 IP，所以这么搞无疑有点浪费。</p>
<p>为了解决上面的问题，ingress 这个玩意就应运而生了，这个东西其实说白了就是 service 的 service（无限套娃？），类似于 web 路由的功能，通过访问不同的域名来完成对不同 service 的访问，就比如这张图：
<img src="https://autsu.github.io/post-images/1654445796590.jpg" alt="">
而且 ingress 工作在应用层，所以可以提供一些 service 不能实现的功能，比如基于 cookie 的会话亲和性 (session affinity) 等</p>
<h3 id="基于-minikube-nginx-ingress-的实践">基于 minikube nginx ingress 的实践</h3>
<blockquote>
<p>该实践基于 minikube</p>
</blockquote>
<p>all.yaml</p>
<p>包含了三个 pod 和三个对应的 service</p>
<pre><code class="language-yaml">---
apiVersion: v1
kind: Pod
metadata:
  name: hello-app
  labels:
    app: hello-app
spec:
  containers:
  - name: hello-app
    image: stdoutt/hello-app-arm64 # 注意这里使用的镜像仅适用于 arm64 机器
    ports:
    - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: hello-app-service
spec:
  type: NodePort
  ports:
  - port: 81 # 该 service 的端口
    targetPort: 8080 # 转发到 pod 的 8080 端口
    nodePort: 30111
  selector:
    app: hello-app
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80 # 该 service 的端口
    targetPort: 80 # 转发到 pod 的 80 端口
    nodePort: 30222
  selector:
    app: nginx
---
apiVersion: v1
kind: Pod
metadata:
  name: redis
  labels:
    app: redis
spec:
  containers:
  - name: redis
    image: redis
    ports:
    - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  type: NodePort
  ports:
  - port: 82 # 该 service 的端口
    targetPort: 6379 # 转发到 pod 的 8080 端口
    nodePort: 30333
  selector:
    app: redis
</code></pre>
<p>ingress.yaml</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: ingress.example.com
    http:
      paths:
      - path: /redis
        pathType: Prefix
        backend:
          service:
            name: redis-service
            port:
              number: 82
      - path: /hello-app
        pathType: Prefix
        backend:
          service:
            name: hello-app-service
            port:
              number: 81
      - path: /nginx
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
</code></pre>
<p>运行上面的 2 个 yaml</p>
<p>查看 ingress 的 ip：</p>
<pre><code class="language-shell">$ kubectl get ingress
NAME              CLASS   HOSTS                 ADDRESS        PORTS   AGE
example-ingress   nginx   ingress.example.com   192.168.49.2   80      9h
</code></pre>
<p>将 ingress ip 写入到 /etc/hosts：</p>
<pre><code class="language-shell">$ vim /etc/hosts
# 写入这一条：192.168.49.2 ingress.example.com
</code></pre>
<p>问题：</p>
<p>访问 /hello-app 可以正常显示结果：</p>
<pre><code class="language-shell">$curl ingress.example.com/hello-app
Hello, world!
Version: 1.0.0
Hostname: hello-app
</code></pre>
<p>但是 /nginx 和 /redis 都显示 404：</p>
<pre><code class="language-shell">$ curl ingress.example.com/nginx
&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.23.1&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>但是直接访问 service 又是通的：</p>
<pre><code class="language-shell">$ kubectl get svc
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
hello-app-service   NodePort    10.103.218.83   &lt;none&gt;        81:30111/TCP   9h
kubernetes          ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        14h
nginx-service       NodePort    10.99.214.102   &lt;none&gt;        80:30222/TCP   9h
redis-service       NodePort    10.104.106.67   &lt;none&gt;        82:30333/TCP   9h

$ minikube ip
192.168.49.2

$ curl 192.168.49.2:30222
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<blockquote>
<p>ps: minikube 如果想访问 service，需要将 service 定义为 nodePort 类型，然后通过 <code>minikube ip</code> 命令来获取 minikube 的 ip，然后用 <code>&lt;minikube_IP:nodePort&gt;</code> 的方式进行访问。似乎不能直接在集群内通过  <code>curl ClusterIP</code> 的方式来访问 service。</p>
</blockquote>
<p>解决方法：</p>
<p>在 ingress.yaml 中添加 <code>nginx.ingress.kubernetes.io/rewrite-target: /</code> 注解</p>
<p>修改后的 metadata：</p>
<pre><code class="language-yaml">metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
</code></pre>
<p>现在试试访问 /nginx：</p>
<pre><code class="language-shell">$ curl ingress.example.com/nginx
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>可以看到成功使用 ingress 访问了 nginx ，而不是之前的 404。</p>
<p>但是访问 /redis 会报 502 错误（可能对于 ingress 而言，redis 不是一个好的例子）：</p>
<pre><code class="language-shell">$ curl ingress.example.com/redis
&lt;html&gt;
&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>这里暂时不知道什么原因，感觉这里用 redis 来做实践本身也不太合理，毕竟 curl redis 本来就不会正常工作，但是用 redis-cli 通过访问 service 的方式可以正常工作：</p>
<pre><code class="language-shell">$ kubectl get svc
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
hello-app-service   NodePort    10.103.218.83   &lt;none&gt;        81:30111/TCP   12h
kubernetes          ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        17h
nginx-service       NodePort    10.99.214.102   &lt;none&gt;        80:30222/TCP   12h
redis-service       NodePort    10.104.106.67   &lt;none&gt;        82:30333/TCP   12h

$ k8s minikube ip
192.168.49.2

$ redis-cli -h 192.168.49.2 -p 30333
192.168.49.2:30333&gt; keys *
1) &quot;1&quot;
192.168.49.2:30333&gt;
</code></pre>
<p>但是用 ingress 的地址访问是不行的：</p>
<pre><code class="language-shell">$ redis-cli -h ingress.example.com/redis -p 82
Could not connect to Redis at ingress.example.com/redis:82: Name or service not known
not connected&gt;

$ redis-cli -h ingress.example.com/redis -p 30333
Could not connect to Redis at ingress.example.com/redis:30333: Name or service not known
not connected&gt;
</code></pre>
<p>这个问题暂时将其搁置，毕竟这里主要还是以学习 ingress 为主。</p>
<h1 id="特殊的-headless-service">特殊的 headless service</h1>
<p>service 提供了稳定的对外访问服务，同时还提供了负载均衡的能力，每次访问 service，都会将其转发到该 service selector 对应的 pod 中的某一个，具体转发给哪个是由 service 决定的，访问者没有自主选择权，但是如果访问者想自主选择访问哪个 pod，或者想访问该 service 下的所有 pod，此时该如何处理呢？为了解决这类问题，就需要 Headless Service 闪亮登场了。</p>
<blockquote>
<h4 id="疑问用户自行选择-pod-的场景有哪些用户需要访问-service-下所有-pod-的场景有哪些">疑问：用户自行选择 pod 的场景有哪些？用户需要访问 service 下所有 pod 的场景有哪些？</h4>
<h4 id="疑问使用-headless-service-后是否代表负载均衡已经失效需要用户自行实现">疑问：使用 headless service 后，是否代表负载均衡已经失效，需要用户自行实现？</h4>
</blockquote>
<p>这个 yaml 会创建一个 nginx rs，以及两个对应的 service，一个是正常的 service，还有一个是 headless 类型的 service：</p>
<p>（ps：好像如果 pod 指定了 containerPort，那么 service 这边可以不指定 targetPort）</p>
<pre><code class="language-yaml">---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-headless
spec:
  ports:
  - port: 80 # 该 service 的端口
    #targetPort: 8080 # 转发到 pod 的 8080 端口
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 80 # 该 service 的端口
    #targetPort: 8080 # 转发到 pod 的 8080 端口
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
</code></pre>
<p>运行：</p>
<pre><code class="language-shell">$ kubectl apply -f headless_service_test.yaml
service/nginx-service-headless unchanged
service/nginx-service created
replicaset.apps/nginx-rs unchanged

$ kubectl get po
NAME             READY   STATUS    RESTARTS   AGE
nginx-rs-f2b4j   1/1     Running   1 (33m ago)   47m
nginx-rs-dhxz7   1/1     Running   1 (33m ago)   47m
nginx-rs-sn8wk   1/1     Running   1 (33m ago)   47m
dnsutils         1/1     Running   1 (33m ago)   36m

$ kubectl get svc
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes               ClusterIP   10.43.0.1      &lt;none&gt;        443/TCP   24h
nginx-service-headless   ClusterIP   None           &lt;none&gt;        80/TCP    48s
nginx-service            ClusterIP   10.43.27.242   &lt;none&gt;        80/TCP    29s
</code></pre>
<p>测试一下普通的 service 能否访问：</p>
<pre><code class="language-shell">$ curl 10.43.27.242
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</code></pre>
<p>因为 headless service 明确指定了 clusterIP 为 None，所以不会给他分配一个集群内 IP，所以无法通过 curl 的方式访问。</p>
<p>使用 nslookup 查看二者的 dns 解析，看看他们两的区别，service 的 dns 格式是 <code>&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code>，注意这里需要在容器内部执行</p>
<pre><code class="language-shell">$ kubectl exec -it nginx-rs-dhxz7 -- nslookup nginx-service.default.svc.cluster.local
Server:		10.43.0.10
Address:	10.43.0.10:53

Name:	nginx-service.default.svc.cluster.local
Address: 10.43.27.242

$ kubectl exec -it nginx-rs-dhxz7 -- nslookup nginx-service-headless.default.svc.cluster.local
Server:		10.43.0.10
Address:	10.43.0.10:53

Name:	nginx-service-headless.default.svc.cluster.local
Address: 10.42.0.36
Name:	nginx-service-headless.default.svc.cluster.local
Address: 10.42.0.32
Name:	nginx-service-headless.default.svc.cluster.local
Address: 10.42.0.30
</code></pre>
<p>发现区别了吗？普通的 service 只返回了一条地址，这条地址正是 service 自身的 ip，而 headless service 返回了 3 条地址，这 3 条地址正是 service 代理的几个 pod 的地址：</p>
<pre><code class="language-shell">$ kubectl get po -o wide
NAME             READY   STATUS    RESTARTS      AGE   IP           NODE     NOMINATED NODE   READINESS GATES
nginx-rs-f2b4j   1/1     Running   1 (40m ago)   54m   10.42.0.36   ubuntu   &lt;none&gt;           &lt;none&gt;
nginx-rs-dhxz7   1/1     Running   1 (40m ago)   54m   10.42.0.30   ubuntu   &lt;none&gt;           &lt;none&gt;
nginx-rs-sn8wk   1/1     Running   1 (40m ago)   54m   10.42.0.32   ubuntu   &lt;none&gt;           &lt;none&gt;
dnsutils         1/1     Running   1 (40m ago)   43m   10.42.0.31   ubuntu   &lt;none&gt;           &lt;none&gt;
</code></pre>

</article>


      
        <div class="my-4">
    
    <a href="/tags/k8s/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#k8s</a>
    
</div>
      

      



      

      
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
        <span class="text-primary-text block font-bold"
          >上一页</span
        >
        <a href="/posts/http_mime_sniff/" class="block">HTTP MIME 嗅探 [待完善]</a>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
        <span class="text-primary-text block font-bold">下一页</span>
        <a href="/posts/multipass-xiu-gai-pei-zhi/" class="block">multipass 修改配置（内存、硬盘、CPU 等）适用于 m1 mac</a>
      
    </div>
  </div>


      



  <div id="valine-comments" class="mt-4"></div>
<script defer src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js" 
  integrity="sha384-e0&#43;DNUCJo75aOAzHQbFWYBCM9/S4f0BhRJXvEgbE3mMS85RM20MSSGStHuNdY2QK"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    new Valine({
      el: "#valine-comments",appId:"6KXGn05vaODkTMKM7zd5lWwl-gzGzoHsz",appKey:"qIMQwH4WrxTe8ds3Ua4HAbet",
    })
  });
</script>

    </div>
    
      <div class="col-span-2">
        
        
          <div
  class="
    bg-primary-bg
   prose sticky top-16 z-10 hidden px-6 py-4 lg:block"
>
  <h3>本页内容</h3>
</div>
<div
  class="sticky-toc  hidden px-6 pb-6 lg:block"
>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#测试"><strong>测试</strong></a></li>
  </ul>

  <ul>
    <li><a href="#使用环境变量">使用环境变量</a></li>
    <li><a href="#使用-dns">使用 DNS</a></li>
  </ul>

  <ul>
    <li><a href="#clusterip">ClusterIP</a>
      <ul>
        <li><a href="#使用-kube-proxy-让外部访问-clusterip-service">使用 kube-proxy 让外部访问 ClusterIP Service</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#nodeport">NodePort</a></li>
    <li><a href="#loadbalance">LoadBalance</a></li>
    <li><a href="#ingress">Ingress</a>
      <ul>
        <li><a href="#基于-minikube-nginx-ingress-的实践">基于 minikube nginx ingress 的实践</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#疑问用户自行选择-pod-的场景有哪些用户需要访问-service-下所有-pod-的场景有哪些">疑问：用户自行选择 pod 的场景有哪些？用户需要访问 service 下所有 pod 的场景有哪些？</a></li>
            <li><a href="#疑问使用-headless-service-后是否代表负载均衡已经失效需要用户自行实现">疑问：使用 headless service 后，是否代表负载均衡已经失效，需要用户自行实现？</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
</div>
<script>
  window.addEventListener("DOMContentLoaded", () => {
    enableStickyToc();
  });
</script>

        
      </div>
    

    
    
      <div
        class=" bg-secondary-bg prose col-span-2 rounded p-6 lg:col-span-6"
      >
        <h3>相关</h3>
        
          <a href="/posts/k8s-nginx-pod-de-containerport-wen-ti/" class="no-underline">k8s nginx pod 的 containerPort 问题</a>
          <br />
        
          <a href="/posts/k8s-daemonset/" class="no-underline">k8s DaemonSet</a>
          <br />
        
          <a href="/posts/k8s-replicaset/" class="no-underline">k8s ReplicaSet</a>
          <br />
        
          <a href="/posts/k8s-replicationcontroller/" class="no-underline">k8s ReplicationController</a>
          <br />
        
          <a href="/posts/k8s-cun-huo-tan-zhen/" class="no-underline">k8s 存活探针</a>
          <br />
        
          <a href="/posts/k8s-pod-bi-ji/" class="no-underline">k8s pod</a>
          <br />
        
      </div>
    
  </div>

  
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        hljs.highlightAll();
      });
    </script>

          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 0000 <a>null</a> 
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
    </footer>
  </body>
</html>
